All of the parameter estimation issues discussed thus far for ERGMs are relevant to 
the larger class of discrete state space exponential family models.
These models are commonly used to model phenomena with dependent structure, 
where the outcomes of the response variable of interest are in fact dependent on one 
another.  For example, the Ising 
model \citep{Ising,Potts} is an exponential family model that has been used to model 
ferromagnetism.  
%A realized 
%sample from this model is depicted in Figure~\ref{F:pottsimage}, where neighboring 
%pixels (representing atoms in a crystal lattice) are more likely to have the same 
%color.  
%We explore this model further in Section~\ref{S:Examples:Ising}.
Other examples of phenomena with dependent structure modeled with exponential 
families include
plant ecology \citep{Besag:1974,Besag:1975}, DNA fingerprint data \citep{Geyer:1992}
 and the lifetime fitness of plants \citep
{Shaw:2008}.

Although motivated by ERGMs, the algorithm we propose is rooted in fundamental
exponential family theory and applicable to any finite state space 
exponential family model.  Thus the theory presented in this section is from
the perspective of general exponential families.

\section{Exponential Family Theory}
Much of the basic background for exponential families has already been
covered in Section~\ref{S:ERGM setup}; our focus is on regular exponential
families on a finite sample space $\YY$  with log likelihood \eqref{E:loglike}.
As noted earlier, when the sample space $\YY$ is even moderately large,
the cumulant function $c(\eta)$ involves a summation that may be prohibitively 
expensive to evaluate.  For example, the sample space $\YY$ for an Ising model 
defined on a $32\times 32$ square lattice where each entry takes values of 0 or 1 
has $2^{1024} \approx 10^{300}$ elements.  
A loop with this many iterations takes too long no matter how programmed.

A useful property of all exponential families \cite[p.~27]{TPE2} on which we rely 
heavily is that 
\begin{align*}
	\E_\eta(g(Y)) &= \nabla c(\eta)	\\
	\Var_\eta(g(Y)) &= \nabla^2 c( \eta ).
\end{align*}

Thus we can express first and second derivatives of the log likelihood \eqref
{E:loglike} and Fisher information, $I
(\eta)$, as
\begin{align}
	\nabla \ell( \eta ) &= g(y) - \E_\eta g(Y) \label{E:nabla ell} \\
	\nabla^2 \ell( \eta ) &=  - \Var_\eta g(Y) \label{E:nabla2 ell} \\
	\I(\eta) &= -\E_\eta \nabla^2 \ell (\eta ) = \Var_\eta g(Y) \label{E:FI}
\end{align}
and thereby avoid evaluation of the problematic cumulant function $c$.


By the strict convexity of the log likelihood function ensured by \eqref{E:nabla2 
ell}, the global 
maximum, if it is exists, is attained when $\eta$ is such that $\nabla \ell( \eta ) = 
0$, or, by \eqref{E:nabla ell},
\begin{align}
	\E_\eta g(Y) = g(\yobs). \label{E:Observed-Expected}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Mean value parameterization}

An alternative parameterization of an exponential family is the mean value 
parameterization.  For each natural parameter $\eta$, we can define the mean parameter 
$\mu$ such that
\begin{align*}
	\mu = \E_\eta g(Y).
\end{align*}
The space for the mean value parameterization is the same as the convex support of the 
natural statistics and thus lends itself to easier interpretability  \citep
{Handcock:degeneracy, Rinaldo:2009}.  In particular, $\mu = g(\yobs)$ is the MLE in 
the mean value parameterization.  \citeauthor{Handcock:degeneracy} observed that mean 
value parameters located too close to the boundary of the convex support correspond to 
degenerate distributions.  

There exists a one-to-one mapping between a natural parameter and its mean value 
parameter, and one can calculate mean value parameters from natural parameters. In 
general there is no simply way to get the natural parameter value from the mean value 
parameter (if there was, then finding MLEs would be very easy and we wouldn't need all 
these algorithms!).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Convex Analysis}
The issue of MLE existence in the conventional sense in an exponential family is 
closely tied to the geometric properties of 
the convex support of the model \citep{Barndorff, Geyer:gdor, Rinaldo:2009}.  We 
describe the relevant theory from convex analysis as it pertains to the case of 
exponential families.

A \emph{convex polytope} $C$ is the convex hull of a finite set of points $V$,
\begin{align*}
	C = \con( V ).
\end{align*}
where [\textbf{DEFINE pos().}]

  By the Minkowski-Weyl theorem \citep[Theorem 19.1]{Rockafellar:1970}, this convex 
set can equivalently be represented as the intersection of a finite collection of 
closed half-spaces.  These two representations of a convex polyhedron are referred to 
as the \emph{V-representation} and \emph{H-representation}, respectively.  
%The V-representation of a convex polyhedron $C$ is the set of all linear combinations
%\begin{align*}
%	\sum_{i \in E \cup I} b_i \alpha_i
%\end{align*}
%where $\alpha_i$ are vectors, $b_i$ are scalars, $E$ and $I$ are disjoint finite sets 
%such that
%\begin{align*}
%	b_i \geq 0, \quad i \in E \cup I
%\end{align*}
%and if $I$ is nonempty
%\begin{align*}
%	\sum_{i \in I} b_i = 1.
%\end{align*}

The H-representation can be expressed as the solution set of a finite set of linear 
equations and inequalities,
\begin{align*}
	C = \{x: Ax \leq b \},
\end{align*}
where $A$ is a matrix and $b$ a vector.

The \emph{relative interior} of a convex set $C$, denoted $\rint C$, is the interior 
relative to its affine hull.  
\textbf{[WHAT ELSE? bd()?  int()?  pos()?]}

A nonempty \emph{face} of a convex polyhedron $C$ is a convex subset of $C$ such that 
every line segment in $C$ with a relative interior point in $F$ has both end points in 
$F$ \citep{Rockafellar:1970}.  It is itself a convex polyhedron.
A \emph{proper} face is a face that is not the empty set or $C$, and 
\emph{facets} are proper faces of the highest dimension.

The \emph{tangent cone} of a convex set $C$ at a point $x \in C$ is
\begin{align*}
	T_C(x) = \cl\{s(w-x):w \in C \text{ and } s \geq 0 \},
\end{align*}
where $\cl$ denotes the closure operation \citep[Theorem 6.9]{Rockafellar}.  

The \emph{normal cone} of a convex set $C$ in $\RR^d$ at a point $x \in C$ is 
\begin{align*}
	N_C(x) = \{ \delta \in \RR^d: \inner{w-x,\delta} \leq 0 \text{ for all } w \in C 
\}.
\end{align*}

Tangent and normal cones are polars of each other, that is, each determines the other.  
The normal cone at $x$ can be defined in terms of the tangent cone at $x$ by
\begin{align*}
	N_C(x) 	&= \{ w \in \RR^d: \inner{ w, v } \leq 0 \text{ for all } v \in T_C(x) \}.
\end{align*}

DEFINE \emph{direction of recession}, \emph{direction of constancy}.  \textbf{get from 
\citep{Rockafellar:1970}.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MLE existence in exponential families}
We now present two equivalent approaches to determine the existence of an MLE
in the conventional sense.  

\subsection{Well-known condition}
The well-known condition for the existence of the MLE \citep{Barndorff, Brown:1986} 
relates the relative location
of the observed statistic to the boundaries of the convex support
and is formally stated as follows:
\begin{theorem} \label{Thm:MLE rint}
Under the conditions [CONDITIONS], the MLE exists in the conventional sense and is 
unique if and only if 
$g(\yobs) \in \rint(C)$.
\end{theorem}

From this one can conclude whether or not the MLE exists in the conventional sense
if $C$ is known using linear programming.

\subsection{Approach of \citet{Geyer:gdor}}
\citet{Geyer:1990,Geyer:gdor} present theory that relates MLE existence to
 not only the boundary of the convex support, but also to how the log likelihood behaves along
directions orthogonal to the boundaries.

We first revisit an older result for which we have developed a simpler proof:
\begin{theorem}[Theorem 2.2 in \citep{Geyer:1990}]\label{Thm:e_c}
\begin{align*}
e^{c(\eta + s \delta) - bs} &\to 
		\begin{cases} 
			0 									& b > \sigma_c(\delta) \\
			e^{c(\eta)} P_\eta(g(Y) \in H_\delta ) 	& b = \sigma_c(\delta) \\
			+\infty								& b < \sigma_c(\delta)
		\end{cases}
& \text{as } s \to +\infty.
\end{align*}
where $\delta$ is a non-zero direction, $C$ the convex support, and
\begin{align*}
	\sigma_C (\delta) &= \sup_{g(y) \in C} \inner{ g(y), \delta} \\
	H_\delta &= \set{w: \inner{w, \delta} = \sigma_C(\delta) }.
\end{align*}
\end{theorem}
Here, $H_\delta$ is the supporting hyperplane to the set $C$ with normal vector $\delta$.

\begin{proof}
\textbf{Case: $b = \sigma_C(\delta)$.}

Starting with \eqref{E:kappa},
\begin{align*}
	e^{c(\eta)} = \kappa(\eta) = \int  e^{\inner{\eta, g(y)}} \, d\mu(y),
\end{align*}
so that
\begin{align*}
	e^{ c(\eta + s \delta ) - bs } &= \int e^{\inner{\eta + s \delta,g(y)} - bs } \, d\mu(y). \\
					&= \int e^{\inner{\eta, g(y)}  + s [ \inner{ g(y), \delta} - b ] } \, d\mu(y). 
\end{align*}
Multiplying by $\frac{f_\eta(y)}{f_\eta(y)}$,
\begin{align*}
	e^{ c(\eta + s \delta ) - bs } &= \int e^{\inner{\eta,g(y)}  + s [ \inner
{g(y),\delta} - b ] }  \frac{ f_\eta(y) }{ e^{\inner{\eta,g(y)} - c(\eta)} }\, d\mu(y) \\
	&= \int e^{  s [ \inner{g(y),\delta} - b ] + c(\eta) }  f_\eta(y) \, d\mu(y) \\
	&= \E_\eta e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) }.
\end{align*}
%What happens as $s \to +\infty$?  We would like to reverse the order of taking the 
%limit and expectation.  Fortunately, we have the monotone convergence theorem.  
The monotone convergence theorem can be applied to reverse the order of the limit and expectation
for monotone sequences of random variables.  For $\inner{g(Y), \delta} \leq b$, we 
have a monotonically decreasing
 sequence of random variables and for $\inner{g(Y), \delta} > b$, the sequence is increasing.  Thus, 
\begin{align*}
	\lim_{s\to \infty} \E_\eta e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } &= E_
\eta \lim_{s\to \infty} e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) }. 
\end{align*}
Ignoring the expectation and examining the just limit component of the above,
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } &= 
			\begin{cases} 
			0 								& \inner{g(Y),\delta} < b \\
			e^{c(\eta)} 			 			& \inner{g(Y),\delta} = b \\
			+\infty							& \inner{g(Y),\delta} > b.
		\end{cases}
\end{align*}
In this first case that we consider, $b = \sigma_C(\delta) = \sup_{g(y) \in C}
\inner{g(y),\delta}$, so $\inner{g(Y),\delta}$ can never be greater than $b$ and 
thus the $+\infty$ outcome above is not possible.  We can rewrite the result above 
succinctly as
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } 
		&= I(\inner{g(Y), \delta} = b ) e^{c(\eta)} 
		= I( g(Y) \in H_\delta ) e^{c(\eta)}.
\end{align*}
Then returning to the original expression of interest,
\begin{align*}
	\lim_{s\to \infty} e^{c(\eta + s\delta) - bs} 
	= \lim_{s\to \infty} \E_\eta e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } 
	&= e^{c(\eta)} P( g(Y) \in H_\delta ).
\end{align*}

\textbf{Case: $b > \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\eta) }P(g(Y)\in H_\delta) \right ) \cdot 0 = 0.
\end{align*}

\textbf{Case: $b < \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\eta) }P(g(Y)\in H_\delta) \right ) \cdot \left ( + \infty \right ) 
= + \infty.
\end{align*}
\end{proof}

We now define directions of constancy and recession in terms of Theorems 1
and 2 of \citep{Geyer:gdor} which we state without complete proofs.

\begin{theorem}[Theorem 1 (Direction of Constancy) from \citet{Geyer:gdor}] \label{Thm:DOC}
For a full exponential family with
\begin{itemize}
\item log likelihood function $\ell(\eta)$ as in \eqref{E:loglike},
\item natural parameter space $\Xi$ as in \eqref{E:paramspace},
\item natural statistic $g(Y)$,
\item observed data $\yobs$ such that $g(\yobs) \in C$, the convex 
support,
\end{itemize}
the following are equivalent:
\begin{enumerate}
\item For all $\eta \in \Xi$, the function $s \mapsto \ell( \eta + s\delta)$ is 
constant on $\RR$ \cite[Theorem 1(b)]{Geyer:gdor}.
\item The parameter values $\eta$ and  $\eta + s\delta$ correspond to the same 
probability distribution for all $\eta \in \Xi$ and all real $s$ \cite[Theorem 1(d)]
{Geyer:gdor}.
\item $\inner{g(Y) - g(\yobs),\delta} = 0$ almost surely for all distributions in the 
family \cite[Theorem 1(f)]{Geyer:gdor}.
\item $\delta \in N_C(g(\yobs))$ and $-\delta \in N_C(g(\yobs))$ \cite[Theorem 1(g)]
{Geyer:gdor}.
\item $\inner{w,\delta} = 0$ for all $w \in T_C(g(\yobs))$ \cite[Theorem 1(h)]
{Geyer:gdor}.
\end{enumerate}
\end{theorem}
Any vector $\delta$ that satisfies any of the conditions above is called a 
\emph{direction of constancy} of the log likelihood.
The set of all directions of constancy is called the \emph{constancy space} of 
the log likelihood, which is a vector subspace.

\begin{theorem}[Direction of Recession: Theorem 3 from \cite{Geyer:gdor}] \label{Thm:DOR}
For a full exponential family with the same setting as Theorem~\ref{Thm:DOC},
%\begin{itemize}
%\item log likelihood function, $\ell(\eta)$, described by \eqref{E:loglike},
%\item natural parameter space, $\Xi$,
%\item natural statistic, $g(Y)$,
%\item observed value of the natural statistic, $g(\yobs)$, such that $g(\yobs) \in C$, the convex 
%support,
%\end{itemize}
the following are equivalent:
\begin{enumerate}
\item For all $\eta \in \Xi$, the function $s \mapsto \ell( \eta + s\delta)$ is 
nondecreasing on $\RR$ \cite[Theorem 3(b)]{Geyer:gdor}.
\item $\inner{g(Y) - g(\yobs),\delta} \leq 0$ almost surely for all distributions in 
the family. \cite[Theorem 3(d)]{Geyer:gdor}.
\item $\delta \in N_C(g(\yobs))$ \cite[Theorem 3(e)]{Geyer:gdor}.
\item $\inner{w,\delta} \leq 0$ for all $w \in T_C(g(\yobs))$ \cite[Theorem 3(f)]
{Geyer:gdor}.
\end{enumerate}
\end{theorem}
Any vector $\delta$ that satisfies any of the conditions above is called a 
\emph{direction of recession} of the log likelihood.  Every direction of constancy
is a direction of recession.

\citeauthor{Geyer:gdor} uses Corollary 2.4.1 in \citep{Geyer:1990} to prove the 
equivalence of conditions 1 and 2 of Theorem~\ref{Thm:DOR}.  Here we present
a more accessible proof without the use of this corollary.  It is the equivalence
of these two conditions the relates the behavior of the log likelihood function
to the convex support of the distribution.
%The proof requires the following lemma, which a PROOF WILL BE NEEDED.
%$\ell(\theta+s\delta)$ is non-dereasing if and only if $\lim_{s \to \infty} \ell
%(\theta+s\delta) > -\infty$.  See Theorem 8.6 in \citep{Rockafellar}
\begin{proof}
\textbf{From $2 \to 1$}:
Assume $\inner{g(Y)-g(\yobs), \delta} \leq 0$.  Take expectations of both sides
with respect to the distribution index by the parameter value $\eta + s \delta$:
\begin{align} \label{E:exp_gy}
\inner{ \E_{\eta + s \delta} g(Y) - g(\yobs), \delta} \leq 0.
\end{align}
Now, taking the derivative of $\ell( \eta + s\delta)$ with respect to $s$,
\begin{align*}
\deriv{\ell( \eta + s \delta)}{s} &= \deriv{}{s} 
			\left ( \inner{g(\yobs), \eta+s \delta} - c(\eta+s\delta)  \right )\\
%	&= \inner{g(\yobs), \delta} - \deriv{}{s} c(\eta+s\delta) \\
	&= \inner{g(\yobs), \delta} - \inner{ \E_{\eta+s\delta}g(Y),\delta }\\
	&= - \inner{ \E_{\eta+s\delta}g(Y) - g(\yobs),\delta },
\end{align*}
which is greater than or equal 0 by \eqref{E:exp_gy}.
Thus $\ell(\eta+s\delta)$ is a non-decreasing function of $s$.

\textbf{From $1 \to 2$}:
\begin{align}
	\ell( \eta+s\delta) &= \inner{g(\yobs), \eta+s\delta} - c(\eta+s\delta) \notag \\ 
	&= \inner{g(\yobs), \eta} + s \inner{g(\yobs),\delta} -bs +bs - c(\eta+s\delta) \notag \\ 
	&= \inner{g(\yobs), \eta} + s [\inner{g(\yobs),\delta} -b]  - \log e^{c(\eta+s\delta) -bs}. \label{E:expanded ell}
\end{align}
%Theorem~8.6 in \citep{Rockafellar:1970} says that $\ell(\theta+s\delta)$ 
%is non-decreasing if and only 
%if $\lim_{s \to \infty} \ell(\theta+s\delta) > -\infty$.  This implies that in the 
%above expression, for the right-most term, $b \geq \sigma_c(\delta)$, and also that

By assumption, $\ell(\eta + s\delta)$ is a non-decreasing function of $s$.  
Then for $\ell(\eta ) > -\infty$, $\ell(\eta + s\delta) \geq \ell(\eta )  > -\infty$
for any $s>0$ and thus $\lim_{s \to \infty} \ell(\eta+s\delta) > -\infty$.
By Theorem~\ref{Thm:e_c}, in order for the righthand side of \eqref{E:expanded ell} 
to be greater than $-\infty$ as $s \to +\infty$ requires
\begin{align*}
	\inner{g(\yobs), \delta} - b \geq 0,
\end{align*}
and
\begin{align*}
	b \geq \sigma_C(\delta). % \sup_{g(y)\in C} \inner{ g(y), \delta }.
\end{align*}
Then
\begin{align*}
	\sigma_C(\delta)  - \inner{g(\yobs), \delta} \leq b - \inner{g(\yobs), \delta}  \leq 0,
%#	\inner{y, \delta} - \inner{Y_{max},\delta } \geq 0.
\end{align*}
and recalling that $\sigma_C(\delta) = \sup_{g(y) \in C} \inner{g(y), \delta}$, 
we conclude that
\begin{align*}
	\inner{g(Y) - g(\yobs),\delta } \leq 0
\end{align*}
almost surely.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Theorems~\ref{Thm:DOC} and \ref{Thm:DOR} induce the following criteria about the
existence of the MLE in the conventional sense:

\begin{theorem}[Theorem 4, \citep{Geyer:gdor}] \label{Thm:MLE existence}
For a full exponential family with the setting as Theorem~\ref{Thm:DOC}, the 
following are equivalent 
\begin{enumerate}
\item the MLE exist.
\item Every direction of recession is a direction of constancy.
\item $N_C(g(\yobs))$ is a vector subspace.
\item $T_C(g(\yobs))$ is a vector subspace.
\end{enumerate}
\end{theorem}

Thus the MLE does not exist in the conventional sense if a there exists a vector 
$\delta$ that is a direction of recession exists but not a direction of constancy.  
Such a direction is called a \emph{generic direction of recession} (GDOR).
Thus if $\delta \in \rint N_C(g(\yobs))$, $\delta$ is a GDOR.  A GDOR exists if and 
only if the MLE does not exist.

Theorem~\ref{Thm:MLE rint} relates the non-existence of the MLE to the location
of $g(\yobs)$ relative to the boundary of $C$.  Here, Theorems \ref{Thm:DOC}, \ref{Thm:DOR}, and \ref{Thm:MLE existence} relate the non-existence of the MLE
to a strictly increasing log likelihood function.  This in turn
occurs when $g(\yobs)$ is such that $\inner{g(Y) - g(\yobs)} < 0$.  This is
exactly the criteria that puts $g(\yobs)$ on the boundary of the convex support,
thus agreeing with Theorem~\ref{Thm:MLE rint}.

\begin{corollary}[Corollary 5, \citep{Geyer:gdor}] \label{Cor:strictly increasing}
For a full exponential family with the setting as Theorem~\ref{Thm:DOC}, if $\delta$ is a
GDOR, then for all $\eta \in \Xi$, the function $s \mapsto \ell(\eta+s\delta)$ is strictly
increasing on the interval where it is finite.
\end{corollary}

This corollary is clearly implied by Theorems \ref{Thm:DOC} and \ref{Thm:DOR}; if
$\delta$ is a GDOR, the $\ell(\eta+s\delta)$ is nondecreasing with respect to $s$ by 
Theorem \ref{Thm:DOR} but cannot be constant by Theorem \ref{Thm:DOC}.  Therefore,
it must be strictly increasing.


%%%%%%%%%%%%%%%%%%%%%

\subsection{Limiting conditional model}
When the MLE does not exist for an exponential family in the conventional sense, there 
exists a GDOR along which the log likelihood goes to $+\infty$.  The behavior of the 
density function of the distribution is described in the following theorem:

\begin{theorem}[Theorem 6 from \citet{Geyer:gdor}] \label{Thm:LCM}
For a full exponential family with the setting of Theorem~\ref{Thm:DOC}, and 
additionally,
\begin{enumerate}
\item density function $f_{\eta}(y)$ defined by \eqref{E:ERGM},
\item direction of recession, $\delta$,
\item $H = \{ w \in \RR^d: \inner{ w-g(\yobs), \delta } = 0 \}$,
\item $P( g(Y) \in H) > 0$ for some distribution in the family,
\end{enumerate}
then for all $\eta \in \Xi$
\begin{align} \label{E:LCM}
\lim_{s \to \infty} f_{\eta+s\delta}(y) = 
			\begin{cases} 
			0 								& \inner{g(Y) - g(\yobs), \delta} < 0 \\
			\frac{f_\eta(y)}{P_\eta(g(Y) \in H)} 	& \inner{g(Y) - g(\yobs),
\delta} = 0 \\
			+\infty							& \inner{g(Y) - g(\yobs), \delta} > 0.
		\end{cases}
\end{align}
If $\delta$ is not a direction of constancy (and thus is a GDOR), 
then $s \mapsto P_{\eta+s\delta}( g(Y) \in H)$ is continuous and 
strictly increasing, and $P_{\eta+s\delta}( g(Y) \in H) \to 1$ as $s \to \infty$.
\end{theorem}


%%%%% BEGIN PROOF
\begin{proof}
By Theorem~\ref{Thm:DOR}, if $\delta$ is a direction of recession, then 
$\inner{g(Y) - g(\yobs), \delta} \leq 0$ almost surely, which implies 
that $\inner{g(Y), \delta} \leq \inner{g(\yobs), \delta}$.  
So, the largest value that $\inner{g(Y), \delta}$ can take is 
$\inner{g(\yobs), \delta}$.  Using our earlier notation, 
\begin{align*}
\sigma_C(\delta) = \sup_{g(y) \in C}\inner{g(y), \delta} = 
\inner{g(\yobs), \delta}.
\end{align*}

From \eqref{E:ERGM}, we can express $f(_{\eta+s\delta}(y)$ as
\begin{align*}
 f_{\eta+s\delta}(y) &= e^{ \inner{\eta+s\delta,g(y)} - c(\eta+s\delta)  } \\
 &= e^{ \inner{\eta,g(y)} -c(\eta)+c(\eta) + s\inner{\delta,g(y)} - c(\eta+s\delta) } \\
 	&= f_\eta(y) \frac{e^{c(\eta)}}{e^{ c(\eta+s\delta) - \inner{g(y),\delta}
s } }.
\end{align*}
The denominator in the fraction above has the form of the starting expression in 
Theorem~\ref{Thm:e_c} with $\inner{g(y),\delta}$ in place of $b$.
Thus we can take the limit of $f(_{\eta+s\delta}(y)$ as
$s \to +\infty$ by applying Theorem~\ref{Thm:e_c},

\begin{align*}
	f_{\eta+s\delta}(y) = f_\eta(\eta) \frac{e^{c(\eta)}}{e^{ c(\eta+s
\delta) - \inner{g(y),\delta}s } } 
	\to	
			\begin{cases} 
			0 					& \inner{g(Y),\delta} < \inner{g(\yobs),\delta} \\
			\frac{f_\eta(y)}{P_\eta(g(Y) \in H)} 	& 
								\inner{g(Y),\delta} = \inner{g(\yobs),\delta} \\
			+\infty				& \inner{g(Y),\delta} > \inner{g(\yobs),\delta}.
			\end{cases}
\end{align*}
This gets us \eqref{E:LCM}.  We next show the behavior of 
$P_{\eta+s\delta}(g(Y) \in H)$ as $s \to +\infty$ when $\delta$ is a GDOR.

\begin{align*}
 P_{\eta+s\delta}(g(Y) \in H) &= \int_H e^{\inner{g(y), \eta+s\delta} - c(\eta
+s\delta)} \, d\mu(y) \\
		&= \int_H  \frac{e^{c(\eta)}}{e^{c(\eta+s\delta)-\inner{g(y),\delta}s}} 
					f_\eta(\eta) \, d\mu(y)\\
		&= \E_\eta  \left ( I_H \frac{e^{c(\eta)}}{e^{c(\eta+s\delta)-\inner{g(y),\delta}s}} \right )
\end{align*}
Since the indicator function $I_H$ evaluates to zero unless 
$\inner{ g(y), \delta} = \inner{ g(\yobs), \delta} $,  
this expression can pulled out of the expectation along with other constants, so that 
\begin{align*}
		 P_{\eta+s\delta}(g(Y) \in H)
		 &= \frac{e^{c(\eta)} }{ e^{ c(\eta+s\delta) -s\inner{g(\yobs),\delta} } }
		 \E_\eta  I_H   \\
		 &= \frac{e^{c(\eta)} }{ e^{ c(\eta+s\delta) -s\inner{g(\yobs),\delta} } }
		 P_\eta  (g(Y) \in H). 
		 \end{align*}
 
Then taking the limit of $s \to +\infty$ via Theorem~\ref{Thm:e_c} gives
\begin{align*}
 P_{\eta+s\delta}(g(Y) \in H)
		&\to \frac{e^{c(\eta)}}{ e^{c(\eta)} P_\eta (g(Y) \in H) } P_\eta (g(Y)\in H) = 1
 \end{align*}
 as desired.   
\end{proof}

%%%%% END PROOF

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Remarks on Theorem~\ref{Thm:LCM}}
Let $W = I(g(Y) \in H)$. Noting that $\inner{g(Y),\delta} > \inner{g(\yobs),\delta}$
has zero probability by 
Theorem~\ref{Thm:DOR}(2) and thus need not be considered, the  
mixed density $f_\eta(y,w)$ can be expressed
\begin{align*}
 f_\eta(y,w) &= f_\eta(y) P(W=w \mid Y )\\
	&= \begin{cases}
 			f_\eta(y)\cdot P(W=w \mid g(Y) \notin H) \quad &\text{for } g(Y) \notin H \\
			f_\eta(y)\cdot P(W=w \mid g(Y) \in H) \quad &\text{for } g(Y) \in H\\
 		\end{cases}\\
	&= \begin{cases}
 			f_\eta(y)\cdot (1-w) \quad &\text{for $g(Y) \notin H$ (or $w = 0$) }\\
			f_\eta(y)\cdot w \quad &\text{for $g(Y) \in H$ (or $w = 1$)}\\
 		\end{cases}\\
		&=f_\eta(y).
\end{align*}
Then the right-hand side of \eqref{E:LCM} can be viewed as a conditional density of a 
distribution with parameter $\eta$ given $g(Y) \in H$ and hence expressed as $f_{\eta}
(\, \cdot\,  | g(Y) \in H)$.  
\hl{
Now I'm confused.  The event $g(Y) \in H$ corresponds to 
$\inner{g(Y),\delta} = \inner{g(\yobs),\delta}$ doesn't it?  Then doesn't the conditional
distribution given $g(Y) \in H$ mean that $g(Y) \in H$???  Isn't there some redundancy/double
counting going on here?  
Note that Charlie use $f(\omega$) and $Y(\omega)$ in his paper.  MAYBE I should be used
lower cased $y$ in the ranges, as in, $g(y) \in H$.  Charlie could not do this because
for him $y$ means the same thing as my $g(\yobs)$, so little $y$ is ambiguous.
}

The log likelihood for $f_\eta( \, \cdot\,  | g(Y) \in H)$, $\eta \in \Xi$, can be expressed as
\begin{align*}
%	\ell_{LCM}(\eta) &= 
	\inner{\eta, g(\yobs)} - c(\eta) - \log P_\eta(g(Y) \in H) 
%			&= \ell(\eta) - \log P_\eta(g(Y) \in H)
\end{align*}
which clearly has exponential family form with the same natural parameter $\eta$ and 
statistic $g(y)$ as the original exponential family.  The cumulant function is different,
however, and thus the family may not be full; the full family containing this exponential family,
\begin{align*}
\{ f_{\eta}(\, \cdot\,  | g(Y) \in H) \text{ for }  \eta + \gamma: \eta \in \Xi \text
{ and } \gamma \in \Gammalim \},
\end{align*}
where $\Gammalim$ is the constancy space of $f_{\eta}( \, \cdot\,  | g(Y) \in H)$,
is called the \emph{limiting conditional model} (LCM) \citep{Geyer:gdor}.  

We are primarily interested in the case when $\delta$ is a GDOR, in which case
we can succinctly summarize the result of Theorem~\ref{Thm:LCM} as
\begin{align*}
\lim_{s \to \infty} f_{\eta+s\delta}(y) = f_{\eta}( y | g(Y) \in H).
\end{align*}

Because $\ell_{LCM}(\eta) = \ell(\eta) - \log P_\eta(g(Y) \in H)$,
\begin{align*}
	\ell(\eta) < \ell_{LCM}(\eta).	
\end{align*}
The inequality above is strict due to Corollary~\ref{Cor:strictly increasing} which states that the log likelihood
is strictly increasing when $\delta$ is a GDOR and so $P_\eta(g(Y) \in H)$ cannot be equal to 1.
Thus even though the MLE does not exist for the original family, its log likelihood
$\ell(\eta)$ is bounded by the LCM log likelihood for which a maximizer will exist.

An MLE for this new model must exist for polyhedral convex support $C$, 
since $g(\yobs)$ cannot lie on the boundary of 
the convex support which was itself determined by $g(\yobs)$ lying in the interior of 
it. 
NOTE: face is $C \cap H$, not $H$ standalone.

The last part of this theorem says that for the sequence of exponential families with parameter 
value $\theta+s\delta$, the probability of $Y$ occurring on the plane $H$ is strictly 
increasing, and in fact, going to 1.  (This is what Charlie is referring to when he 
says ``the probability is accumulating on the boundary".)  So, $H$ 
actually coincides with the boundary of the convex hull
(to be precise, the face is $C \cap H$) and the observed data $g(\yobs)$ sits 
is on this boundary.  

The exponential family with the \emph{full} natural parameter space.  
\textbf{
``If an MLE exists 
for the LCM, then it maximizes the likelihood in \hl{the family} that is the union of the 
LCM and the original family, and it maximizes the likelihood in the family that is 
the set of all limits of sequences of distributions in the original family.  When 
this happens, we say we have found an MLE in the Barndorff-Nielsen completion of the 
original family."
}

By Theorem~\ref{Thm:LCM}, when $\delta$ is a GDOR and $\etaLCM$ is the MLE of the LCM, $\gamma \in \Gammalim$, then
\begin{align*}
	\lim_{s \to +\infty} \ell(\etaLCM + \gamma + s\delta) = \sup_{\RR^d} \ell(\eta).
\end{align*}

When no GDOR exists and the MLE exists in the conventional sense, then the 
original family is the LCM, corresponding to $\delta=0$ in Theorem~\ref{Thm:LCM}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{summary}
By the well-known theorem of MLE existence for exponential families (Theorem~\ref{Thm:MLE rint}), when the observed statistic, $g(\yobs)$, is 
on the boundary of the convex support $C$, the MLE does not exist in the conventional 
sense.
Here, we have expressed this result with more detail: by Theorem~\ref{Thm:MLE existence}, there must exist a direction of recession, $\delta$, that is not a direction of constancy.  

According to Theorem~\ref{Thm:LCM}, as we move in the natural parameter space in the 
direction of $\delta$ that is a GDOR, the distribution with this parameter value will 
put increasing probability on points for which the natural statistics, $g(Y)$, land 
on the face $H \cap C$ on which $g(\yobs)$ lies.  This face is orthogonal to $\delta
$ by construction and contains the boundary of the convex support.  Samples generated 
from these models will therefore increasingly fall on the face on which $g(\yobs)$ 
lies as $\eta$ increases in the direction of $\delta$.  Theorem~\ref{Thm:LCM} says 
that eventually, all generated sample points will fall on this boundary.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{One-sided confidence interval}

Consider the probability distributions defined with log likelihood $\ell( \eta + s 
\delta)$ as defined 
by \eqref{E:loglike}, where $s$ is a real scalar, $\delta$ is a GDOR and 
we have found $\etaLCM$, the MLE in the LCM, and $\gamma \in \Gammalim$
(and hence $\etaLCM + \gamma$ also indexes the same LCM MLE distribution).  Then 
as $s$ goes from $-\infty$ to $+\infty$, the probability of observing $g(Y) \in H$ 
goes from zero to one in a strictly increasing manner, per Theorem~\ref{Thm:LCM}.  Thus we can find the unique $s$, call it $\hat{s}$, that makes 
this probability 0.05.  That is, we find the unique $s$ such that
\begin{align*}
	P_{\etaLCM + \gamma + s\delta}\left (g(Y) \in H \right ) = \alpha.
\end{align*}
Then $[\hat{s}, +\infty)$ is a 95\% confidence interval for 
the scalar parameter 
$s$, and, in turn, $[\etaLCM + \gamma + \hat{s}\delta, +\infty)$ gives 
a 95\% confidence interval for $\etaMLE$.

\subsection{Confidence interval for mean value parameterization}
The confidence intervals calculated for the natural parameters can of 
course be converted to those for mean value parameters, where the observed
data, and hence MLE, are known.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programming LCM theory}
\text{rcdd} \citep{rcdd}

\subsection{Linearity}\label{S:linearity}
\citet{Geyer:gdor} defines the \emph{linearity} of a set of points $V$ to be
\begin{align*}
	L = \{ w \in V: -w \in \con( \pos V) \}.
\end{align*}
It is the subspace with bases determined by $V$.

We use this to find the face of the polyhedral convex support on which the observed 
statistic lies in the interior of.  The function \texttt{linearity} in the \texttt
{rcdd} package can take as input the rays directed from observed statistic to sample 
points generated via MCMC from the distribution with parameter value of interest.  
From these, it determines the rays that form the basis of a vector subspace.  Of 
course we are not interested in the vector subspace; we need only take the points 
corresponding to these bases as our empirical face $F$.  By construction, the observed 
statistic lies in the interior of $F$.

%Let $\widetilde{T}_C(x)$ be the empirical tangent cone at the point $x$ constructed 
%from a set of points $W$ all in $C$.  That is,
%\begin{align*}
%	\widetilde{T}_C(x) = \{ w - x: w \in W\}.
%\end{align*}
%Then for $V = \widetilde{T}_C(g(\yobs))$, the linearity corresponds to the set of 
%points from $W$ (shifted by $g(\yobs)$) that form what we refer to as the empirical 
%face $F$.  
%In our algorithm, we use the MCMC sample points for $W$.  \citet{Geyer:gdor} provides 
%the function \texttt{linearity} in the R package \texttt{rcdd} to do this computation.

%Consider the case when $g(\yobs) \in \rintr C$.  Then the MLE exists, and the 
%linearity will correspond to all the points $W \in C$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Subsampling}
%Once our algorithm has determined that the observed data is on the boundary and thus 
%the MLE does not exist in the conven
Our algorithm depends on being able to to subsample from the MC sample points when 
optimizing in the LCM, restricting to the sample points that comprise the support of 
this model.  The subsampling operation itself is straightforward since we determined 
empirically the face that forms this support.  

However, it is not obvious that when we draw $g(Y_1), \ldots, g(Y_m)$ from the 
original model with parameter value $\eta$ that the subsample restricted to the 
support is in fact a sample from the LCM with parameter value $\eta$.  This is a 
convenient consequence of Theorem~\ref{Thm:LCM}: by this stage in the algorithm, the 
majority of sample points (greater than 60\%) are on the face.  Then $s$ in the 
expression $P_{\eta + s \delta}(g(Y) \in H)$ is a large value.  But since the LCM 
density is the limit of the density of the original model as $s$ goes to $+\infty$, 
the distribution of the LCM is accordingly well approximated.  (\textbf{BUT WELL 
ENOUGH?})
