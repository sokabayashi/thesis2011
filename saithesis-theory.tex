All of the parameter estimation issues discussed thus far for ERGMs are relevant to 
the larger class of discrete state space exponential family models.
The latter are commonly used to model phenomena with dependent structure, 
where the outcomes of the response variable of interest are in fact dependent on one 
another.  For example, the Ising 
model \citep{Ising,Potts} is an exponential family model that has been used to model 
ferromagnetism, where neighboring 
pixels (representing atoms in a crystal lattice) are more likely to have the same 
spin orientation.  
%A realized 
%sample from this model is depicted in Figure~\ref{F:pottsimage},   
%We explore this model further in Section~\ref{S:Examples:Ising}.
Other examples of phenomena with dependent structure modeled with exponential 
families include
plant ecology \citep*{Besag:1974,Besag:1975}, DNA fingerprint data \citep{Geyer:1992}
 and the lifetime fitness of plants \citep*{Shaw:2008}.

Although motivated by ERGMs, the methods we propose are rooted in fundamental
exponential family theory and applicable to any 
exponential family model.  Thus the theory presented in this section is from
the perspective of this more general setting; in fact, only Theorem~\ref{Thm:C-H} and
Corollary~\ref{Cor:LCM MLE exists} require 
the additional assumption of a finite state space.

\section{Exponential family theory} \label{S:Expfam theory}
Much of the fundamental background for exponential families has already been
covered in Section~\ref{S:ERGM setup}: our interest is in the properties of
 regular exponential
families on a finite sample space $\YY$  with log likelihood \eqref{E:loglike}.
As noted earlier, when the sample space $\YY$ is even moderately large,
the cumulant function $c(\eta)$ involves a summation that may be prohibitively 
expensive to evaluate.  
%For example, the sample space $\YY$ for an Ising model 
%defined on a $32\times 32$ square lattice where each entry takes values of 0 or 1 
%has $2^{1024} \approx 10^{300}$ elements.  
%A loop with this many iterations takes too long no matter how programmed.

A useful property of all exponential families \cite[p.~27]{TPE2} when 
$\eta$ is in the interior of $\Xi$ is that 
\begin{align*}
	\E_\eta(g(Y)) &= \nabla c(\eta)	\\
	\Var_\eta(g(Y)) &= \nabla^2 c( \eta ).
\end{align*}

Thus we can express first and second derivatives of the log likelihood \eqref
{E:loglike} and Fisher information $I(\eta)$ as
\begin{align}
	\nabla \ell( \eta ) &= g(y) - \E_\eta g(Y) \label{E:nabla ell} \\
	\nabla^2 \ell( \eta ) &=  - \Var_\eta g(Y) \label{E:nabla2 ell} \\
	\I(\eta) &= -\E_\eta \nabla^2 \ell (\eta ) = \Var_\eta g(Y) \label{E:FI}
\end{align}
and thereby avoid evaluation of the problematic cumulant function $c$.


Strict concavity of the log likelihood function is assured by \eqref{E:nabla2 
ell} being strictly positive definite, which in turn is assured by the natural statistic
not being concentrated on a hyperplane.  The global 
maximum, if it is exists, is attained when $\eta$ is such that $\nabla \ell( \eta ) = 
0$, or, using \eqref{E:nabla ell}, by setting ``expected equal to observed"
\begin{align}
	\E_\eta g(Y) = g(\yobs). \label{E:Observed-Expected}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Convex analysis} \label{S:Convex analysis}
The issue of MLE existence in the conventional sense in an exponential family is 
closely tied to the geometric properties of 
the convex support of the model \citep[Chapter 9]{Geyer:gdor, Rinaldo:2009, Barndorff}.  We 
describe the relevant theory from convex analysis as it pertains to the case of 
discrete state space exponential families.

Define the \emph{convex hull} of a set of points $V$, denoted $\con V$, to be
\begin{align*}
	\left \{ \sum_{i} \alpha_i v_i \mid \alpha_i \in \RR, \alpha_i \geq 0, \sum_i \alpha_i = 1 \right \},
\end{align*}
and the \emph{positive hull} of a set of points $V$, denoted $\pos V$, to be
\begin{align*}
	\left \{ \sum_{i} \alpha_i v_i \mid \alpha_i \in \RR, \alpha_i \geq 0 \right \} .
\end{align*}

A \emph{convex polytope} $C$ is the convex hull of a finite set of points $V$.
By the Minkowski-Weyl theorem \citep[Theorem 19.1]{Rockafellar:1970}, this convex 
set can be represented equivalently as the intersection of a finite collection of 
closed half-spaces.  These two representations of a convex polytope are referred to 
as the \emph{V-representation} and \emph{H-representation}, respectively.  
%The V-representation of a convex polytope $C$ is the set of all linear combinations
%\begin{align*}
%	\sum_{i \in E \cup I} b_i \alpha_i
%\end{align*}
%where $\alpha_i$ are vectors, $b_i$ are scalars, $E$ and $I$ are disjoint finite sets 
%such that
%\begin{align*}
%	b_i \geq 0, \quad i \in E \cup I
%\end{align*}
%and if $I$ is nonempty
%\begin{align*}
%	\sum_{i \in I} b_i = 1.
%\end{align*}
%The positive hull of points $V$, denoted $\pos V$, 
The H-representation can be expressed as the solution set of a finite set of linear 
equations and inequalities,
\begin{align*}
	C = \{x: Ax \leq b \},
\end{align*}
where $A$ is a matrix and $b$ a vector (each linear equation expressed as a pair of inequalities).

The \emph{relative interior} of a convex set $C$, denoted $\rint C$, is the interior 
relative to its affine hull.  The concept is motivated by the idea that a triangle
in $\RR^3$ has empty interior 
since no three-dimensional ball lies in the triangle.  The \emph{closure} of a set $C$, denoted $\cl C$, is equal to
\begin{align*}
	\cl C = \bigcap \{C + \epsilon B \mid \epsilon > 0 \}
\end{align*}
where $B$ is a Euclidean ball \citep[Section 6]{Rockafellar:1970}.  The \emph{relative boundary} of a set $C$, denoted $\rbd C$, is the set difference $\cl C \setminus \rint C$.

A nonempty \emph{face} $F$ of a convex polytope $C$ is a convex subset of $C$ such that 
every line segment in $C$ with a relative interior point in $F$ has 
both end points in $F$ \citep[Section 18]{Rockafellar:1970}.  It is itself a convex polytope.
A \emph{proper} face is a face that is not the empty set or $C$, and 
\emph{facets} are proper faces of the highest dimension.

For a polyhedral convex $C$, every face $F$ is \emph{exposed} \citep[Section 18]{Rockafellar:1970}, 
meaning there exists a vector $\delta$ such that
\begin{align*}
	F = \{ x \in C: \inner{x, \delta} = \sup_{y \in C} \inner{y,\delta}  \}.
\end{align*}
Smooth convex sets like the convex hull of a torus, do not have every face
exposed; here we deal only with convex polyhedral sets, so all the faces we
encounter are exposed.


The \emph{tangent cone} of a polyhedral convex set $C$ at a point $x \in C$ is
\begin{align*}
	T_C(x) = \{s(w-x):w \in C \text{ and } s \geq 0 \}.
\end{align*}


The \emph{normal cone} of a convex set $C$ in $\RR^d$ at a point $x \in C$ is 
\begin{align*}
	N_C(x) = \{ \delta \in \RR^d: \inner{w-x,\delta} \leq 0 \text{ for all } w \in C 
\}.
\end{align*}

Tangent and normal cones are \emph{polars} of each other, that is, each determines the other.  
The normal cone at $x$ can be defined in terms of the tangent cone at $x$ by
\begin{align*}
	N_C(x) 	&= \{ w \in \RR^d: \inner{ w, v } \leq 0 \text{ for all } v \in T_C(x) \} 
\end{align*}
and the tangent cone at $x$ in terms of the normal cone at $x$ by
\begin{align*}
	T_C(x) 	&= \{ v \in \RR^d: \inner{ w, v } \leq 0 \text{ for all } w \in N_C(x) \}.
\end{align*}

%\hl{DEFINE \emph{direction of recession}, \emph{direction of constancy}.}  \textbf{get from 
%\citep{Rockafellar:1970} p 69.}


\section{Mean value parameterization} \label{S:Mean value parameterization}
An alternative parameterization of an exponential family is the mean value 
parameterization.  For a natural parameter $\eta$, we can define the mean parameter 
$\mu$ such that
\begin{align*}
	\mu = \E_\eta g(Y).
\end{align*}
This maps any $\eta \in \Xi$ for which the expectation exists 
(and it does exist for all $\eta \in \rint \Xi$)
to a point in
the relative interior of the convex support.
Thus mean value parameters are more easily interpretable quantities, since
they reside in the same space as the natural statistics 
\citep{Handcock:degeneracy,Rinaldo:2009}.
In particular, $\hat{\mu}_{\textrm{MLE}} = g(\yobs)$ is the MLE in the mean value parameterization
by \eqref{E:Observed-Expected}.  
A point on the 
relative boundary of the convex support cannot be the mean of any distribution
in the family.  The models with $g(\yobs)$ on this relative boundary
are the degenerate models mentioned in Section~\ref{S:Non-existent MLE};
thus even if the MLE does not exist in the natural parameterization it does 
still exist in this mean value parameterization.

%\citeauthor{Handcock:degeneracy} observed that mean 
%value parameters located too close to the boundary of the convex support tend to 
%correspond to degenerate distributions; this was corroborated by 
%\citeauthor{Rinaldo:2009} from the perspective of Shannon's entropy function.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MLE existence in exponential families} \label{S:MLE existence}
We now present two equivalent approaches to determining the existence of an MLE
in the conventional sense.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Approach of \citet{Barndorff}}
The well-known condition for the existence of the MLE 
relates the relative location of the observed statistic to the boundaries of 
the convex support and is formally stated as follows:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Corollary 9.6 in \citet{Barndorff}] \label{Thm:MLE rint}
For a regular exponential family with convex support $C$ and observed statistic $g(\yobs)$, the MLE exists in the conventional sense if and only if 
$g(\yobs) \in \rint(C)$ and is unique if and only if $\intr(C)$ is nonempty, which
happens if and only if $\rint(C) = \intr(C)$. 
\end{theorem}

While the above theorem is concise, we note that in order to apply this result, 
the geometry of $C$ must be known.

\subsection{Approach of \citet{Geyer:gdor}}
\citet{Geyer:1990,Geyer:gdor} related MLE existence to
 not only the relative boundary of the convex support, 
 but also to behavior of the log likelihood function along certain directions.

We retrace the steps leading to this result, and present simpler proofs where possible:
\begin{theorem}[Theorem 2.2 in \citet{Geyer:1990}]\label{Thm:e_c}
For a full exponential family with
\begin{itemize}
%\item log likelihood function $\ell(\eta)$ as in \eqref{E:loglike},
\item density function $f_\eta(y)$ as in \eqref{E:ERGM},
\item natural parameter space $\Xi$ as in \eqref{E:paramspace},
\item natural statistic $g(Y)$,
%\item observed data $\yobs$ such that $g(\yobs) \in
\item convex support $C$,
\item  a non-zero direction $\delta$,
\item $\sigma_C (\delta) = \sup_{g(y) \in C} \inner{ g(y), \delta}$,
\item $H = \set{w: \inner{w, \delta} = \sigma_C(\delta) }$,
\item $P_\eta(g(Y) \in H ) > 0$ for some distribution in the family.
\end{itemize}
Then
\begin{align*}
e^{c(\eta + s \delta) - bs} &\to 
		\begin{cases} 
			0 									& b > \sigma_c(\delta) \\
			e^{c(\eta)} P_\eta(g(Y) \in H ) 		& b = \sigma_c(\delta) \\
			+\infty								& b < \sigma_c(\delta)
		\end{cases}
& \text{as } s \to +\infty.
\end{align*}

\end{theorem}
Here, $H$ is the supporting hyperplane to the set $C$ with normal vector $\delta$.

\begin{proof}
\textbf{Case: $b = \sigma_C(\delta)$.}

Starting with \eqref{E:kappa},
\begin{align*}
	e^{c(\eta)} = \kappa(\eta) = \int  e^{\inner{\eta, g(y)}} \, d\mu(y),
\end{align*}
so that
\begin{align*}
	e^{ c(\eta + s \delta ) - bs } &= \int e^{\inner{\eta + s \delta,g(y)} - bs } \, d\mu(y). \\
					&= \int e^{\inner{\eta, g(y)}  + s [ \inner{ g(y), \delta} - b ] } \, d\mu(y). 
\end{align*}
Multiplying by $\frac{f_\eta(y)}{f_\eta(y)}$,
\begin{align*}
	e^{ c(\eta + s \delta ) - bs } &= \int e^{\inner{\eta,g(y)}  + s [ \inner
{g(y),\delta} - b ] }  \frac{ f_\eta(y) }{ e^{\inner{\eta,g(y)} - c(\eta)} }\, d\mu(y) \\
	&= \int e^{  s [ \inner{g(y),\delta} - b ] + c(\eta) }  f_\eta(y) \, d\mu(y) \\
	&= \E_\eta e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) }.
\end{align*}
%What happens as $s \to +\infty$?  We would like to reverse the order of taking the 
%limit and expectation.  Fortunately, we have the monotone convergence theorem.  
The monotone convergence theorem can be applied to reverse the order of the limit and expectation
for monotone sequences of random variables.  For $\inner{g(Y), \delta} \leq b$, we 
have a monotonically decreasing
 sequence of random variables and for $\inner{g(Y), \delta} > b$, the sequence is increasing.  Thus, 
\begin{align*}
	\lim_{s\to \infty} \E_\eta e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } &= E_
\eta \lim_{s\to \infty} e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) }. 
\end{align*}
Ignoring the expectation and examining just the limit component of the above,
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } &= 
			\begin{cases} 
			0 								& \inner{g(Y),\delta} < b \\
			e^{c(\eta)} 			 			& \inner{g(Y),\delta} = b \\
			+\infty							& \inner{g(Y),\delta} > b.
		\end{cases}
\end{align*}
In this first case we consider, $b = \sigma_C(\delta) = \sup_{g(y) \in C}
\inner{g(y),\delta}$, so $\inner{g(Y),\delta}$ can never be greater than $b$ and 
thus the $+\infty$ outcome above is not possible.  We can rewrite the above result 
succinctly as
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } 
		&= I(\inner{g(Y), \delta} = b ) e^{c(\eta)} 
		= I( g(Y) \in H ) e^{c(\eta)}.
\end{align*}
Then returning to the original expression of interest,
\begin{align*}
	\lim_{s\to \infty} e^{c(\eta + s\delta) - bs} 
	= \lim_{s\to \infty} \E_\eta e^{  s [ \inner{g(Y),\delta} - b ] + c(\eta) } 
	&= e^{c(\eta)} P( g(Y) \in H ).
\end{align*}

\textbf{Case: $b > \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\eta) }P(g(Y)\in H) \right ) \cdot 0 = 0.
\end{align*}

\textbf{Case: $b < \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\eta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\eta) }P(g(Y)\in H) \right ) \cdot \left ( + \infty \right ) 
= + \infty,
\end{align*}
since $P_\eta(g(Y)\in H) > 0$ by assumption.
\end{proof}

We now define directions of constancy and recession in terms of Theorems 1
and 2 of \citet{Geyer:gdor} which we state without complete proofs.

\begin{theorem}[Direction of Constancy: Theorem 1 in \citet{Geyer:gdor}] \label{Thm:DOC}
For a full exponential family with
\begin{itemize}
\item log likelihood function $\ell(\eta)$ as in \eqref{E:loglike},
\item natural parameter space $\Xi$ as in \eqref{E:paramspace},
\item natural statistic $g(Y)$,
\item observed data $\yobs$ %such that $g(\yobs) \in C$, the convex support,
% 3/15/11: Charlie says don't need the rest of the above since always true for 
% discrete 
\end{itemize}
the following are equivalent:
\begin{enumerate}
\item For all $\eta \in \Xi$, the function $s \mapsto \ell( \eta + s\delta)$ is 
constant on $\RR$ \cite[Theorem 1(b)]{Geyer:gdor}.
\item The parameter values $\eta$ and  $\eta + s\delta$ correspond to the same 
probability distribution for all $\eta \in \Xi$ and all real $s$ \cite[Theorem 1(d)]
{Geyer:gdor}.
\item $\inner{g(Y) - g(\yobs),\delta} = 0$ almost surely for all distributions in the 
family \cite[Theorem 1(f)]{Geyer:gdor}.
\item $\delta \in N_C(g(\yobs))$ and $-\delta \in N_C(g(\yobs))$ \cite[Theorem 1(g)]
{Geyer:gdor}.
\item $\inner{w,\delta} = 0$ for all $w \in T_C(g(\yobs))$ \cite[Theorem 1(h)]
{Geyer:gdor}.
\end{enumerate}
\end{theorem}
Any vector $\delta$ that satisfies any of the conditions above is called a 
\emph{direction of constancy} of the log likelihood.
The set of all directions of constancy is called the \emph{constancy space} of 
the log likelihood, which is a vector subspace.

\begin{theorem}[Direction of Recession: Theorem 3 in \cite{Geyer:gdor}] \label{Thm:DOR}
For a full exponential family with the same setting as Theorem~\ref{Thm:DOC},
%\begin{itemize}
%\item log likelihood function, $\ell(\eta)$, described by \eqref{E:loglike},
%\item natural parameter space, $\Xi$,
%\item natural statistic, $g(Y)$,
%\item observed value of the natural statistic, $g(\yobs)$, such that $g(\yobs) \in C$, the convex 
%support,
%\end{itemize}
the following are equivalent:
\begin{enumerate}
\item For all $\eta \in \Xi$, the function $s \mapsto \ell( \eta + s\delta)$ is 
nondecreasing on $\RR$ \cite[Theorem 3(b)]{Geyer:gdor}.
\item $\inner{g(Y) - g(\yobs),\delta} \leq 0$ almost surely for all distributions in 
the family. \cite[Theorem 3(d)]{Geyer:gdor}.
\item $\delta \in N_C(g(\yobs))$ \cite[Theorem 3(e)]{Geyer:gdor}.
\item $\inner{w,\delta} \leq 0$ for all $w \in T_C(g(\yobs))$ \cite[Theorem 3(f)]
{Geyer:gdor}.
\end{enumerate}
\end{theorem}
Any vector $\delta$ that satisfies any of the conditions above is called a 
\emph{direction of recession} of the log likelihood.  Every direction of constancy
is a direction of recession.

\citeauthor{Geyer:gdor} uses Corollary 2.4.1 in \citet{Geyer:1990} to prove the 
equivalence of conditions 1 and 2 of Theorem~\ref{Thm:DOR}.  Here we present
a more accessible proof without the use of this corollary.  It is the equivalence
of these two conditions that relates the behavior of the log likelihood function
to the convex support of the model.
%The proof requires the following lemma, which a PROOF WILL BE NEEDED.
%$\ell(\theta+s\delta)$ is non-decreasing if and only if $\lim_{s \to \infty} \ell
%(\theta+s\delta) > -\infty$.  See Theorem 8.6 in \citep{Rockafellar}
\begin{proof}
\textbf{From $2 \to 1$}:
Assume $\inner{g(Y)-g(\yobs), \delta} \leq 0$ almost surely.  Take expectations of both sides
with respect to the distribution indexed by the parameter value $\eta + s \delta$:
\begin{align} \label{E:exp_gy}
\inner{ \E_{\eta + s \delta} g(Y) - g(\yobs), \delta} \leq 0.
\end{align}
Now, taking the derivative of $\ell( \eta + s\delta)$ with respect to $s$,
\begin{align*}
\deriv{\ell( \eta + s \delta)}{s} &= \deriv{}{s} 
			\left ( \inner{g(\yobs), \eta+s \delta} - c(\eta+s\delta)  \right )\\
%	&= \inner{g(\yobs), \delta} - \deriv{}{s} c(\eta+s\delta) \\
	&= \inner{g(\yobs), \delta} - \inner{ \E_{\eta+s\delta}g(Y),\delta }\\
	&= - \inner{ \E_{\eta+s\delta}g(Y) - g(\yobs),\delta },
\end{align*}
which is greater than or equal 0 by \eqref{E:exp_gy}.
Thus $\ell(\eta+s\delta)$ is a non-decreasing function of $s$.

\textbf{From $1 \to 2$}:
\begin{align}
	\ell( \eta+s\delta) &= \inner{g(\yobs), \eta+s\delta} - c(\eta+s\delta) \notag \\ 
	&= \inner{g(\yobs), \eta} + s \inner{g(\yobs),\delta} -bs +bs - c(\eta+s\delta) \notag \\ 
	&= \inner{g(\yobs), \eta} + s [\inner{g(\yobs),\delta} -b]  - \log e^{c(\eta+s\delta) -bs}. \label{E:expanded ell}
\end{align}
%Theorem~8.6 in \citep{Rockafellar:1970} says that $\ell(\theta+s\delta)$ 
%is non-decreasing if and only 
%if $\lim_{s \to \infty} \ell(\theta+s\delta) > -\infty$.  This implies that in the 
%above expression, for the right-most term, $b \geq \sigma_c(\delta)$, and also that

By assumption, $\ell(\eta + s\delta)$ is a non-decreasing function of $s$.  
Then for $\ell(\eta ) > -\infty$, $\ell(\eta + s\delta) \geq \ell(\eta )  > -\infty$
for any $s>0$ and thus $\lim_{s \to \infty} \ell(\eta+s\delta) > -\infty$.
By Theorem~\ref{Thm:e_c}, the limit of \eqref{E:expanded ell} is greater than $-\infty$ 
only if
\begin{align*}
	\inner{g(\yobs), \delta} - b \geq 0,
\end{align*}
and
\begin{align*}
	b \geq \sigma_C(\delta). % \sup_{g(y)\in C} \inner{ g(y), \delta }.
\end{align*}
Then
\begin{align*}
	\sigma_C(\delta)  - \inner{g(\yobs), \delta} \leq b - \inner{g(\yobs), \delta}  \leq 0,
%#	\inner{y, \delta} - \inner{Y_{max},\delta } \geq 0.
\end{align*}
and recalling that $\sigma_C(\delta) = \sup_{g(y) \in C} \inner{g(y), \delta}$, 
we conclude that
\begin{align*}
	\inner{g(Y) - g(\yobs),\delta } \leq 0, \quad \text{almost surely}.
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Theorems~\ref{Thm:DOC} and \ref{Thm:DOR} induce the following criteria about the
existence of the MLE in the conventional sense:

%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Extension of Theorem 4 in \citet{Geyer:gdor}] \label{Thm:MLE existence}
For a full exponential family with the setting of Theorem~\ref{Thm:DOC}, the 
following are equivalent 
\begin{enumerate}
\item the MLE exist.
\item Every direction of recession is a direction of constancy.
\item $N_C(g(\yobs))$ is a vector subspace.
\item $T_C(g(\yobs))$ is a vector subspace.
\item $g(\yobs) \in \rint C$.
\end{enumerate}
\end{theorem}
\begin{proof}
The equivalence of 1 -- 4 are shown in \citet{Geyer:gdor}.  We have added condition 5 above
and show the equivalency of this condition to condition 4.  By Theorem~6.4 in 
\citet{Rockafellar:1970}, the point $g(\yobs) \in C$ if and only if 
$v \in T_C(g(\yobs))$ implies $-v \in T_C(g(\yobs))$.  But by Proposition 3.8 in 
\citet{Rockafellar}, the latter condition on  $T_C(g(\yobs))$ is necessary and sufficient 
for  $T_C(g(\yobs))$ to be a vector subspace.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}[Corollary 5 in \citet{Geyer:gdor}] \label{Cor:strictly increasing}
For a full exponential family with the setting as Theorem~\ref{Thm:DOC}, if $\delta$ is a
direction of recession that is not a direction of constancy, 
then for all $\eta \in \Xi$, the function $s \mapsto \ell(\eta+s\delta)$ is strictly
increasing on the interval where it is finite.
\end{corollary}
%%%%%%%%%%%%%%%%%%%%%

%This corollary is implied by Theorems \ref{Thm:DOC} and \ref{Thm:DOR}; if
%$\delta$ is a direction of recession that is not a direction of constancy, 
%then $\ell(\eta+s\delta)$ is nondecreasing with respect to $s$ by 
%Theorem \ref{Thm:DOR} but cannot be constant by Theorem \ref{Thm:DOC}.  Therefore,
%it must be strictly increasing.

%Theorem~\ref{Thm:MLE rint} relates the non-existence of the MLE to the location
%of $g(\yobs)$ relative to the boundary of $C$.  
With the addition of Condition 5 in Theorem~\ref{Thm:MLE existence}, we have now  
tied this approach to the one previously described of \citeauthor{Barndorff} in 
Theorem~\ref{Thm:MLE rint}.  
The MLE does not exist in the conventional sense if there exists a vector 
$\delta$ that is a direction of recession but not a direction of constancy.  
According to Corollary~\ref{Cor:strictly increasing}, this is very intuitive:
it means the log likelihood is always increasing in that direction.  
Theorem~\ref{Thm:MLE existence} relates this
precisely to the setting where $g(\yobs)$ is on the relative boundary of
the convex support.





%%%%%%%%%%%%%%%%%%%%%

\subsection{Limiting conditional model} \label{S:LCM}
When the MLE does not exist for an exponential family in the conventional sense, there 
exists a direction of recession that is not a direction of constancy 
along which the log likelihood is strictly increasing.  The behavior of the 
density function of the distribution along such a direction 
is described in the following theorem:

\begin{theorem}[Theorem 6 in \citet{Geyer:gdor}] \label{Thm:LCM}
For a full exponential family with the setting of Theorem~\ref{Thm:DOC}, and 
additionally,
\begin{enumerate}
\item density function $f_{\eta}(y)$ defined by \eqref{E:ERGM},
\item direction of recession $\delta$,
\item $H = \{ w \in \RR^d: \inner{ w-g(\yobs), \delta } = 0 \}$,
\item $P( g(Y) \in H) > 0$ for some distribution in the family,
\item $\sigma_C (\delta) = \sup_{g(y) \in C} \inner{ g(y), \delta}$,
\end{enumerate}
then for all $\eta \in \Xi$
\begin{align} \label{E:LCM}
\lim_{s \to \infty} f_{\eta+s\delta}(y) = 
			\begin{cases} 
		0 								
			& \inner{g(y), \delta} < \sigma_C(\delta) \\
			%& \inner{g(y) - g(\yobs), \delta} < 0 \\
		\frac{f_\eta(y)}{P_\eta(g(Y) \in H)} 	
			& \inner{g(y),\delta} =  \sigma_C(\delta)\\
			%& \inner{g(y) - g(\yobs),\delta} = 0 \\
		+\infty							
			& \inner{g(y), \delta} > \sigma_C(\delta).
			%& \inner{g(y) - g(\yobs), \delta} > 0.
		\end{cases}
\end{align}
If $\delta$ is not a direction of constancy, 
then $s \mapsto P_{\eta+s\delta}( g(Y) \in H)$ is continuous and 
strictly increasing, and $P_{\eta+s\delta}( g(Y) \in H) \to 1$ as $s \to \infty$.
\end{theorem}


%%%%% BEGIN PROOF
\begin{proof}
By Theorem~\ref{Thm:DOR}, if $\delta$ is a direction of recession, then 
$\inner{g(Y) - g(\yobs), \delta} \leq 0$ almost surely, which implies 
that $\inner{g(Y), \delta} \leq \inner{g(\yobs), \delta}$.  
So, the largest value that $\inner{g(Y), \delta}$ can take is 
$\inner{g(\yobs), \delta}$.  Then 
\begin{align*}
\sigma_C(\delta) = \sup_{g(y) \in C}\inner{g(y), \delta} = 
\inner{g(\yobs), \delta}.
\end{align*}

From \eqref{E:ERGM}, we can express $f_{\eta+s\delta}(y)$ as
\begin{align*}
 f_{\eta+s\delta}(y) &= e^{ \inner{\eta+s\delta,g(y)} - c(\eta+s\delta)  } \\
 &= e^{ \inner{\eta,g(y)} -c(\eta)+c(\eta) + s\inner{\delta,g(y)} - c(\eta+s\delta) } \\
 	&= f_\eta(y) \frac{e^{c(\eta)}}{e^{ c(\eta+s\delta) - \inner{g(y),\delta}
s } }.
\end{align*}
The denominator in the fraction above has the form of the starting expression in 
Theorem~\ref{Thm:e_c} with $\inner{g(y),\delta}$ in place of $b$.
Thus we can take the limit of $f_{\eta+s\delta}(y) $ as
$s \to +\infty$ by applying Theorem~\ref{Thm:e_c},

\begin{align*}
	f_{\eta+s\delta}(y) = f_\eta(y) \frac{e^{c(\eta)}}{e^{ c(\eta+s
\delta) - \inner{g(y),\delta}s } } 
	\to	
			\begin{cases} 
			0 					& \inner{g(y),\delta} < \sigma_C(\delta) \\
			\frac{f_\eta(y)}{P_\eta(g(Y) \in H)} 	& 
								\inner{g(y),\delta} = \sigma_C(\delta) \\
			+\infty				& \inner{g(y),\delta} > \sigma_C(\delta).
			\end{cases}
\end{align*}
This gets us \eqref{E:LCM}.  We next show the behavior of 
$P_{\eta+s\delta}(g(Y) \in H)$ as $s \to +\infty$ when $\delta$ is a direction of
recession that is not a direction of constancy.

\begin{align*}
 P_{\eta+s\delta}(g(Y) \in H) &= \int_H e^{\inner{g(y), \eta+s\delta} - c(\eta
+s\delta)} \, d\mu(y) \\
		&= \int_H  \frac{e^{c(\eta)}}{e^{c(\eta+s\delta)-\inner{g(y),\delta}s}} 
					f_\eta(\eta) \, d\mu(y)\\
		&= \E_\eta  \left ( I_H \frac{e^{c(\eta)}}{e^{c(\eta+s\delta)-\inner{g(y),\delta}s}} \right )
\end{align*}
Since the indicator function $I_H$ evaluates to zero unless 
$\inner{ g(y), \delta} = \inner{ g(\yobs), \delta} $,  
this expression can pulled out of the expectation along with other constants, so that 
\begin{align*}
		 P_{\eta+s\delta}(g(Y) \in H)
		 &= \frac{e^{c(\eta)} }{ e^{ c(\eta+s\delta) -s\inner{g(\yobs),\delta} } }
		 \E_\eta  I_H   \\
		 &= \frac{e^{c(\eta)} }{ e^{ c(\eta+s\delta) -s\inner{g(\yobs),\delta} } }
		 P_\eta  (g(Y) \in H). 
		 \end{align*}
 
Then taking the limit of $s \to +\infty$ via Theorem~\ref{Thm:e_c} gives
\begin{align*}
 P_{\eta+s\delta}(g(Y) \in H)
		&\to \frac{e^{c(\eta)}}{ e^{c(\eta)} P_\eta (g(Y) \in H) } P_\eta (g(Y)\in H) = 1
 \end{align*}
 as desired.   
\end{proof}

%%%%% END PROOF

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Remarks on Theorem~\ref{Thm:LCM}}
There are many implications of Theorem~\ref{Thm:LCM} described in \citet{Geyer:gdor}
which we repeat here and go into more detail for further clarity.

The event $\inner{g(Y),\delta} > \inner{g(\yobs),\delta}$ has zero probability by 
Theorem~\ref{Thm:DOR}(2) and thus the $+\infty$ case need not be considered.
The right-hand side of \eqref{E:LCM} can be viewed as a conditional density of a 
distribution with parameter $\eta$ given $g(Y) \in H$, and expressed as $f_{\eta}
(\, \cdot\,  \mid g(y) \in H)$.  This is because the numerator
is in fact a joint density, which can be reasoned as follows:

Let $W = I(g(Y) \in H)$, which has density 
\begin{align*}
	f_\eta(w) &= \begin{cases}
					P_\eta(g(Y) \in H) \quad \text{for $w=1$} \\
					P_\eta(g(Y) \notin H) \quad \text{for $w=0$}.
				\end{cases}
\end{align*}

The conditional density of $W \mid Y$ is
\begin{align*}
	f_\eta(w \mid y) &= \begin{cases}
			1, 	\quad &\text{for $w=0$ and $g(y) \notin H$}\\
			0, 	\quad &\text{for $w=1$ and $g(y) \notin H$}\\
			0, 	\quad &\text{for $w=0$ and $g(y) \in H$}\\
			1, 	\quad &\text{for $w=1$ and $g(y) \in H$}.\\
 		\end{cases}
\end{align*}

Then the conditional density for $Y \mid g(Y)\in H$ can be expressed
\begin{align*}
	f_\eta(y \mid g(y) \in H) &= f_\eta(y \mid w=1) \\
					&= \frac{f_\eta(w=1, y)}{f_\eta(w=1)}	
					= \frac{f_\eta(w=1 \mid y) f_\eta(y)}{P_\eta(g(Y) \in H)}  \\%\quad 
%					 \text{for $g(y) \in H$}	\\
	&= \begin{cases}
 			\frac{0 \cdot f_\eta(y)}{P_\eta(g(Y) \in H)}   \quad 
					 \text{for $g(y) \notin H$}	\\
 			\frac{1 \cdot f_\eta(y)}{P_\eta(g(Y) \in H)}   \quad 
					 \text{for $g(y) \in H$}	\\
 		\end{cases}
\end{align*}
which is exactly the right-hand side of \eqref{E:LCM}.

The log likelihood for $f_\eta( \fatdot \mid g(y) \in H)$ for $\eta \in \Xi$ can be expressed as
\begin{align*}
%	\ell_{LCM}(\eta) &= 
	\inner{\eta, g(\yobs)} - c(\eta) - \log P_\eta(g(Y) \in H) 
%			&= \ell(\eta) - \log P_\eta(g(Y) \in H)
\end{align*}
which clearly has exponential family form with the same natural parameter $\eta$ and 
statistic $g(\yobs)$ as the original exponential family,
and is called the \emph{limiting conditional model} (LCM) \citep{Geyer:gdor}.  
The cumulant function is different but is always finite since it only 
involves a summation over a finite number of terms.  The parameter space for 
$f_\eta( \fatdot \mid g(y) \in H)$ is still the 
whole Euclidean space, as in the original family.  
%\begin{align*}
%\{ f_{\eta}(\, \cdot\,  \mid g(Y) \in H) \text{ for }  \eta + \gamma: \eta \in \Xi \text
%{ and } \gamma \in \Gammalim \},
%\end{align*}
%where $\Gammalim$ is the constancy space of $f_{\eta}( \, \cdot\,  \mid g(Y) \in H)$,

When $\delta$ is a direction of recession that is not a direction of constancy, 
we can succinctly summarize the result of Theorem~\ref{Thm:LCM} as
\begin{align*}
\lim_{s \to \infty} f_{\eta+s\delta}(y) = f_{\eta}( y \mid g(y) \in H).
\end{align*}
The last part of Theorem~\ref{Thm:LCM} says that for the sequence of distributions with parameter 
value $\eta+s\delta$, the probability of $g(Y)$ occurring on the hyperplane $H$ is strictly 
increasing in $s$, going to 1.  We may think of this as 
``probability accumulating on the boundary."  This intuition is the basis for our
extended algorithm in Section~\ref{S:pseudocode new}.

We can express the log likelihood of the LCM as a function of the 
log likelihood of the original family,
\begin{align} \label{E:LCM ll bound}
 \ell_{LCM}(\eta) = \ell(\eta) - \log P_\eta(g(Y) \in H)
\end{align}
implying
\begin{align*}
	\ell(\eta) < \ell_{LCM}(\eta).	
\end{align*}
The inequality above is strict: by Corollary~\ref{Cor:strictly increasing}, the 
log likelihood is strictly increasing on the entire interval on which it is finite
and hence $P_\eta(g(Y) \in H)$ cannot equal 1.
Thus even though the MLE does not exist for the original family, its log likelihood
$\ell(\fatdot)$ is bounded by the LCM log likelihood and increases strictly
towards $\ell_{LCM}(\eta)$ for a given $\eta$ as $s$ increases.
This is illustrated in Figure~\ref{F:LCM} in the case of a two-dimensional parameter
space.


\begin{figure}[h]
\centering
    \scalebox{.55}{\input{Figures/ll-LCM.pdf_t}}
	\caption[Log likelihood convergence to LCM]{The log likelihood in the
	the direction $\delta$, converging to the LCM log likelihood for a two-dimensional parameter space.  The log likelihood $\ell(\etaLCM + s\delta)$
	converges to the maximum of $\ell_{LCM}(\eta)$ as $s \to +\infty$.}
\label{F:LCM}
\end{figure}


Another consequence of Theorem~\ref{Thm:LCM} is in the mean value parameter space:
%%%%%%%%%%%%%%%
\begin{corollary} \label{Cor:LCM mu-space}
Under the setting of Theorem~\ref{Thm:LCM}, let $\delta$ be a direction of 
recession that is not a direction of constancy.  Then
\begin{align} \label{E:LCM mu-space}
	\E_{\eta+ s\delta} g(Y) \to \E_\eta \left ( g(Y) \mid g(Y) \in H  \right )
\end{align}
as $s \to +\infty$.
\end{corollary}
\begin{proof}
By definition,
\begin{align*}
	\E_{\eta+ s\delta} g(Y) = \int g(y) f_{\eta+ s\delta}(y) \, d\mu(y).
\end{align*}
There exist a constant $M > g(y)$ for all $y \in \YY$ for the family to
be full.  Then
by the dominated convergence theorem and the application of Theorem~\ref{Thm:LCM}, 
the limit of the above expectation gets us \eqref{E:LCM mu-space}.
\end{proof}

\subsection{Generic direction of recession}
We now define a special kind of direction of recession called a 
\emph{generic direction of recession} (GDOR) that facilitates finding the 
LCM for which the MLE exists.
A vector $\delta$ is a GDOR if in addition to being a direction of recession,  
$\delta \in \rint N_C(g(\yobs))$ and $N_C(g(\yobs))$ is not a vector subspace.
Since normal cones are convex sets and relative interiors of nonempty convex sets 
are nonempty, by Theorem~\ref{Thm:MLE existence},
a GDOR exists if and only if the MLE does not exist.

The following theorems and corollaries provide us with the foundation
for finding and using GDORs.  In essence, GDORs give us a convenient way
to find an LCM for which the MLE exists.
This first theorem provides the framework through
which we can find a GDOR.  
%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Theorem 7 from \citet{Geyer:gdor}] \label{Thm:L-GDOR}
For a full exponential family having polyhedral convex support $C$ and observed value 
of the natural statistic $g(\yobs)$ such that $g(\yobs) \in C$, 
let $T_C(g(\yobs) ) = \con(V)$, and define
\begin{align*}
	L = \{ v \in V: -v \in T_C(g(\yobs)) \}.
\end{align*}
Then a GDOR exists if and only if $L \neq V$, in which case a vector $\delta$ is a GDOR if and 
only if
\begin{align*}
	\inner{w, \delta} = 0, \quad w \in L \\
	\inner{w, \delta} < 0, \quad w \in V \setminus L \\
\end{align*}
\end{theorem}
It is not immediately obvious why defining $L$ in such a manner is helpful; in fact,
the motivation is rooted in linear programming.  We demonstrate the application of 
this theorem in Chapter~\ref{Chapter:Linear programming}.

%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}[Corollary 8 from \citet{Geyer:gdor}]
Under the assumptions of Theorem~\ref{Thm:L-GDOR}, a GDOR is not a direction of constancy.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}[Corollary 9 from \citet{Geyer:gdor}] \label{Cor:spanL}
Under the assumptions of Theorem~\ref{Thm:L-GDOR}, suppose $\delta$ is a GDOR.  Then
\begin{align*}
	T_{C \cap H} (g(\yobs)) &= \spanl L \\
	C \cap H &= C \cap (g(\yobs) + \spanl L ). \\
\end{align*}
\end{corollary}

The following theorem shows that $C \cap H$ defines the support of the LCM 
in the \emph{finite} state space setting that is of interest to us.  
%%%%%%%%%%%%%%%%%%%%%
\begin{theorem} \label{Thm:C-H}
For a full exponential family having finite polyhedral convex support $C$ and observed value 
of the natural statistic $g(\yobs)$ such that $g(\yobs) \in C$, suppose $\delta$ is a GDOR.
Then $C \cap H$ defines the convex support of the LCM.
\end{theorem}
\begin{proof}
Theorem~\ref{Thm:LCM} says that if $\delta$ is a GDOR, which is a direction of recession
that is not a direction of constancy, then for the original model,
\begin{align*}
	P_{\eta+s\delta}( g(Y) \in H ) \to 1.
\end{align*}
Since the LCM is the limiting distribution of the original model as $s \to +\infty$, it follows that $P^{LCM}(g(Y) \in H) = 1$.  
This puts 0 probability on points outside of $H$.  

The original model puts 0 probability on any $x \notin C$, including $H \setminus C$.  Because of the pointwise convergence of the original density to the LCM, since the original density puts zero probability on any $x \in H \setminus C$, the limiting model must as well.  Then
\begin{align*}
	P^{LCM}(g(Y) \in H) &= P^{LCM} \left( g(Y) \in ( (H \setminus C)  \cup (H \cap C)   \right ) \\
	 			&= P^{LCM} \left( g(Y) \in H \setminus C \right) 
								+ P^{LCM} \left( g(Y) \in H \cap C \right ) \\
	 			&= 0 + P^{LCM} \left( g(Y) \in H \cap C  \right ) = 1.
\end{align*}

It is left to show that there is no smaller set than $C \cap H$ that 
can be the convex support of the LCM.
%Let $S = g(\YY)$, and define $A = \con( S \cap H )$.  Does $A = C \cap H )$?
%Clearly $ A \subset (C \cap H)$.  But is there an $x \in C\cap H$ where $x \notin A$?
Let $S = g(\YY)$.  The convex support of the LCM by construction is $\con(S \cap H)$, 
which is closed since $S$ is finite.  
%Clearly, $(S \cap H) \subset (C \cap H)$.  
Is $(C \cap H) = \con(S \cap H)$?
If a point $x \in C$, it is a convex combination of the points in $S$,
\begin{align*}
	x = \alpha_1 w_1 + \alpha_2 w_2 + \cdots + \alpha_n w_n
\end{align*}
where $\alpha_i \geq 0$, $\sum_i \alpha_i = 1$, and $w_i \in S$.

We claim for any $x \in C \cap H$, the coefficients of the $w_i$ not in $S \cap H$ must all be zero.  Suppose to get a contradiction that for an $x \in S \cap H$, there exist $\alpha_j > 0$ for $w_j \notin S \cap H$ for $j \in J$.  Then 
\begin{align*}
	\inner{x - g(\yobs), \delta } &= \left \inner{ \sum_i \alpha_i w_i - g(\yobs), \delta \right } \\
	&= \left \inner{ \sum_i \alpha_i \left( w_i - g(\yobs) \right), \delta \right }\\
	&=  \left \inner{ \sum_{i \notin J} \alpha_i \left( w_i - g(\yobs) \right), \delta \right }
		+ \left \inner{ \sum_{j\in J} \alpha_j \left( w_j - g(\yobs) \right), \delta \right }
\end{align*}
where the first term is less than or equal to zero by the definition of a direction of
recession, and the second term is strictly less than zero since $w_j \notin H$.  Thus 
$\inner{x - g(\yobs), \delta } < 0$, which means $x \notin H$, leading to a contradiction.

Thus $x$ will equal a convex combination of just the points in $S \cap H$.  Since this is 
true if and only if $x \in C \cap H$, we have that $C \cap H = \con( S\cap H)$.
\end{proof}

\begin{corollary} \label{Cor:LCM MLE exists}
For a full exponential family having finite polyhedral convex support $C$ and 
observed value of the natural statistic $g(\yobs)$ such that $g(\yobs) \in C$, 
suppose $\delta$ is a GDOR.  Then the LCM found by using $\delta$ has convex
support $C \cap H$ for which $g(\yobs)$ lies in its relative interior, and the MLE exists.
\end{corollary}
\begin{proof}
By Theorem~\ref{Thm:C-H}, the LCM found by maximizing along $\delta$ has 
convex support $C \cap H$.  By Corollary~\ref{Cor:spanL},  the tangent cone 
$T_{C \cap H} (g(\yobs))$ is in fact a vector subspace and thus 
$g(\yobs) \in \rint (C \cap H$).  Finally, by Theorem~\ref{Thm:MLE existence}, 
the MLE for this model must then exist.
\end{proof}

When we find the MLE in a LCM, $\etaLCM$, we say we have found an MLE in the Barndorff-Nielsen completion of the original family.  This LCM MLE maximizes the likelihood in 
the family that is the union of the LCM family and the original family.

Note that the LCM is not identifiable, since its support is necessarily lower dimensional 
than that of the original model.  That is, there must exist a constancy space, $\Gammalim$, such that 
\begin{align} \label{E:Gammalim}
\ell( \eta + \gamma )^{LCM} = \ell( \eta )^{LCM}
\end{align}
for any $\gamma \in \Gammalim$.  By Theorem~\ref{Thm:DOC}, a GDOR $\delta$ 
is one such $\gamma$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{One-sided confidence interval} \label{S:CI}
Theorem~\ref{Thm:LCM} and Corollary~\ref{Cor:LCM MLE exists} 
tells us that when $\ell(\fatdot)$ is the log likelihood for the original model,
$s$ is a scalar, and $\delta$ a GDOR, then
\begin{align*}
	\lim_{s \to +\infty} \ell(\etaLCM + s\delta) = \ell(\etaLCM)^{LCM} = \sup_{\RR^d} \ell(\eta).
\end{align*}
The MLE for the original model, which we may think of as ``at infinity", has
more structure in this context---it is the value $\etaLCM$ sent to 
infinity in direction $\delta$.

We can then characterize the distributions in the original model that are in the
neighborhood of the LCM MLE distribution by means of a confidence interval.
As $s$ goes from $-\infty$ to $+\infty$, the probability of observing 
$g(Y) \in H$ strictly increases to 1.  
To construct a one-sided $1-\alpha$ confidence interval for how ``close"
distributions in the original model indexed by $\etaLCM + s\delta$ 
are to the LCM MLE distribution, we find the unique $s$, call it $\hat{s}$, such that
\begin{align*}
	P_{\etaLCM + s\delta}\left (g(Y) \in H \right ) = \alpha.
\end{align*}
Then $[\hat{s}, +\infty)$ is a $1-\alpha$ confidence interval for the scalar parameter 
$s$, and, in turn, 
\begin{align*}
\left \{\etaLCM + t\delta, t \geq \hat{s} \right \}
\end{align*}
 gives a $1-\alpha$ confidence interval for the parameter $\etaLCM + s \delta$. 
These are a range of indices to distributions in the neighborhood of the LCM MLE
distribution.
%Note that when $\eta$ is multi-dimensional, these are non-simultaneous 
%multiple confidence intervals.


