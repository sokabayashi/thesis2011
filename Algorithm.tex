\documentclass{amsbook}
%\documentclass[11pt]{book}
%\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm,amssymb}
\usepackage{verbatim}
\usepackage{natbib}
% 9/15 added this
\usepackage{epsfig}
% 9/15 and
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfigure}

%\input{/Figures/test}    
\graphicspath{{myfigures/}}

% ----- Declare Math Operators

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\rint}{rint}
\DeclareMathOperator{\conhull}{con}
\DeclareMathOperator{\dom}{dom}
%\newcommand{\lev}[1]{\textrm{lev}_{#1}}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\bd}{bd}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\rintr}{rint}
\DeclareMathOperator{\con}{con}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\lev}{lev}
\DeclareMathOperator{\pos}{pos}

% ----- Create Useful or More Evocative Shorcuts

\newcommand{\parmspace}{\boldsymbol{\vartheta}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\statespace}{\mathcal{W}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\given}{ \vert }
\newcommand{\takes}{\colon}
\newcommand{\eps}{\varepsilon}
\newcommand{\phee}{\varphi}
\newcommand{\goesto}{\to}
\newcommand{\W}{\mathsf{W}}
\newcommand{\T}{\mathsf{T}}

% ----- Some of Sai's additions
\def\RR{{\mathbb R}}
\def\ZZ{{\mathbb Z}}
\def\XX{{\mathcal X}}
\def\YY{{\mathcal Y}}
\def\DD{{\mathcal D}}
\def\NN{{\mathcal N}}
\def\LL{{\mathcal L}}
\def\FF{{\mathcal F}}
\def\TT{{\mathcal T}}
\def\Dfx{{\nabla f(x)}}
\def\Dgx{{\nabla g(x)}}
\def\sigu2{{\sigma_u^2}}
\def\sige2{{\sigma_e^2}}
\def\ySigy{{y\Sigma^{-1}y}}
\def\yij{{y_{ij}}}
\def\Xij{{X_{ij}}}
\def\Xjk{{X_{jk}}}
\def\Xkl{{X_{kl}}}
\def\Xji{{X_{ji}}}
\def\xij{{x_{ij}}}
\def\Xb{{\boldsymbol{X}}}
\def\xb{{\boldsymbol{x}}}
\def\Xijb{{\boldsymbol{Xij}}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\dderiv}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\ppderiv}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\ppmderiv}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\etaMLE}{\hat{\eta}_{\textrm{MLE}}}
\def\betak{{\beta_k}}
\def\betak1{{\beta_{k+1}}}
\def\etak1{{\eta_{k+1}}}
\def\xk1{{x_{k+1}}}


% Charlie's
\newcommand{\set}[1]{\{\,#1\,\}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\fatdot}{\,\cdot\,}
\newcommand{\inner}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\norm}[1]{\lVert #1 \rVert}

% ----- If there has to white space force it to the bottom of the page
\raggedbottom

% ----- Allow chapters to be in table of contents without numbers
\newcommand{\chapterX}[1]{\chapter*{#1
        \markboth{#1}{#1}}%
        \addcontentsline{toc}{chapter}{#1}
}

% ----- Make the Bibliography and Citations look right -----
\usepackage{natbib}
\renewcommand\bibname{References}
\renewcommand\bibsection{\chapterX{\bibname}}

% ----- For Double Spacing, or any Variant -----
\usepackage{setspace}

%  \setstretch{1.5}

% ----- Fix chapter titles and title page back to single spacing -----
% \let\origmakechapterhead\@makechapterhead
% \let\origmakeschapterhead\@makeschapterhead
% \renewcommand{\@makechapterhead}[1]{\begin{singlespace}\origmakechapterhead{#1}
%\end{singlespace}}
% \renewcommand{\@makeschapterhead}[1]{\begin{singlespace}\origmakeschapterhead{#1}
%\end{singlespace}}

% \let\origmaketitle\maketitle
% \renewcommand\maketitle{\begin{singlespace}\origmaketitle\end{singlespace}}

% \let\origtitlepage\titlepage
% \let\origendtitlepage\endtitlepage
% \renewenvironment{titlepage}{\origtitlepage\setstretch{1}}{\origendtitlepage}

% ----- Theorem Environments

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

% ----- Define Example Environment

% ----- Title

\title{Algorithm for Maximizing Loglikelihood of Exponential Families}
\author{Saisuke Okabayashi \\ Department of Statistics \\ University of Minnesota}
%\date{May 13, 2009}
%\institution{University of Minnesota}

\begin{document}
\maketitle

\section{ALGORITHM} 
Our curvature condition is as follows:
\begin{align*}
	0 < \nabla \ell( \theta_{current} + \alpha p ) ^T p < c \nabla \ell( \theta_
{current}) ^T p
\end{align*}
$\theta_{current} = \theta_0$, some initial value of $\theta$.\\ 
Calculate
\begin{align*}
	\nabla \ell( \theta_{current} ) &\approx t( y_{obs}) - \frac{1}{m} \sum_{i=1}^m t
( Y_i)
\end{align*}
where $Y_i$ are MCMC draws from the distribution with parameter $\theta_{current}$.

\noindent Set $p_0 = \nabla \ell( \theta_0)$, the direction of steepest ascent. \\

%%%% WHILE
\noindent \textbf{while}  $\parallel \nabla \ell( \theta_k) \parallel > \epsilon$ \\ 
\hspace{4mm} 
\indent Guess a value for $\alpha_k$ (bisection search).  Then
\begin{align*}
	\theta_{prop} &= \theta_{current} + \alpha_k p_k \\
	\nabla \ell( \theta_{prop} ) &\approx t( y_{obs}) - \frac{1}{m} \sum_{i=1}^m t
( Y_i)
\end{align*}
\indent where $Y_i$ are MCMC draws from the distribution with parameter $\theta_
{prop}$.

\indent Estimate variances for the quantities in the curvature condition:
\begin{align*}
	\Var \left( \nabla \ell(\theta_{prop})^T p \right ) &= p^T \Sigma_{prop} \, p \\
	\Var \left( c \nabla \ell(\theta_{current})^T p \right ) &= c^2 \,p^T \Sigma_
{current} \, p \\
	\Var(combined) = \Var \left( \nabla \ell(\theta_{prop})^T p - c \nabla \ell
(\theta_{current})^T p \right ) &=  p^T \Sigma_{prop} \, p + c^2 \, p^T \Sigma_
{current} \, p 
\end{align*}
where $\Sigma_{prop}$ and $\Sigma_{current}$ are diagonal matrices with the diagonal 
occupied by the components of the variance estimates of $\nabla \ell(\theta )$ for 
the respective parameter values. \\

%%%% IF
\noindent \textbf{if}( $\nabla \ell(\theta_{prop})^T p - 1.645 se \left( \nabla \ell
(\theta_{prop})^T p \right ) > 0$) \textbf{and} \\
\indent ( $\nabla \ell(\theta_{prop})^T p - c \nabla \ell(\theta_{current})^T p + 
1.645 se(combined) < 0$) \\
\indent then \textbf{ACCEPT} $\alpha$;  \\
\indent Find new search direction $p_{k+1}$ using conjugate gradients. \\
\indent	$\theta_{current} = \theta_{prop}$, $\nabla \ell(\theta_{current}) = \nabla 
\ell(\theta_{prop})$. \\
\indent Reset sample size. \\

\noindent \textbf{else if}( $\| \nabla \ell(\theta_{prop})^T p \| \, / \, se \left
( \nabla \ell(\theta_{prop})^T p \right ) < min.dp$ ) \\
\indent then INCREASE sample size. \\

\noindent \textbf{else if}( $\left( \| \nabla \ell(\theta_{prop})^T p - c \nabla \ell
(\theta_{current})^T p \| \right ) \, / \, se (combined ) < min.dp$ ) \\
\indent then INCREASE sample size. \\

\noindent \textbf{else if}( $\nabla \ell(\theta_{prop})^T p - 1.645 se \left( \nabla 
\ell(\theta_{prop})^T p \right ) < 0$) \\
\indent then angle between $\nabla \ell(\theta_{prop})$ and $p$ is too LARGE;\\ 
\indent \textbf{DECREASE} $\alpha$.\\

\noindent \textbf{else if}( $\nabla \ell(\theta_{prop})^T p - c \nabla \ell(\theta_
{current})^T p + 1.645 se(combined) > 0$) \\
\indent then angle between $\nabla \ell(\theta_{prop})$ and $p$ is too SMALL;\\ 
\indent \textbf{INCREASE} $\alpha$. \\

\noindent $k = k + 1$.  \\

\noindent \textbf{end(while)}

\section{Standard Error for $\| \nabla \ell (\theta) \|$}
The exit condition (currently) for our algorithm is for $\| \nabla \ell (\theta) \|$ 
to be smaller than some pre-specified $\epsilon$, say 0.1.  However, we can only 
approximate $\| \nabla \ell (\theta) \|$ (for the same reason that we can only 
approximate $\nabla \ell (\theta)$).  So, if we wish to construct a confidence 
interval around $\| \nabla \ell (\theta) \|$ to ensure that we are, say 95\% 
confident that we are less than $\epsilon$, then we will require standard errors for 
$\| \nabla \ell (\theta) \|$.
%For a vector $\beta$ of length $p$, we define $\| \beta \|$ to be the Euclidean norm 
of $\beta$.  In our algorithm, $\| \nabla \ell (\theta) \| = 0$ when $\theta$ is 
equal to the MLE.  Thus we would like to continue our loop until $\| \nabla \ell 
(\theta) \| = 0$, but of course $\nabla \ell (\theta)$ is estimated via MCMC and so 
we must consider the Monte Carlo error for this expression.  
This can be estimated via the delta method.

The multivariate version of the delta method states that for a sequence of r.v. $B_n$ 
such that
\begin{align*}
	\sqrt{n} ( B_n - \beta) \stackrel{\DD}{\longrightarrow} N( 0, \Sigma ),
\end{align*}
then for a function $h(B)$, where $h$ such that $\nabla h$ is defined and non-zero,
\begin{align*}
	\sqrt{n} \left ( h(B_n) - h(\beta) \right ) \stackrel{\DD}{\longrightarrow} N 
\left ( 0, \nabla h( \beta)^T \Sigma\nabla h( \beta)  \right ).
\end{align*}

We set $B_n = \nabla \ell_n( \theta)$ and $\beta =\nabla \ell( \theta)$, where we 
know that $B_n \stackrel{a.s.}{\longrightarrow} B$ by SLLN.  We also do not know $
\Sigma$, the variance of $\nabla \ell( \theta)$, but this we will approximate with $
\hat{\Sigma}$, the scaled sample variance-covariance matrix of our MCMC batches of 
the canonical statistic.  That is,
\begin{align*}
	\hat{\Sigma} = \frac{1}{nbatch}\frac{1}{n-1}\sum_{i=1}^{n} (X_i - \bar{X})( X_i - 
\bar{X})^T.
\end{align*}


\section{Newton-Raphson to find the MLE}
The Newton-Raphson update for optimization is
\begin{align*}
	\theta_{k+1} = \theta_k + \left [  \nabla^2 \ell (\theta_k) \right ]^{-1} \nabla 
\ell (\theta_k).
\end{align*}

When $\theta_k$ is sufficiently close to the MLE, it has been shown to converge 
quadratically (super-quadratically?).  little o()?  

In order to compare how well our search algorithm performs, we compare the results to 
the MLE we attain through using the Newton-Raphson for 10 iterations, starting at $
\theta_{True}$.  By starting at $\theta_{True}$, we are of course cheating but the 
hope is that $\theta_{True}$ should be close to the MLE.  By iterating 10 times, we 
know that the error of the attained estimated for the MLE will be \textbf{SMALL}.

Mahalanobis distance
\begin{align*}
	( \theta - \hat{\theta}_{MLE} )^T \, FI \, ( \theta - \hat{\theta}_{MLE} )
\end{align*}
where $FI$ is the Fisher Information matrix, equal to $\left [  \nabla^2 \ell 
(\theta_k) \right ]^{-1}$.

How do we calculate FI?
Recall that 
\begin{align*}
	\nabla \ell (\theta) &= t(y_{obs}) - \E_{\theta} t(Y) \\
	\nabla^2 \ell (\theta) &= - \Var_{\theta} t(Y)
\end{align*}
We estimate $\Var_{\theta} t(Y)$ via MCMC, by calculating $t(Y)$ at each batch as 
well as $t(Y) [t(Y)]^T$ and using these intermediate results to construct the sample 
variance.

\section{MCMCMLE}
The advantage of the MCMCMLE algorithm here is that we can in fact re-use the same 
MCMC samples.  If we want to use 100 different observed data sets to estimate our MLE 
distribution (that is, we will approximate MLEs for each of the 100 data sets), and 
we will first need to generate 100 different observed samples.  Then, for each sample 
we will want to MCMC sampling to maximize the log-likelihood ratio, $r( \theta, 
\theta_0)$.  The beauty here is that in fact we can generate MCMC samples only once 
(for a reasonable large size, say 100,000) and then use these to approximate the log-
likelihood ratio.

\section{Potts models, still}
We applied the Newton algorithm and MCMCMLE (well, Charlie did) to a $32 \times 32$ 
4-color Potts model.  To our surprise, the MLE distribution appeared to be skewed and 
biased:
\begin{align*}
%\theta_{MLE} = (0.004626115, 0.003245121, -0.001108131, 1.06203 )^T
\theta_{MLE} = (0.00463, 0.00325, -0.00111, 1.062 )^T
\end{align*}
with a standard deviation of 0.028 for $\theta_*$ (the sample standard deviation of 
the 455 $\theta$ values we calculated).

So, the mean of the MLEs we calculate is significantly lower than the true value of 
1.098.

What now?

back to the line search.

\section{Markov chain convergence to equilibrium distribution}
We have proved that our simple line search algorithm will converge to the MLE in the 
deterministic setting when we know with certainty that every step length $\alpha_k$ 
we choose fulfills the curvature condition we specified.  However, in models we will 
be interested in, because we use MCMC methods to estimate all values in the curvature 
condition, we cannot be absolutely certain an $\alpha_k$ that we accept actually 
meets the condition.  

We can treat each updating iteration of the algorithm as a state in a MC chain, and 
then look for conditions to show that the MC chain converges to its equilibrium 
distribution, which must also be proved to exist.

This is really hard!!  At first we thought it might require some work with uniformly 
ergodic chain.  but we don't have the theorems to make this easy for us ...

\section{2-color Potts (Ising model)}  
We revisit the simpler two-color Potts model, otherwise known as the Ising model, to 
explore the behavior of our line search algorithm.  Keeping our parameter space in 
two-dimensions allows us to see if any of our steps are less than optimal.  For 
example, we want to see if we backtrack to correct for an earlier step that was too 
large.  

In addition, using a smaller parameter space will also speed up the MCMC sample 
generation, thus making it easier to test the algorithm from a variety of starting 
points as well as making Fisher information that would be required for a Newton 
update to be calculated.

We run our algorithm to see if we can get to a parameter value from which a single 
Newton update will get us to the MLE value (which we compute in advance by using the 
Newton algorithm, starting at the true parameter value).

\newpage
\section{Degeneracy, non-existence of MLEs, and GDORs}
This begins an entirely new area of research.  Up until now, we have assumed the 
existence of a unique MLE.  However, an MLE is not guaranteed to exist.

There have been three overlapping approaches to this issue: \citet{Handcock:Degeneracy} 
appears to be the first to define ``model degeneracy" from a theoretical view for 
social network models, \citet{Rinaldo:2009} and \citet{Geyer:gdor} explored the non-
existence of MLEs for exponential families in general, with \citet{Rinaldo:2009} 
focusing on a 9-node social network model and \citet{Geyer:gdor} expanding on several 
different generalized linear models.

Example of a degenerate social network model is in \citet{statnet-tutorial}.  As 
summarized by \citet{Rinaldo:2009}, model degeneracy refers to instances where

\begin{enumerate}
\item Random graphs generated from the fitted model lack variability, often only 
giving rise to empty graphs and complete or nearly complete graphs
\item MLE of $\theta$ does not exist.  OR HARD TO OBTAIN.  includes case where MCMC 
methods fails to converge.
\item fitted model makes the observed network very unlikely.
\end{enumerate}

\newpage
\section{GDOR}
We work through the major theorems that relate to the GDOR approach.

\subsection{Charlie's exponential family convention, 7/23/10}
The distribution with parameter value $\theta$ has a density \emph{with respect to 
the distribution with parameter value $\psi$} of the form
\begin{align*}
	f_\theta(\omega) = e^{ \inner{ Y(\omega), \theta - \psi } - c(\theta) + c(\psi)}.
\end{align*}
The emphasized portion above means that say we want to integrate $f_\theta(\omega)$ 
over the ``entire space", and this integrates to one, we mean (I think)
\begin{align*}
	\int f_\theta(\omega) \, d\omega = \int f_\theta(\omega) \, h(y) e^{ \inner{Y,
\psi} - c(\psi) } \, dy = 1.
\end{align*}
Substituting in for $f_\theta(\omega)$,
\begin{align*}
	\int  e^{ \inner{Y,\theta - \psi} - c(\theta) + c(\psi) }  \, h(y) e^{ \inner{Y,
\psi} - c(\psi) } \, dy = 1.
\end{align*}
Then taking $\log()$ of both sides,
\begin{align*}
	\log \left ( e^{ - c(\theta) + c(\psi) } \int  e^{ \inner{Y,\theta - \psi}}  \, h
(y) e^{ \inner{Y,\psi} - c(\psi) } \, dy \right ) = 0 \\
	  - c(\theta) + c(\psi) + \log \left ( \int  e^{ \inner{Y,\theta - \psi}}  \, h
(y) e^{ \inner{Y,\psi} - c(\psi) } \, dy \right ) = 0 \\
	  - c(\theta) + c(\psi) + \log E_\psi e^{ \inner{Y,\theta - \psi}} = 0 
\end{align*}
which finally leads to the result in equation (5) p. 268 that
\begin{align*}
	  c(\theta) = c(\psi) + \log E_\psi e^{ \inner{Y,\theta - \psi}}.
\end{align*}


\section{Thm 2.2 in Geyer:1990, 7/23/10}
Charlie's GDOR theorems rely heavily upon some results from his thesis \citep{Geyer:
1990}.  Adapted here 
using the notation from his GDOR paper,
\begin{theorem}
\begin{align*}
e^{c(\theta + s \delta) - bs} &\to 
		\begin{cases} 
			0 								& b > \sigma_c(\delta) \\
			e^{c(\theta)} P_\theta(Y \in H ) 	& b = \sigma_c(\delta) \\
			+\infty							& b < \sigma_c(\delta)
		\end{cases}
& \text{as } s \to +\infty.
\end{align*}
where $\delta$ is a non-zero direction, $C$ the convex support, and
\begin{align*}
	\sigma_C (\delta) = \sup_{y \in C} \inner{ y, \delta} \\
	H_\delta = \set{w: \inner{w, \delta} = \sigma_C(\delta) }.
\end{align*}
\end{theorem}
$H_\delta$ is the supporting hyperplane to the set $C$ with normal vector $\delta$.

\begin{proof}

\textbf{Case: $b = \sigma_C(\delta)$.}

Starting with density of the exponential family with parameter $\theta$, we know that
\begin{align*}
	e^{c(\theta)} = \int h(y) e^{\inner{y,\theta}} \, dy.
\end{align*}
So,
\begin{align*}
	e^{ c(\theta + s \delta ) - bs } &= \int h(y) e^{\inner{y,\theta + s \delta - 
bs} } \, dy. \\
									&= \int h(y) e^{\inner{y,\theta}  + s [ \inner
{y,\delta} - b ] } \, dy. 
\end{align*}
Multiply by $\frac{p_\theta(y)}{p_\theta(y)}$ to get
\begin{align*}
	e^{ c(\theta + s \delta ) - bs } &= \int h(y) e^{\inner{y,\theta}  + s [ \inner
{y,\delta} - b ] }  \frac{ h(y)e^{\inner{y, \theta} - c(\theta)} }{ h(y)e^{\inner{y, 
\theta} - c(\theta)} }\, dy \\
	&= \int e^{  s [ \inner{y,\delta} - b ] + c(\theta) }  h(y)e^{\inner{y, \theta} - 
c(\theta)} \, dy \\
	&= \E_\theta e^{  s [ \inner{Y,\delta} - b ] + c(\theta) }
\end{align*}
What happens as $s \to +\infty$?  We would like to reverse the order of taking the 
limit and expectation.  Fortunately, we have the monotone convergence theorem.  For $
\inner{y, \delta} \leq b$, we have a monotonically decreasing sequence of random 
variables.  For $\inner{y, \delta} > b$, the sequence is increasing.  Thus,
\begin{align*}
	\lim_{s\to \infty} \E_\theta e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= E_
\theta \lim_{s\to \infty} e^{  s [ \inner{Y,\delta} - b ] + c(\theta) }. 
\end{align*}
Ignoring the expectation and examining the just limit component of the above,
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= 
			\begin{cases} 
			0 								& \inner{Y,\delta} < b \\
			e^{c(\theta)} 		 			& \inner{Y,\delta} = b \\
			+\infty							& \inner{Y,\delta} > b.
		\end{cases}
\end{align*}
In the case that we are considering, however, $b = \sigma_C(\delta) = \sup_{y \in C}
\inner{y,\delta}$, so $\inner{Y,\delta}$ can never be greater than $b$, so the third 
case in the limit above is not possible.  Thus we can rewrite the result above 
succinctly as
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= I(\inner{Y,
\delta} = b ) e^{c(\theta)} 
	= I( Y \in H_\delta ) e^{c(\theta)}.
\end{align*}
Then returning to the full expression,
\begin{align*}
	\lim_{s\to \infty} \E_\theta e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= e^
{c(\theta)} P( Y \in H_\delta ).
\end{align*}

\textbf{Case: $b > \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\theta) }P(Y\in H_\delta) \right ) \cdot 0 = 0.
\end{align*}

\textbf{Case: $b < \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\theta) }P(Y\in H_\delta) \right ) \cdot \left ( + \infty \right ) 
= + \infty.
\end{align*}

Charlie does more work on this (p. 14) for the case when $\lambda(H_\delta) = 0$.
\end{proof}

\section{Theorem 3(a) and (c), \citet[p. 270]{Geyer:gdor}}
So what do we this theorem for?  It relates when $\delta$ is a direction of recession 
or not.  Theorem 3 in GDOR has says the following are equivalent:
\begin{itemize}
\item (a) There exists a $\theta \in \Theta$ such that the function $s \mapsto \ell
(\theta+s\delta)$ is nondecreasing on $\RR$.
\item (c) $\inner{Y-y, \delta} \leq 0$ almost surely for all distributions in the 
family.
\end{itemize}

The $\delta$ in (a) in the above is a direction of recession.  

The proof requires the following lemma, which a PROOF WILL BE NEEDED.
$\ell(\theta+s\delta)$ is non-dereasing if and only if $\lim_{s \to \infty} \ell
(\theta+s\delta) > -\infty$.

\textbf{From $(c) \to (a)$}:

Take $\E_{\theta + s \delta}()$ of $\inner{Y-y, \delta} \leq 0$.  Then
\begin{align*}
\inner{ \E_{\theta + s \delta} Y-y, \delta} \leq 0.
\end{align*}
Now, take the derivative of $\ell( \theta + s\delta)$ with respect to $s$.
\begin{align*}
\deriv{\ell( \theta + s \delta}{s}) &= \deriv{}{s} \left (   \inner{y, \theta+s
\delta} - c(\theta+s\delta)  \right )\\
	&= \inner{y, \delta} - \deriv{}{s} c(\theta+s\delta) \\
	&= \inner{y, \delta} - \inner{ \E_{\theta+s\delta}Y,\delta }\\
	&= - \inner{ \E_{\theta+s\delta}Y - y,\delta }.
\end{align*}
Applying the above from the assumption, we get that 
\begin{align*}
\deriv{\ell( \theta + s \delta)}{s} \geq 0.
\end{align*}
Thus $\ell(\theta+s\delta)$ is a non-decreasing function of $s$.

\textbf{From $(a) \to (c)$}:
\begin{align*}
	\ell( \theta+s\delta) &= \inner{y, \theta+s\delta} - c(\theta+s\delta) \\
	&= \inner{y, \theta} + s \inner{y,\delta} -bs +bs - c(\theta+s\delta) \\
	&= \inner{y, \theta} + s [\inner{y,\delta} -b]  - \log e^{c(\theta+s\delta) -bs}.
\end{align*}
We know from the above lemma that $\ell(\theta+s\delta)$ is non-dereasing if and only 
if $\lim_{s \to \infty} \ell(\theta+s\delta) > -\infty$.  This implies that in the 
above expression, for the right-most term, $b \geq \sigma_c(\delta)$, and also that
\begin{align*}
	\inner{y, \delta} - b \geq 0,
\end{align*}
or 
\begin{align*}
	b - \inner{y, \delta}  \leq 0.
\end{align*}
If the above is true, then 
\begin{align*}
	\sigma_C(\delta)  - \inner{y, \delta} \leq b - \inner{y, \delta}  \leq 0,
%#	\inner{y, \delta} - \inner{Y_{max},\delta } \geq 0.
\end{align*}
and recalling that $\sigma_C(\delta) = \sup_{y \in C} \inner{y, \delta}$, then
\begin{align*}
	\inner{Y - y,\delta } \leq 0.
\end{align*}
(with probability one---do I even need to add this).

\section{Theorem 6, \citet[p. 271]{Geyer:gdor}, 7/25/10}
And now the big one:
\begin{theorem}
If $\delta$ is a direction of recession, and
\begin{align*}
H = \set{ w \in \RR^p: \inner{w-y,\delta}=0 },
\end{align*}
and $P(Y \in H) > 0$ for some distribution in the family, and hence for all, then for 
all $\theta \in \Theta$,
\begin{align*}
\lim_{s \to \infty} f_{\theta+s\delta}(\omega) = 
			\begin{cases} 
			0 								& \inner{Y(\omega) - y ,\delta} < 0 \\
			\frac{f_\theta(\omega)}{P_\theta(Y \in H)} 	& \inner{Y(\omega) - y ,
\delta} = 0 \\
			+\infty							& \inner{Y(\omega) - y ,\delta} > 0.
		\end{cases}
\end{align*}
If $\delta$ is not a direction of constancy, then $s \mapsto P_{\theta+s\delta}( Y 
\in H)$ is continuous and strictly increasing, and $P_{\theta+s\delta}( Y \in H) \to 
1$ as $s \to \infty$.
\end{theorem}
Before going on to the proof, let's think about what this means.  Suppose $\delta$ is 
a DOR and not a DOC, because the latter is not interesting.  So, the MLE does not 
exist because the log-likelihood, despite being strictly convex, is strictly 
increasing when we go in the direction of $\delta$.  That's what DOR means.  So then 
what?

Well, we need to think first about $H$ again, this plane that is perpendicular to $
\delta$ on which the observed data $y$ lies.  Looking first at the last sentence of 
this theorem, it says that for the sequence of exponential families with parameter 
value $\theta+s\delta$, the probability of $Y$ occurring on the plane $H$ is strictly 
increasing, and in fact, going to 1.  This is what Charlie is referring to when he 
says things like ``the probability is accumulating on the boundary".  So, $H$ 
actually coincides with the boundary of the convex hull, the observed data $y$ sits 
is on this boundary (hence the MLE does not exist, by the usual MLE existence 
theorem, and hence a DOR exists).  When we take the parameter value further and 
further along in the direction of the DOR, the samples generated from that model will 
be increasingly cluster around that boundary face of the convex hull on which $y$ 
sits.

So how about the rest of the theorem?  We saw in Theorem 3 that $\inner{Y-y,\delta} 
\leq 0$ so the set where the above is $+\infty$ has probability zero.  I think the 
set for which $\inner{Y-y,\delta} = 0$ corresponds to a DOC, doesn't it?  Hmm, 
something's not right with my reasoning.

Charlie's paper says: the right side (excluding the 0 probability part) is the 
density of the conditional distribution given the event $Y \in H$ of the distribution 
having parameter value $\theta$, so we could denote the RHS by $f_\theta(\omega | Y 
\in H)$.  The exponential family with the \emph{full} natural parameter space (I'm 
glossing over some stuff) is the limiting condition model (LCM).  ``If an MLE exists 
for the LCM, then it maximizes the likelihood in the family that is the union of the 
LCM and the original family, and it maximizes the likelihood in the family that is 
the set of all limits of sequences of distributions in the original family.  When 
this happens, we say we have found an MLE in the Barndorff-Nielsen completion of the 
original family."

\begin{proof}
Hmm, I'm concerned about the different forms of $H$: are they actually all the same 
thing?  Well, carry on.

By Theorem 3, if $\delta$ is a DOR, then $\inner{Y-y, \delta} \leq 0$ which implies 
that $\inner{Y, \delta} \leq \inner{y, \delta}$.  So, the largest value that $\inner
{Y, \delta}$ can take on is $\inner{y, \delta}$.  Then $\sigma_C(\delta) = \inner{y, 
\delta}$.

Focus on the form of the density for $f_{\theta+s\delta}(\omega)$.  According the 
form of the relative density previously defined with respect to $\psi$,
\begin{align*}
 f_{\theta+s\delta}(\omega) &= e^{ \inner{y,\theta+s\delta - \psi} - c(\theta+s
\delta) + c(\psi)  } \\
 	&= e^{ \inner{y,\theta - \psi} -c(\theta) + c(\theta) + s \inner{y,\delta} - c
(\theta+s\delta) + c(\psi)  } \\
 	&= e^{ \inner{y,\theta - \psi} -c(\theta) + c(\psi) - c(\theta+s\delta) + s 
\inner{y,\delta} + c(\theta)   } \\
 	&= f_\theta(\omega) \frac{e^{c(\theta)}}{e^{ c(\theta+s\delta) - \inner{y,\delta}
s } }.
\end{align*}
This form is starting to look familiar, in particular, the denominator is from 
Theorem 2.2 from \citet{Geyer:1990} except with $\inner{y,\delta}$ instead of $b$ 
(where this 
$y$ is the dummy variable in the density, not the observed data).  So what happens as 
$s \to \infty$?  

It depends on how $\inner{Y(\omega),\delta}$ relates to $\sup_{y \in C}\inner{y,
\delta} = \inner{y,\delta}$, for the observed data $y$.  (our notation is getting a 
little confusing since $y$ can denote the observed data $y$ or the dummy variable in 
the density).  
\begin{align*}
	f_{\theta+s\delta}(\omega) = f_\theta(\omega) \frac{e^{c(\theta)}}{e^{ c(\theta+s
\delta) - \inner{y,\delta}s } } 
	\to	
			\begin{cases} 
			0 								& \inner{Y(\omega),\delta} < \inner{y,
\delta} \\
			\frac{f_\theta(\omega)}{P_\theta(Y \in H)} 	& \inner{Y(\omega) ,
\delta} = \inner{y,\delta} \\
			+\infty							& \inner{Y(\omega),\delta} > \inner{y,
\delta}.
	\end{cases}
\end{align*}
For $\delta$ a DOR but not DOC, we need to show that the $P_{\theta+s\delta}(Y \in H) 
\to 1$ 
as $s \to \infty$, where $H = \set{w \in \RR^p: \inner{w-y_{obs},d}=0}$.
\begin{align*}
 P_{\theta+s\delta}(Y \in H) &= \int_H e^{\inner{y, \theta+s\delta - \psi} - c(\theta
+s\delta) + c(\psi)} \, d\psi \\
		&= \E_\psi \left \{ I_H e^{\inner{y, \theta - \psi} +c(\psi) - [c(\theta+s
\delta) -s\inner{y,\delta}]} \right 
\}\\
		&= \E_\psi \left \{ I_H  \frac{e^{c(\theta)} }{ e^{ c(\theta+s\delta) -s\inner
{y,\delta} } } 
		e^{\inner{y, \theta - \psi} -c(\theta)+c(\psi)}  \right \}.
\end{align*}
Since the indicator function, $I_H$, fixes $\inner{ y, \delta}$ to be a constant, it 
can pulled out of the expectation, so that 
\begin{align*}
		 P_{\theta+s\delta}(Y \in H)
		 &= \frac{e^{c(\theta)} }{ e^{ c(\theta+s\delta) -s\inner{y,\delta} } }
		 \E_\psi \left \{ I_H   
		e^{\inner{y, \theta - \psi} -c(\theta)+c(\psi)}  \right \}, \\
		 &= \frac{e^{c(\theta)} }{ e^{ c(\theta+s\delta) -s\inner{y,\delta} } } 
		 P_\theta ( Y \in H ).
		 \end{align*}
 
Then applying Theorem 2.2 from Geyer:1990,
\begin{align*}
 P_{\theta+s\delta}(Y \in H)
		&\to \frac{1}{P_\theta(Y \in H)}  P_{\theta}(Y \in H) = 1
 \end{align*}
 as $s \to \infty$.  
 
 That this probability is strictly increasing follows directly from the DOR property 
which requires $\ell( \theta+s
\delta)$ to be strictly increasing.   
\end{proof}


\newpage
\section{Graphs of 5, 6, 7, 8, and 9 Nodes}
We want some toy examples to work with.  Some obvious ones:
\begin{itemize}
\item \citet{Handcock:Degeneracy} worked with a 7-node graph with (edges, two-stars) as 
statistics.
\item \citet{Rinaldo:2009} worked primary with a 9-node graph with (edges, triangles), 
but also with 7-node graph with (edges, triangles) and also with the same (edges, two-stars) that \citet{Handcock:Degeneracy} uses.
\end{itemize}

\subsection{Some Basics}
Why these choices of graphs?  9 nodes is not many at all!

For an undirected graph with $n$-nodes, there are $n \choose 2$ dyads.  For each dyad, 
an edge may be present or absent, so there are $2^{n \choose 2}$ possible graphs to 
consider.\\

\begin{table}[ht]
\caption{Combinatorics for undirected graphs}
\begin{tabular}{ccl}
\hline 
Nodes & Possible Edges & Total Graphs \\ [1ex]
\hline
5 & ${5 \choose 2} = 10$ & $2^{10} = 1024$ \\ [1ex]
6 & ${6 \choose 2} = 15$ & $2^{15} = 32,768$ \\ [1ex]
7 & ${7 \choose 2} = 21$ & $2^{21} = 2,097,152$ \\ [1ex]
8 & ${8 \choose 2} = 28$ & $2^{28} = 268,435,456$ \\ [1ex]
9 & ${9 \choose 2} = 36$ & $2^{36} = 68,719,476,736$ \\ [1ex]
\hline 
\end{tabular}
\end{table}

So, while a 5-node graph seems manageable, a 9-node graph is already getting difficult 
to handle.  For example, \texttt{as.integer(2\textasciicircum31)} returns \texttt{NA}.  
\textbf{Going forward, we work with the 9-node edge-triangle model}.

\begin{figure}[!h]
\centering
\includegraphics[height=3.75in,width=4.5in]{/Users/saipuck/Tako/RCode/GDOR/Figures/graph9-hull}
\caption{Sufficient statistics of 9-node graph gradiated by frequency of occurrence.  
Grey perimeter is the convex hull, orange points are points that fall on the boundary, 
red points are extreme points of hull.  
Note that points like $(21,4)$, while very close to the boundary are actually on the 
interior ($(21, 3.857)$ is on the boundary).
}
%\label{F:pottsimage}
\end{figure}

\subsection{What are useful plots to look at?}

The point of using a two-dimensional sufficient statistic is that they lend easily to 
plots.  So what are the plots to look at?

One plot of interest will be that of the sufficient statistics, performed by both 
\citet{Handcock:Degeneracy} and \citet{Rinaldo:2009}.  This requires enumerating all 
possible graphs and counting the sufficient statistics combination for each, a 
computing intensive task (my version for the 9-node graph is in \texttt{/Tako/RCode/
GDOR/Triangles/CountTriC-mcl.R}.  It required employing bitwise arithmetic in C and 
parallel computing using the package \texttt{mclapply} across an 8-core computer and 
took 1 hour to complete).


\subsection{\citet{Handcock:Degeneracy}}


What values of $\theta$ are \textbf{not} going to be interesting (realistic)?  \textbf
{The emphasis is on what parameter values will give unrealistic graphs, not necessarily 
problematic from a theoretical perspective.  Unrealistic = uninteresting.}  \\

On the parameter space ($\theta_1-\theta_2$ plane), plot light-dark shades 
corresponding to the probability of observing 
	\begin{itemize}
		\item empty graphs (that is, $P_\theta(X=0)$).  The dark regions correspond to 
high probability.  So, these values of $\theta$ are probably not very interesting or 
realistic.
		\item full graphs (that is, $P_\theta(X=1)$)
		\item minimal 2-star graphs
		\item maximal 2-stars graphs
		\item missing 1 edge graphs
		\item exactly 1-edge graphs
		\item missing 2 edge graphs
		\item exactly 2-edge graphs
		\item \textbf{Combine} all of the above graphs.  The dark regions are 
unrealistic.  So what is left as light corresponds to parameter values that may be of 
interest in practice.  This only leaves about one quarter of the parameter space, what 
Handcock calls the \textbf{``effective" natural parameter space}.
	\end{itemize}
	

For the 7-node graph with edges \& two-stars, there are 144 distinct pairs of 
sufficient statistics.  22 pairs of statistics are on the boundary (all on bottom curve 
of hull).  These correspond to graphs with the minimum possible number of 2-
stars for the given number of edges.

\textbf{Handcock's main point: $\mu$-parameter space is better.}  The axes are the sufficient 
statistics,  the feasible parameter space is the same as the convex hull.  Just inside 
the boundary of the hull corresponds to parameter values which generate the 
uninteresting graphs (he calls ``near-degenerate").

\textbf{One plot of interest: } Figure 4c, p 24.  ``Stealth-bomber" region of non-
degenerate models with realizable MLEs superimposed.  

\subsection{8/29/10: Perhaps the final prize?}
In the natural parameter space, a plot that shows CI towards each face?  not unlike 
Handcock's cumulative plot in goal, but with more solid theoretical justification.  
Now, just thinking about this for a minute, this plot as I describe it is impossible, 
certainly as a single plot.  
\subsubsection{Natural vs. mean parameter space} So if we do mean parameter space, I 
came up with some single dot that represented the lower-95\% confidence bound.  What 
does that mean??  What's the interpretation of this??

\section{\citet{Rinaldo:2009} and Shanon Entropy plots}  
What's the point of the entropy plot?

\newpage
\section{A Bit of Computational Geometry}
In order to pursue the GDOR approach further, we also need to explore the theory of 
computational geometry/convex analysis.
The \texttt{rcdd} package in R \citep{rcdd} fortunately(!) does all the work for us but 
we should know what it is doing.

\subsection{V-representation}
$m = [l \quad b \quad V ]$.

First column: $l$, either 0 or 1.  A zero means nonnegative coefficients only.    

Second column: $b$, either 0 or 1.  A one means that the point in this row is included 
in the linear combination of points summing to 1 (across rows).

When the we have the summary data of all graphs in the form (edges, triangles, count of graphs), then the 
first two columns of this data determine the set of interest.  To complete the V-
representation of the set, we simple put (in R notation)
\begin{align*}
m = c(0, 1, edges, triangles )
\end{align*}

\subsection{H-representation}

The H-representation is a set of inequalities with form $(l \quad b \quad -A)$, where
$Ax \leq b$ ... 

First column: $l$.  Either 0 or 1.  A zero means inequality, $Ax \leq b$, a one means 
equality, $Ax = b$.

Second column: $b$.  

\subsection{Equivalence H-rep to V-rep} 

Going back and forth between the H-rep and V-rep is not an entirely trivial process---
we rely upon the Minkowski-Weyl theorem \citep[Theorem 19.1]{Rockafellar:1970}.  
Charlie of course does this for us with the \texttt{scdd} function in the \texttt{rcdd} 
package.

\subsection{Tangent and normal cone}
Hasn't this already been formally defined?

The tangent cone of a convex set $C$ at a point $x \in C$ is
\begin{align*}
	T_C(x) = cl\{ s(y-x): y \in C, s \geq 0 \}
\end{align*}

Normal cone
\begin{align*}
	N_C(x) 	&= T_C(x)^* \\
			&= \{ w: \inner{ w, y-x } \leq 0: y \in C \}  \\
			&= \{ w: \inner{ w, v } \leq 0: v \in T_C(x) \}.
\end{align*}

\subsection{Special case: polyhedral convex sets}
``The \emph{relative interior} of a convex set $C$, denoted $\rintr C$, is 
its interior relative to its affine hull Every 
nonempty convex set has a nonempty relative interior" \citep{Geyer:gdor, Rockafellar:1970}.  
``We say a vector $\delta$ is a \emph{generic direction of recession} (GDOR) if $\delta 
\in \rintr N_C(y)$ and $N_C(y)$ is not a vector subspace, where $C$ is the convex 
support and $y$ an observed value of the natural statistic such that $y \in C$.


For \emph{polyhedral} convex set, we can represent the convex set $C$ with a set of 
inequalities and equalities (This is the H-representation):
\begin{align*}
	C = \{ x: \inner{a_i, x} \leq b_i, i \in I \text{ and } \inner{a_i, x} = b_i, i 
\in E \}.
\end{align*}
  Then, to find the tangent cone at at particular point 
$x \in C$, 
\begin{align*}
	C = \{ x: \inner{a_i, x} \leq 0, i \in A \text{ and } \inner{a_i, x} = b_i, i \in 
E \},
\end{align*}
where $A = \{ i \in I: \inner{a_i,x} = b_i\}$.  Essentially, we take the active set of 
inequality constraints (the set $A$), drop the inactive constraints. From a practical standpoint, we are deleting these inactive constraints from our H-rep matrix.  Finally, $x$ is 
made the new origin (hence the $b_i$ are now all 0).

Then the V-representation of the normal cone can just be gotten direction from this 
representation of the tangent cone,
\begin{align*}
	N_C(x) = \pos( \{ a_i: i \in A \cup E\} \cup \{ - a_i: i \in E \} ).
\end{align*}

A word of JUSTIFICATION on why this is true ...

\section{Example: 9-node graph with edges and triangles}
\subsection{Boundary points}
\begin{verbatim}
m <- cbind( 0, 1, indices )
> head( m )
      edges triangles
1 0 1     0         0
2 0 1     1         0
3 0 1     2         0
4 0 1     3         0
5 0 1     4         0
6 0 1     5         0

out <- scdd( d2q(m), representation = "V" ) # to get H-rep
> out
$output
     [,1] [,2]    [,3]    [,4]
[1,] "0"  "136"   "-6"    "1" 
[2,] "0"  "540/7" "-27/7" "1" 
[3,] "0"  "0"     "0"     "1" 
[4,] "0"  "126"   "-17/3" "1" 
[5,] "0"  "0"     "7/3"   "-1"
[6,] "0"  "168"   "-7"    "1" 
attr(,"representation")
[1] "H"
b 	<- out$output[, 2]
v 	<- out$output[, -c(1,2) ]
a <- qneg(v) # a = -v, using rational
axb <- qmatmult(a, t(indices) ) # a %*% t(indices)
axb <- sweep( axb, 1, b, FUN = qmq ) # subtract
satisfy <- apply( axb, 2, function(foo) max(qsign(foo)) )
all( axb <= 0 )
boundary <- indices[satisfy==0,]
interior <- indices[satisfy<0,]
> boundary
    edges triangles
1       0         0
2       1         0
3       2         0
4       3         0
5       4         0
6       5         0
7       6         0
8       7         0
9       8         0
10      9         0
11     10         0
12     11         0
13     12         0
14     13         0
15     14         0
16     15         0
17     16         0
18     17         0
19     18         0
20     19         0
21     20         0
309    27        27
398    30        44
416    31        50
429    32        56
437    33        63
441    34        70
443    35        77
444    36        84
\end{verbatim}


\subsection{Extreme points}
\begin{verbatim}
out <- redundant( d2q(m), representation = "V" )
> out$output
     [,1] [,2] [,3] [,4]
[1,] "0"  "1"  "0"  "0" 
[2,] "0"  "1"  "20" "0" 
[3,] "0"  "1"  "27" "27"
[4,] "0"  "1"  "30" "44"
[5,] "0"  "1"  "32" "56"
[6,] "0"  "1"  "36" "84"
attr(,"representation")
[1] "V"
boundary.minimal <- q2d( out$output[ ,-c(1,2) ] )
> boundary.minimal
     [,1] [,2]
[1,]    0    0
[2,]   20    0
[3,]   27   27
[4,]   30   44
[5,]   32   56
[6,]   36   84
\end{verbatim}

\section{Tangent and Normal Cones using rcdd}
Presumably I'm going to insert some step-by-step R code here demonstrating it for our example.

\section{MLEs -- 8/10/10}
For 9 nodes, the sample space has a manageable size (about 68B different graphs).  So, 
we can explicitly evaluate the log-likelihood function.  In this special setting, we 
can in fact calculate MLEs for all possible values of the observed data when the MLE exists.
This should be straightforward---we have the objective function, find the maximizing value.

If we use the log-likelihood as the objective function,
\begin{align*}
	\ell( \theta) &= \inner{t(y_{obs}), \theta} - \log \kappa( \theta ) \\
				&=  \inner{t(y_{obs}), \theta} - \log \sum_{y \in \YY}e^{\inner{t(y),
\theta}}.
\end{align*}
  Since we have already enumerated all 444 possible graphs, counting how many times 
each occurs, we can simplify this further, 
\begin{align*}
	\ell( \theta) &= \inner{t(y_{obs}), \theta} - \log \sum_{t \in \T}e^{\inner{t,
\theta}} \nu(t),
\end{align*}
where $\nu(t)$ is the frequency of occurrence of statistic $t$ for all the graphs in $
\YY$.  Since $|\TT| = 444$ is much less than $|\YY|$, the calculating will be much 
quicker.
Because we are also concerned about overflow possibilities where $e^x$ can become too 
large (See Charlie's arithmetic.pdf slides), we can make a simple adjustment for this, 
\begin{align*}
	\ell( \theta) &=  \inner{t(y_{obs}), \theta} - a - \log \sum_{t \in \TT}e^{\inner
{t,\theta}-a}\nu(t),
\end{align*}
where $a = \max_y \inner{t(y),\theta}$.  If we use the \texttt{trust} package, we will 
also need to supply first and second derivatives of the objective function (\texttt
{optim} doesn't need these).  We have seen this before,
\begin{align*}
	\nabla \ell( \theta ) &= t(y_{obs}) - \nabla \log \kappa \\
						&=  t(y_{obs}) - \E_\theta t(Y). 
\end{align*}
and
\begin{align*}
	\nabla^2 \ell( \theta ) &= - \nabla^2 \log \kappa = - \Var_\theta t(Y)\\
					&= -I(\theta),
\end{align*}
where $I(\theta)$ is Fisher information.  So, we need to come up with formulas for $\E_
\theta t(Y)$ and $\Var_\theta t(Y)$.  From standard probability theory, 
\begin{align*}
\E_\theta t(Y) &= \sum_{y\in Y} t(y) P( Y = y )\\
				&= \sum_{y\in Y} t(y) \frac{1}{\kappa(\theta)} e^{\inner{t(y),
\theta }} = \frac{1}{\kappa(\theta)} \sum_{y\in Y} t(y)  e^{\inner{t(y),\theta }} \\
				&= \frac{1}{\kappa(\theta)} \sum_{t\in \TT} t  e^{\inner{t,\theta }}
\nu(t) \\
				&= \frac{1}{\sum_{t\in \TT} e^{\inner{t,\theta }}\nu(t)} \sum_{t\in 
\TT} t  e^{\inner{t,\theta }}\nu(t) \\
				&= \frac{1}{e^a\sum_{t\in \TT} e^{\inner{t,\theta}-a}\nu(t)} e^a\sum_
{t\in \TT} t  e^{\inner{t,\theta}-a}\nu(t) \\
				&= \frac{1}{\sum_{t\in \TT} e^{\inner{t,\theta}-a}\nu(t)} \sum_{t\in 
\TT} t  e^{\inner{t,\theta}-a}\nu(t).
\end{align*}
For $\Var_\theta t(Y)$, we again begin with the definition, 
\begin{align*}
\Var_\theta t(Y) &= \E (t(y) - \mu)( t(y) - \mu)^T \\
				&= \sum_{y \in \YY} (t(y) - \mu)( t(y) - \mu)^T P(Y=y)\\
				&= \frac{1}{\kappa(\theta)} \sum_{t\in \TT}
					(t - \mu)( t - \mu)^T e^{\inner{t,\theta}} \nu(t).
\end{align*}
This can be handled similarly in R.

Note that since $\nabla(\theta) = - I(\theta)$ as well, it may be worth keeping this 
calculation around in case we become more interested in $I(\theta)$.
Charlie says that when $\hat{\theta}$ is the MLE in the B-N completion,
\begin{align*}
	I(\hat{\theta}) \delta &= 0 \\
	\tau^T I(\hat{\theta}) \tau \delta &= \Var{\hat{\theta}}\inner{y, \tau}
\end{align*}
where $\tau$ is the vector along the relevant face orthogonal to the GDOR $\delta$.

\subsection{Optimization issues --- 8/12/10}
Because we can evaluate the normalizing constant and hence log likelihood, we should be able to do optimization in 
the traditional way using \texttt{trust} or \texttt{optim} in R.  Yet what we found is 
that for points such as (21,4) which are very close to the boundary of the convex hull, 
these methods fail to converge.  The function is strictly convex---theory guarantees us 
this---but from a practical standpoint, the log-likelihood looks completely flat.  

Evaluating the log-likelihood at points on a grid with grid size $0.01 \times 0.05$, gets us 
an estimated MLE at $(28.83, -7.75)$ with log-likelihood $-8.077761$.  But \texttt
{optim} and \texttt{trust} both fail to converge when starting from $(0,0)$.   Even 
when kindly told to start their searches from $(25, -5)$, \texttt{optim} converges to $
(24.443317, -6.653155)$ with a log-likelihood value of $-8.078375$, and \texttt{trust} 
converges to $(24.564363, -6.683482)$ with log-likelihood $-8.078339$.  
\begin{figure}[!h]
\centering
\includegraphics[height=3.75in,width=4.5in]{/Users/saipuck/Tako/RCode/GDOR/Figures/graph9-contour-wide}
\includegraphics[height=3.75in,width=4.5in]{/Users/saipuck/Tako/RCode/GDOR/Figures/graph9-contour-narrow}
\caption{Contour plot of log-likelihood when $y_{obs} = (21,4)$.  The log-likelihood 
function is nearly flat near the maximum point of $(28.83, -7.75)$ before dropping of 
rapidly.  Points such as $(24.44, -6.65)$ have nearly the same value for log
likelihood. }
%\label{F:pottsimage}
\end{figure}

\textbf{8/12: Question:  Find the GDOR for the face that this point $(21,4)$ is close to.  what happens to $\ell(\theta + s \delta)$?}

\section{Face points}
For a given observed value $y_{obs}$, finding the sample points on the face on which 
this point lies is not as easy as I initially thought.

\section{MLE in LCM (Limiting Conditional Model)}

When $y_{obs}$ is on the boundary of the convex hull of the sample space, theory (and our efforts in R!) say that the MLE does not exist.  According to \citet{Geyer:gdor}, the MLE exists in the Barndorff-Nielsen completion.  
\begin{itemize}
\item What is the Limiting Conditional Model?  Sure, you've got some formulas.  But equate the formulas again 
with what is actually happening.  Go back to \citet[Theorem~6]{Geyer:gdor}.  LCM is 
defined on p. 271.

From p. 277.  ``We constrain certain components of the response vector to their 
observed values.  This can be achieved by removing those components from the response 
vector and proceeding as if the resulting subvector were the entire response vector."
\item Find the MLE in this completion is the same as before.  We just have a different exponential family that we are trying to maximize.
\end{itemize}

From p. 278.  ``This LCM produced by deleting some components of the response will 
always be non-identifiable, because $\delta$ will always be a direction of constancy 
for the LCM and there may be other directions of constancy."

So now you've found $\hat{\theta}$, the MLE in the LCM.  Now what?  Look at $\ell(\hat
{\theta} + s \delta)$.

\section{Confidence intervals}
Once we have found $\hat{\theta}$, the MLE in the LCM, we can then go about 
constructing one-sided CIs.  \textbf{(This is the end goal of all this work, and yet it 
is unclear to me how to present it for a multidimensional parameter}.  Need to read 
Charlie's supplemental paper, \citet{geyer:2008}.)

If $\hat{\theta}$ is the MLE in the LCM, then so is $\hat{\theta} + \gamma$, where
$\gamma \in \Gamma_{\lim}$, the constancy space of the LCM (which is spanned by the 
GDOR of the original model).

Look at what happens to $\ell(\hat{\theta} + \gamma + s \delta)$ as $s$ increases.  
Should go off to $+\infty$.  Find the value of $s$ for which $P_{\hat{\theta} + \gamma 
+ s \delta}(Y \in H )$, the probability that the distribution allocates to the event 
that a sample falls on the face of interest, is 5\%. (Confusing logic of why this is 
5\% and not 95\%.  Charlie reasoned it out in his office.  write this up ).

\section{MCMC sampling}
\citet[p. 11]{Handcock:Degeneracy} says, ``if the MCMC is based on a near degenerate 
model then the convex hull of the sampled statistics will be a small subset of the 
convex hull of the model."

\textbf{Try this out on a 9-node graph: 8/15/10.}

Yes, of course Handcock is correct.  We can see this without even going to the sampler. 
Using the MLE of $(28.83, -7.75)$ calculated for the observed statistics of $(21,4)$, 
we generate the following probabilities:
\begin{verbatim}
> prob.data[ prob.data$prob > 0.01, ]
    edges triangles       prob
21     20         0 0.33887159
82     21         4 0.39080408
133    22         8 0.20281287
179    23        12 0.05717414
> colSums( prob.data[ prob.data$prob > 0.01, ] )
     edges  triangles       prob 
86.0000000 24.0000000  0.9896627 
\end{verbatim}
These are not empty or full graphs, but these are the graphs on the face to which 
$(21,4)$ is very close.  So, assume we have the ideal sampler, we'll be seeing these 4 
points nearly 99\% of the time.  

But at this point, I think Charlie would ask, ``But what's wrong with that??"  The MLE 
is doing the right thing!  

\section{Working with MC samples}

\subsection{Sampler}
It is easy to construct an ideal MC sampler for this toy example where we have described the 
full space.  We know that there are 444 possible network statistics and can calculate 
the probability of observing each.  So, we can just use the \texttt{sample()} function 
from R (actually, I use \texttt{sample.init()}, even easier).

Charlie says to use \texttt{scdd} sparingly and using \texttt{redundant} as much as possible.  The linear programming in \texttt{scdd} is expensive when there are too many points.

\subsection{Gradient ascent}
Gradient ascent method does not appear to be working very well---lots of zigzagging.  Excessive 
zigzagging, and not moving efficiently to where we need to go.  

INSERT PICTURE.  can do better than this one.
\begin{figure}[!h]
\centering
\includegraphics[height=3.75in,width=4.5in]{/Users/saipuck/Tako/RCode/GDOR/Figures/contour31-50}
\caption{Contour plot of log-likelihood when $y_{obs} = (31, 50)$.  Using a steepest 
ascent algorithm to find a direction of recession is not very efficient because the 
updates will zigzag up the log likelihood with very small steps.}
%\label{F:pottsimage}
\end{figure}

What to do?  Use a regression direction!

Also, if we've identified a non-trivial face (i.e., not zero-dimensional = single point), then we can find the tangent and normal cones.  Use the direction in the normal cone!

\begin{itemize}
\item The space for direction of constancy in the LCM is spanned by the GDORs of the 
original model
\item New idea.  Running two optimizations simultaneously from same sampler.
\end{itemize}

For $y_{obs} = 
(31,50)$, starting at $\theta = (0,0)$, the MLE in the LCM is found to be $\hat{\theta} 
= (0, 0.0239735)$ and the normal/GDOR is $(6, -1)$ (that gives a slope of $-1/6 \approx 
-1.667$).  To get 5\% of the distribution on the face (consisting of 3 pts, $(30,44)$ 
and $(32,56)$ sandwiching $y_{obs}$), we a need to use a $\theta$ of about $(9.11, 
-1.49)$.

\begin{itemize}
\item If I run a regression through the $\theta$ points that I sample from, I get a 
slope of $-0.1758399$.  What gives??  Why is this not the same as the GDOR?
\end{itemize}


\section{8/24/10: Charlie comes to my office}
Phase I refers to the computational 
geometry aspect of the problem to identify the face.  This is done with the rcdd 
package, working with normal and tangent cones, etc.  It knows nothing about 
the likelihood function.  Phase II occurs after the face has been identified, and is 
the maximization of the likelihood on this face.

So far I have been just doing maximum likelihood (via gradient ascent).  I choose my 
search direction $p_k$ to be the steepest ascent direction.  But here Charlie is 
suggesting a vector in the normal cone.  Hmm!  
\begin{enumerate}
\item Draw sample.
\item Find the tangent and normal cones relative to $y_{obs}$.  
\item Choose a vector in the normal cone to be $p_k$.  Perhaps the average of the 
boundary rays (recall that the boundary rays can be DOR but not GDOR).
\item Move in the direction of $p_k$.  How much?  Who knows!  A lot.  We're going to 
treat it like a GDOR.
\item Resample.  Are there any rays outside of the \emph{cumulative} tangent cone to 
date?  If $p_k$ was an actual GDOR, there would not be!  
\item Make the new tangent cone, and hence normal cone.  
\end{enumerate}

\newpage
\section{LCM MLE Algorithm}
\noindent Get an initial value, $\eta_1$.  The observed data, $y_{obs}$, is of course known.\\ 
\textbf{Sample} $Y_1, \ldots, Y_m$ from the distribution at $\eta_{1}$.\\
Make a V-rep from this sample.\\
Reduce the sample using \texttt{redundant}.\\
These non-redundant vertices are the convex hull. Store them in \texttt{ty.hull}.\\

\noindent \textbf{Estimate} $\nabla \ell( \eta_1)$, the direction of steepest ascent, and $\lVert \nabla \ell( \eta_1) \rVert$ using the sample.\\
Set $p_1 = \nabla \ell( \eta_1)$. \\
Set $k=2$, $LCM.k=1$. \\

\noindent \textbf{while}  $\lVert \nabla \ell( \eta_k) \rVert > \epsilon$? \\ 
\hspace{4mm} \indent	 \textbf{Find} a step size $\alpha_k$ that satisfies the \textit{curvature condition}
\begin{align*}
	 0 & \leq \nabla \ell( \eta_k + \alpha_k p_k)^T p_k \leq c \nabla \ell(\eta_k)^T p_k
\end{align*}
\indent for some $0 < c < 1$.  This is not as easy a problem as it may seem---\\
\indent CJG thinks simulated tempering will be required to do this well.\\

$\eta_{k+1} = \eta_k + \alpha_k p_k$.\\
\indent \textbf{Sample} $Y_1, \ldots, Y_m$ from the distribution at $\eta_{k+1}$.\\
\indent Make a V-rep from this sample.\\
\indent Reduce the sample using \texttt{redundant}.\\
\indent Combine non-redundant sample with \texttt{ty.hull} and apply \texttt{redundant} again.\\

\indent \textbf{H-rep} from reduced sample \texttt{scdd}.  \\
\indent \indent Is $y_{obs}$ outside the convex hull of the sample?\\
\indent \indent \textbf{Yes.}  Keep sampling.  Skip to estimating $\nabla \ell( \eta_{k+1})$.\\
\indent \indent \textbf{No, it's inside.}  Then MLE exists, finding it should be straightforward.\\  \indent \indent Skip to estimating $\nabla \ell( \eta_{k+1})$.\\
\indent \indent \textbf{No, it's on the border.}  Then 
\indent \indent \begin{enumerate}
\item either the MLE exists but we got a sample that just touches $y_{obs}$, 
\item the MLE does not exist since $y_{obs}$ is itself on the border of the convex hull of the entire sample space.
\end{enumerate}

\indent \indent We will assume for now that (1) is not possible when we have 10\% of the sample touching $y_{obs}$.  It would seem very unlikely that we would have such a large number of points hitting $y_{obs}$ and points on its perceived face but not beyond it.\\
\indent \indent So, we are in case (2), the MLE does not exist.\\
\indent \indent Apply the function \texttt{linearity} on the rays formed from the cumulative convex hull relative, \texttt{ty.hull}, relative to $y_obs$.  \\
\indent \indent \texttt{linearity} returns only 1 point.  We have either a single-point face, or, we haven't sampled enough yet so we're only touching one point ($y_{obs}$) of the face.

\indent \indent \texttt{linearity} returns multiple points.


\indent Calculate $\nabla \ell( \eta_{k+1})$.\\
\indent \textbf{Find} the new search direction $p_{k+1}$, which must be an ascent direction. \\
\indent $k = k + 1$.  \\
\textbf{end while}\\


Do gradient search optimization, with every 3rd search direction chosen by regression.  
This search direction should approximate GDOR.

Is $y_{obs}$ in the sample?

\subsection{Got 10\%.  Now what?}

\textbf{Q: Why wouldn't we use the normal cone from this face as our next search 
direction?}
I think we should.  But if we are using uniroot, this will give us an infinite step 
size so be careful what you wish for!

We look to find the MLE in LCM using this subsample.  So, we now compute a new search 
direction
\begin{align*}
	p_{k+1}^{LCM} = \nabla \ell^{LCM}(\theta_{k+1}) = t(y_{obs}) - \E_{\theta_{k+1}, 
LCM}t(Y)
\end{align*}

\newpage
\section{Next steps, as I see it on 8/31/10:}
\begin{enumerate}
\item Make sure every case works---$y_{obs}$ on any boundary, extreme pt.  Even points 
like (21,4).  The algorithm needs to be generalized.  Record some pictures!
\item Turn on MC for estimation of gradient.  
\item MCMC?  Can we use the ergm sampler?
\item 3-dimensional statistics?
\item Bridge sampling, reverse logistic regression, Tan, P. McCullaugh, Ming.
\item simulated tempering.
\end{enumerate}

\section{STILL TO DO}

\begin{itemize}
\item Write to file a mapping of network statistics--(21,4)--to observed graph index, 
g.  The function is already written \texttt{/Tako/RCode/GDOR/Triangles/
FindGraphforFixedEdgesC.R}.
\item SUPER martingale as part of proof for convergence?
\item what happens to H-rep inequalities when you go from 5 to 7 to 9 nodes?
\item Algorithm R from Charlie's Thesis, Ch 2?  GDOR from linear programming?  should 
be easier because Charlie says at the time he didn't know how to handle ... what case?  
hmm.  something to do with infinite or unbounded.
\end{itemize}



\bibliographystyle{apalike}
\bibliography{/Users/saipuck/Tako/THESIS/References}
\end{document}
