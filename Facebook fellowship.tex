\documentclass[12pt]{article}

\usepackage{amsbsy,amsmath,amsthm,amssymb,verbatim,color}

\usepackage{geometry}
\geometry{hmargin=1in,vmargin={1in,1in},footskip=.5in}
\usepackage[numbers]{natbib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{center}
{\LARGE{\textbf{Social Computing: Constructing a}}} 

\vspace*{2mm}
{\LARGE{\textbf{Social Network Model}}}

\vspace{0.25in}
{\large{\textbf{Saisuke Okabayashi}}} \\
\vspace{0.05in}
{\large{{University of Minnesota, Department of Statistics}}}\\
\end{center}

%``clearly identifies the area of focus and applicability to Facebook"

\section{Background}
Is it possible to build a mathematical model that captures the behavioral tendencies of individuals in how they form relationships?

Studies of social networks typically look to identify gender, age, ethnicity, and other individual-specific attributes to explain which relationships form.  However, researchers have come to realize that each potential relationship cannot be studied in isolation; rather, relationships form in an interdependent manner, necessitating a new ``network" perspective that recognizes relationship structures as essential factors in the analysis.  
Sociologists have developed descriptive measures of network structures to correspond to social theory ideas.  For example, the frequency of triangular structures, where each individual in a group of three individuals has relationships with the other two, may measure the tendency of individuals to form \textit{transitive} relationships, where because $A$ is friends with $B$ and $B$ is friends with $C$, $A$ is friends with $C$.  %These measures can be used to characterize an observed network data set, capturing qualities beyond the individual specific attributes.

%%%%%% STATISTICAL MODEL $$$$$$$
A statistical model provides further clarity of the underlying forces that shape the structure of an observed network.   
For example, the frequency of triangles might instead be explained by the tendency of individuals to form relationships with others of the same gender, an alternative theory to the transitivity one referred to above.  A good statistical model can see past the noise in the data to identify if one or both of these forces are important.   
In addition, statistical models can simulate new random networks whose distribution retains essential characteristics of the observed network.  Researchers can use these to further test hypotheses about the process of relationship formation.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Potential applicability of research to Facebook}
Facebook could employ statistical models to complement their current network analyses.  The simulation of random networks can provide a framework for testing the effectiveness of new advertising strategies, giving a distribution of performance measures.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research goals: model calibration and degeneracy}
We restrict our attention to networks at one point in time where relationships are either present or absent---like a snapshot of a friendship network on Facebook.  \textit{Exponential random graph models} (ERGMs) are the most commonly used framework for social networks and are the focus of our research.  The attractiveness of ERGMs stems from their flexibility in allowing one to handpick \textit{network statistics}, or relationship structures of interest, which, when paired with their \textit{parameters}, or coefficients, completely define the model.

%\subsection{A new algorithm for calibrating social network models}
Our first area of research is to develop a new algorithm for calibrating social network models to data sets.  Due to the complexity of the underlying phenomenon, there are no simple methods for finding the parameter values, called \textit{maximum likelihood estimators} (MLEs), which maximize the probability of generating the observed data.  All methods rely upon computer simulation techniques called \textit{Markov chain Monte Carlo} (MCMC) to estimate quantities of interest.  Software packages such as \texttt{statnet} in the \texttt{R} platform have only recently implemented methods based on \citeauthor{Geyer:1992}'s (\citeyear{Geyer:1992}) MCMC-MLE algorithm \citep{statnet:R}.  This method, while theoretically guaranteed to converge to the MLE from any initial value of the parameters, in practice tends to be very sensitive to the initial guess and can even fail to converge when the guess is poor.
%The MCMCMLE algorithm uses MCMC to estimate a ratio of two models with different values and then uses conventional optimization methods on this resulting ratio.  

We apply MCMC in a different way than the MCMC-MLE algorithm, employing it to estimate the optimal \textit{search direction} for our next guess of the parameter value.  The algorithm proceeds by iterating through these search directions.  Even when given a poor initial guess, we have established criteria guaranteeing that it will still make sufficient progress towards the MLE.  

%Because our line search algorithm is less efficient than the MCMCMLE algorithms when in close proximity to the MLE, we believe it can be used to complement the MCMCMLE methods; our line search algorithm could be used initially and then after sufficient progress has been made, a switch can be made to the MCMCMLE algorithm.

%\subsection{Model degeneracy}
Our second area of research is to devise methods for clarifying network model pathologies.
Researchers have occasionally encountered undesirable behavior with these models that extends beyond the problem of model calibration, ranging from cases where the MLE does not exist to cases where a properly fitted model produces random networks that look nothing like the original network data set \citep{Rinaldo:2009}.  These have loosely been swept under the umbrella of ``model degeneracy."  The problem typically arises from a poor combination of network statistics that specify the model---it turns out that just because one \textit{can} build a model from any choice of network statistics, it does not mean that one should.

Most attempts to address this problem have focused on introducing new sets of network statistics that appear to circumvent the pathologies described \citep{Hunter:2006, Morris:2008}.  We would like to conduct a deeper exploration of the underlying issues, which revolve around the geometric properties of the space defined by the selected network statistics.  In addition to providing a clear understanding of which type of degeneracy has occurred, we would like to devise a method to calculate confidence intervals for parameter estimates, even in cases when the MLEs themselves cannot be found.  

%Even when the models have been well-calibrated (the MLEs have been well approximated), researchers have discovered that they may exhibit a particularly troubling pathological behavior: the model may generate only networks that look nothing like the original data!  In these situations, the simulated networks are often fully connected or nearly empty.  The phenomenon has been referred to under the broader umbrella of model ``degeneracy", a term which is associated with this and other model construction problems.

%In this case, the pathology is brought on by a poor combination of network statistics in specifying the model.  While the researcher may have had good reason for being interested in the particular network structures, like chemical compounds that individually may be harmless but are dangerous together, the resulting model produce undesirable results.  

%The deeper issue is that the underlying geometry of the parameter space for social network models is not well understood.  Existing theory describes conditions when MLEs will not exist, 

%Clarify definitions, 

%\textit{network structures} triangles, k-stars

%The science of building models that can capture such complex behaviors of the collective individual decisions

%\section{Design and methodology}
\section{Progress to date and schedule for completion}
We have completed a proof for the convergence of the line search algorithm we described above in a deterministic setting (e.g., logistic regression).  Further, we have empirical evidence based on numerous simulations that the method converges for an MCMC setting.  However, a proof for convergence in the MCMC setting is still being pursued.  

We have only just begun exploring the issues of degeneracy.  We will look to extend \citeauthor{Geyer:gdor}'s (\citeyear{Geyer:gdor}) method on exponential family confidence intervals to social network models.

We expect to complete research in the areas above by May 2011.

\newpage
\bibliographystyle{apalike}
\bibliography{References}

%\begin{thebibliography}{77}

%
%\bibitem{kend}
%Kendall, W. S. 2004.
%\newblock Geometric ergodicity and perfect simulation,
%\newblock {\sl Electronic Communications in Probability}, 9:140--151.

%

%\bibitem{jones}
%Jones, G. L. (2004)
%\newblock On the {M}arkov chain central limit theorem,
%\newblock {\sl Probability Surveys}, 1:299--320.

%
%\bibitem{jone:hobe:2004}
%Jones, G. L. and Hobert, J. P. (2004).
%\newblock Sufficient burn-in for {G}ibbs samplers for a hierarchical random effects model, 
%\newblock {\sl The Annals of Statistics}, 32:784--817.

%\bibitem{prop:wils:1996} 
%Propp, J. G. and Wilson, D. B. (1996).
%\newblock Exact sampling with coupled {M}arkov chains and applications to statistical mechanics,
%\newblock {\sl Random Structures and Algorithms}, 9:223--252.
% 
%\end{thebibliography}




\end{document}
%:
