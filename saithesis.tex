\documentclass[oneside]{myumnStatThesis}
%\usepackage{epsfig}
\graphicspath{{Figures/}}
\usepackage{graphicx}
\usepackage{epsfig,color}
\usepackage{algpseudocode}
\usepackage{multirow}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\I}{I}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\bd}{bd}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\rint}{rint}
\DeclareMathOperator{\con}{con}
\DeclareMathOperator{\pos}{pos}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\lev}{lev}

\def\RR{{\mathbb R}}
\def\ZZ{{\mathbb Z}}
\def\DD{{\mathcal D}}
\def\XX{{\mathcal X}}
\def\YY{{\mathcal Y}}
\def\TT{{\mathcal T}}
\def\NN{{\mathcal N}}
\newcommand{\deriv}[2]{\frac{d #1}{d #2}}
\newcommand{\dderiv}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\ppderiv}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\ppmderiv}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\fatdot}{\,\cdot\,}
\newcommand{\inner}[1]{\langle #1 \rangle}
\newcommand{\set}[1]{\{\, #1 \,\}}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\etaMLE}{\hat{\eta}_{\textrm{MLE}}}
\newcommand{\betaMLE}{\hat{\beta}_{\textrm{MLE}}}
\newcommand{\thetaLCM}{\hat{\theta}_{\textrm{LCM}}}
\newcommand{\etaLCM}{\hat{\eta}_{\textrm{LCM}}}
\newcommand{\yobs}{y_{\textrm{obs}}}
\newcommand{\Gammalim}{\Gamma_{\textrm{lim}}}
\newcommand{\CLCM}{C_{\textrm{LCM}}}



\author{Saisuke Okabayashi}
\adviser{Charles J. Geyer}
%\coadviser{Co-Adviser Name Here}
\title{Maximum Likelihood in 
 Social Network Models}
\month{May}
\year{2011}
% Month and Year of Degree Clearance, NOT necessarily when you defended

\begin{document}

%\makesignaturepage % required
\maketitlepage % required
%\makecopyrightpage % recommended, required if registering copyright
%\frontmatter
%\begin{acknowledgementspage} % optional
%I owe all of my success to Alicia who is awesome.
%\end{acknowledgementspage}

\begin{abstract}
\input{abstract}
\end{abstract}

\tableofcontents % required

\newpage
\chapter*{List of Tables}
\addcontentsline{toc}{chapter}{List of Tables}
{\def\chapter*#1{}
\listoftables}

\newpage
\chapter*{List of Figures}
\addcontentsline{toc}{chapter}{List of Figures}
{\def\chapter*#1{}
\listoffigures}


\mainmatter
\onehalfspacing % SAI - REMOVE WHEN DONE EDITING
\small  % SAI - REMOVE WHEN DONE EDITING
\chapter{Introduction}
%\input{saithesis-intro}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background Theory}\label{Section:Background}
%\input{Background}
All of the parameter estimation issues discussed thus far for ERGMs are relevant to 
the larger class of discrete state space exponential family models.
These models are commonly used to model phenomena with dependent structure, 
where the outcomes of the response variable of interest are in fact dependent on one 
another.  For example, the Ising 
model \citep{Ising,Potts} is an exponential family model that has been used to model 
ferromagnetism.  
%A realized 
%sample from this model is depicted in Figure~\ref{F:pottsimage}, where neighboring 
%pixels (representing atoms in a crystal lattice) are more likely to have the same 
%color.  
%We explore this model further in Section~\ref{S:Examples:Ising}.
Other examples of phenomena with dependent structure modeled with exponential 
families include
plant ecology \citep{Besag:1974,Besag:1975} and the lifetime fitness of plants \citep
{Shaw:2008}.

Although motivated by ERGMs, the algorithm we propose is rooted in fundamental
exponential family theory and applicable to any discrete state space 
exponential family model.  Thus the theory presented in this section is from
the perspective of exponential families in general.

\section{Exponential Family Theory}
An exponential family of distributions \citep{Barndorff,Geyer:gdor}
on a sample space $\YY$ has log likelihood \eqref{E:loglike}.
%\begin{align} 
%   \ell(\eta) = \inner{g(y), \eta} - c(\eta)
%\end{align}
%where $g(y)$ is a $d$-dimensional vector of canonical statistics, $\eta$ a $d$-
%dimensional vector of
%canonical parameters, and $\inner{\fatdot, \fatdot}$ denotes the bilinear form
%$$
%   \inner{g, \eta} = \sum_{i =1}^d g_i \eta_i.
%$$
%So that the probability function integrates to 1,
%the cumulant function $c$ must have the form
%\begin{align} \label{E:cumulant}
%      c(\eta) = \log \left(\int e^{\inner{g(y), \eta}} \, d\mu(y) \right),
%\end{align}
%where $\mu$ is a measure on $\YY$.
%Define
%\begin{align} \label{E:fullparam}
%	\Xi = \{ \eta \in \RR^q: c(\eta) < \infty \}.
%\end{align}
%The exponential family is \emph{full} if the natural parameter space is \eqref
%{E:fullparam}, and \emph{regular} if, in 
%addition, $\Xi$ is an open set.  We say an exponential family is \emph{minimal} if $g
%(y)$ is not concentrated on a 
%hyperplane. Minimality guarantees that if an MLE exists, it is unique \citep
%{Geyer:gdor}.

In finite state space models with complicated dependence like an Ising model or 
exponential random graph model,  \eqref
{E:cumulant} is a sum which may have no simple expression and can only be evaluated by 
explicitly doing the sum.
When the sample space $\YY$ is even moderately large, this can be prohibitively 
expensive.  For example, an Ising model 
defined on a $32\times 32$ square lattice where each entry takes values of 0 or 1, 
there are $2^{1024} \approx 10^
{300}$ elements in $\YY$.  A loop with this many iterations takes too long no matter 
how programmed.

A useful property of all exponential families \cite[p.~27]{TPE2} on which we rely 
heavily is that 
\begin{align*}
	\E_\eta(g(Y)) &= \nabla c(\eta)	\\
	\Var_\eta(g(Y)) &= \nabla^2 c( \eta ).
\end{align*}

Thus we can express first and second derivatives of the log likelihood \eqref
{E:loglike} and Fisher information, $I
(\eta)$, as
\begin{align}
	\nabla \ell( \eta ) &= g(y) - \E_\eta g(Y) \label{E:nabla ell} \\
	\nabla^2 \ell( \eta ) &=  - \Var_\eta g(Y) \label{E:nabla2 ell} \\
	\I(\eta) &= -\E_\eta \nabla^2 \ell (\eta ) = \Var_\eta g(Y) \label{E:FI}
\end{align}
and thereby avoid evaluation of the problematic cumulant function $c$.


By the strict convexity of the log likelihood function ensured by \eqref{E:nabla2 
ell}, the global 
maximum, if it is exists, is attained when $\eta$ is such that $\nabla \ell( \eta ) = 
0$, or, by \eqref{E:nabla ell},
\begin{align}
	\E_\eta g(Y) = g(\yobs). \label{E:Observed-Expected}
\end{align}


\section{Convex Analysis}
The issue of MLE existence in the conventional sense in an exponential family is 
closely tied to the geometric properties of 
the convex support of the model \citep{Barndorff, Geyer:gdor, Rinaldo:2009}.  We 
describe the relevant theory from convex analysis as it pertains to the case of 
exponential families.

A \emph{convex polytope} $C$ is the convex hull of a finite set of points $V$,
\begin{align*}
	C = \con( V ).
\end{align*}
where [\textbf{DEFINE pos().}]

  By the Minkowski-Weyl theorem \citep[Theorem 19.1]{Rockafellar:1970}, this convex 
set can equivalently be represented as the intersection of a finite collection of 
closed half-spaces.  These two representations of a convex polyhedron are referred to 
as the V-representation and H-representation, respectively.  
%The V-representation of a convex polyhedron $C$ is the set of all linear combinations
%\begin{align*}
%	\sum_{i \in E \cup I} b_i \alpha_i
%\end{align*}
%where $\alpha_i$ are vectors, $b_i$ are scalars, $E$ and $I$ are disjoint finite sets 
%such that
%\begin{align*}
%	b_i \geq 0, \quad i \in E \cup I
%\end{align*}
%and if $I$ is nonempty
%\begin{align*}
%	\sum_{i \in I} b_i = 1.
%\end{align*}

The H-representation can be expressed as the solution set of a finite set of linear 
equations and inequalities,
\begin{align*}
	C = \{x: Ax \leq b \},
\end{align*}
where $A$ is a matrix and $b$ a vector.

A nonempty \emph{face} of a convex polyhedron $C$ is a convex subset of $C$ such that 
every line segment in $C$ with a relative interior point in $F$ has both end points in 
$F$ \citep{Rockafellar:1970}.  It is itself a convex polyhedron.
A \emph{proper} face is a face that is not the empty set or $C$, and 
\emph{facets} are proper faces of the highest dimension.

The \emph{tangent cone} of a convex set $C$ at a point $x \in C$ is
\begin{align*}
	T_C(x) = \cl\{s(w-x):w \in C \text{ and } s \geq 0 \},
\end{align*}
where $\cl$ denotes the closure operation \citep[Theorem 6.9]{Rockafellar}.  

The \emph{normal cone} of a convex set $C$ in $\RR^d$ at a point $x \in C$ is 
\begin{align*}
	N_C(x) = \{ \delta \in \RR^d: \inner{w-x,\delta} \leq 0 \text{ for all } w \in C 
\}.
\end{align*}

Tangent and normal cones are polars of each other, that is, each determines the other.  
The normal cone at $x$ can be defined in terms of the tangent cone at $x$ by
\begin{align*}
	N_C(x) 	&= \{ w \in \RR^d: \inner{ w, v } \leq 0 \text{ for all } v \in T_C(x) \}.
\end{align*}

DEFINE \emph{direction of recession}, \emph{direction of constancy}.  \textbf{get from 
\citep{Rockafellar:1970}.}

\section{MLE existence in exponential families}
Charlie's GDOR theorems rely heavily upon some results from his thesis \citep{Geyer:
1990}.  Adapted here 
using the notation from his GDOR paper,
\begin{theorem}[Theorem 2.2 in \citep{Geyer:1990}]
\begin{align*}
e^{c(\theta + s \delta) - bs} &\to 
		\begin{cases} 
			0 								& b > \sigma_c(\delta) \\
			e^{c(\theta)} P_\theta(Y \in H ) 	& b = \sigma_c(\delta) \\
			+\infty							& b < \sigma_c(\delta)
		\end{cases}
& \text{as } s \to +\infty.
\end{align*}
where $\delta$ is a non-zero direction, $C$ the convex support, and
\begin{align*}
	\sigma_C (\delta) = \sup_{y \in C} \inner{ y, \delta} \\
	H_\delta = \set{w: \inner{w, \delta} = \sigma_C(\delta) }.
\end{align*}
\end{theorem}
$H_\delta$ is the supporting hyperplane to the set $C$ with normal vector $\delta$.

\begin{proof}
\textbf{Case: $b = \sigma_C(\delta)$.}

Starting with density of the exponential family with parameter $\theta$, we know that
\begin{align*}
	e^{c(\theta)} = \int h(y) e^{\inner{y,\theta}} \, dy.
\end{align*}
So,
\begin{align*}
	e^{ c(\theta + s \delta ) - bs } &= \int h(y) e^{\inner{y,\theta + s \delta - 
bs} } \, dy. \\
									&= \int h(y) e^{\inner{y,\theta}  + s [ \inner
{y,\delta} - b ] } \, dy. 
\end{align*}
Multiply by $\frac{p_\theta(y)}{p_\theta(y)}$ to get
\begin{align*}
	e^{ c(\theta + s \delta ) - bs } &= \int h(y) e^{\inner{y,\theta}  + s [ \inner
{y,\delta} - b ] }  \frac{ h(y)e^{\inner{y, \theta} - c(\theta)} }{ h(y)e^{\inner{y, 
\theta} - c(\theta)} }\, dy \\
	&= \int e^{  s [ \inner{y,\delta} - b ] + c(\theta) }  h(y)e^{\inner{y, \theta} - 
c(\theta)} \, dy \\
	&= \E_\theta e^{  s [ \inner{Y,\delta} - b ] + c(\theta) }
\end{align*}
What happens as $s \to +\infty$?  We would like to reverse the order of taking the 
limit and expectation.  Fortunately, we have the monotone convergence theorem.  For $
\inner{y, \delta} \leq b$, we have a monotonically decreasing sequence of random 
variables.  For $\inner{y, \delta} > b$, the sequence is increasing.  Thus,
\begin{align*}
	\lim_{s\to \infty} \E_\theta e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= E_
\theta \lim_{s\to \infty} e^{  s [ \inner{Y,\delta} - b ] + c(\theta) }. 
\end{align*}
Ignoring the expectation and examining the just limit component of the above,
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= 
			\begin{cases} 
			0 								& \inner{Y,\delta} < b \\
			e^{c(\theta)} 		 			& \inner{Y,\delta} = b \\
			+\infty							& \inner{Y,\delta} > b.
		\end{cases}
\end{align*}
In the case that we are considering, however, $b = \sigma_C(\delta) = \sup_{y \in C}
\inner{y,\delta}$, so $\inner{Y,\delta}$ can never be greater than $b$, so the third 
case in the limit above is not possible.  Thus we can rewrite the result above 
succinctly as
\begin{align*}
	\lim_{s\to \infty} e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= I(\inner{Y,
\delta} = b ) e^{c(\theta)} 
	= I( Y \in H_\delta ) e^{c(\theta)}.
\end{align*}
Then returning to the full expression,
\begin{align*}
	\lim_{s\to \infty} \E_\theta e^{  s [ \inner{Y,\delta} - b ] + c(\theta) } &= e^
{c(\theta)} P( Y \in H_\delta ).
\end{align*}

\textbf{Case: $b > \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\theta) }P(Y\in H_\delta) \right ) \cdot 0 = 0.
\end{align*}

\textbf{Case: $b < \sigma_C(\delta)$}.
\begin{align*}
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - bs } &= 
	\lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s + \sigma_C
(\delta)s - bs }  \\
	&= \left( \lim_{s\to \infty} e^{ c(\theta + s \delta ) - \sigma_C(\delta)s} 
\right ) \left(  \lim_{s\to \infty} e^{ s[ \sigma_C(\delta) - b] } \right )\\
	&= \left (e^{c(\theta) }P(Y\in H_\delta) \right ) \cdot \left ( + \infty \right ) 
= + \infty.
\end{align*}
\end{proof}

The \emph{convex support} of an exponential family is the smallest closed convex set 
that the contains the natural statistic \citep{Geyer:gdor}.

\begin{theorem}[Theorem 1 from \citet{Geyer:gdor}] \label{Thm:DOC}
For a full exponential family with
\begin{itemize}
\item log likelihood function, $\ell(\eta)$, as in \eqref{E:loglike},
\item natural parameter space, $\Xi$, as in \eqref{E:fullparam},
\item natural statistic, $g(Y)$,
\item observed data, $\yobs$, such that $g(\yobs) \in C$, the convex 
support,
\end{itemize}
the following are equivalent:
\begin{enumerate}
\item $\delta$ is a direction of constancy of the log likelihood.
\item For all $\eta \in \Xi$, the function $s \mapsto \ell( \eta + s\delta)$ is 
constant on $\RR$ \cite[Theorem 1 (b)]{Geyer:gdor}.
\item The parameter values $\eta$ and  $\eta + s\delta$ correspond to the same 
probability distribution for all $\eta \in \Xi$ and all real $s$ \cite[Theorem 1 (d)]
{Geyer:gdor}.
\item $\inner{g(Y) - g(\yobs),\delta} = 0$ almost surely for all distributions in the 
family \cite[Theorem 1 (f)]{Geyer:gdor}.
\item $\delta \in N_C(g(\yobs))$ and $-\delta \in N_C(g(\yobs))$ \cite[Theorem 1 (g)]
{Geyer:gdor}.
\item $\inner{w,\delta} = 0$ for all $w \in T_C(g(\yobs))$ \cite[Theorem 1 (h)]
{Geyer:gdor}.
\end{enumerate}
\end{theorem}
The set of all directions of constancy is called the \emph{constancy space} of the log 
likelihood.

\begin{theorem}[DOR: Theorem 3 from \cite{Geyer:gdor}] \label{Thm:DOR}
For a full exponential family with the same setting as Theorem~\ref{Thm:DOC},
%\begin{itemize}
%\item log likelihood function, $\ell(\eta)$, described by \eqref{E:loglike},
%\item natural parameter space, $\Xi$,
%\item natural statistic, $g(Y)$,
%\item observed value of the natural statistic, $g(\yobs)$, such that $g(\yobs) \in C$, the convex 
%support,
%\end{itemize}
the following are equivalent:
\begin{enumerate}
\item $\delta$ is a direction of recession of the log likelihood.
\item For all $\eta \in \Xi$, the function $s \mapsto \ell( \eta + s\delta)$ is 
nondecreasing on $\RR$ \cite[Theorem 3 (b)]{Geyer:gdor}.
\item $\inner{g(Y) - g(\yobs),\delta} \leq 0$ almost surely for all distributions in 
the family. \cite[Theorem 3 (d)]{Geyer:gdor}.
\item $\delta \in N_C(g(\yobs))$ \cite[Theorem 3 (e)]{Geyer:gdor}.
\item $\inner{w,\delta} \leq 0$ for all $w \in T_C(g(\yobs))$ \cite[Theorem 3 (f)]
{Geyer:gdor}.
\end{enumerate}
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%% 1/26/11 - Added from Algorithm.tex
\subsection{Theorem 3(a) and (c), \citet[p. 270]{Geyer:gdor}}
So what do we this theorem for?  It relates when $\delta$ is a direction of recession 
or not.  Theorem 3 in GDOR has says the following are equivalent:
\begin{itemize}
\item (a) There exists a $\theta \in \Theta$ such that the function $s \mapsto \ell
(\theta+s\delta)$ is nondecreasing on $\RR$.
\item (c) $\inner{Y-y, \delta} \leq 0$ almost surely for all distributions in the 
family.
\end{itemize}

The $\delta$ in (a) in the above is a direction of recession.  

The proof requires the following lemma, which a PROOF WILL BE NEEDED.
$\ell(\theta+s\delta)$ is non-dereasing if and only if $\lim_{s \to \infty} \ell
(\theta+s\delta) > -\infty$.

\textbf{From $(c) \to (a)$}:

Take $\E_{\theta + s \delta}()$ of $\inner{Y-y, \delta} \leq 0$.  Then
\begin{align*}
\inner{ \E_{\theta + s \delta} Y-y, \delta} \leq 0.
\end{align*}
Now, take the derivative of $\ell( \theta + s\delta)$ with respect to $s$.
\begin{align*}
\deriv{\ell( \theta + s \delta}{s}) &= \deriv{}{s} \left (   \inner{y, \theta+s
\delta} - c(\theta+s\delta)  \right )\\
	&= \inner{y, \delta} - \deriv{}{s} c(\theta+s\delta) \\
	&= \inner{y, \delta} - \inner{ \E_{\theta+s\delta}Y,\delta }\\
	&= - \inner{ \E_{\theta+s\delta}Y - y,\delta }.
\end{align*}
Applying the above from the assumption, we get that 
\begin{align*}
\deriv{\ell( \theta + s \delta)}{s} \geq 0.
\end{align*}
Thus $\ell(\theta+s\delta)$ is a non-decreasing function of $s$.

\textbf{From $(a) \to (c)$}:
\begin{align*}
	\ell( \theta+s\delta) &= \inner{y, \theta+s\delta} - c(\theta+s\delta) \\
	&= \inner{y, \theta} + s \inner{y,\delta} -bs +bs - c(\theta+s\delta) \\
	&= \inner{y, \theta} + s [\inner{y,\delta} -b]  - \log e^{c(\theta+s\delta) -bs}.
\end{align*}
We know from the above lemma that $\ell(\theta+s\delta)$ is non-dereasing if and only 
if $\lim_{s \to \infty} \ell(\theta+s\delta) > -\infty$.  This implies that in the 
above expression, for the right-most term, $b \geq \sigma_c(\delta)$, and also that
\begin{align*}
	\inner{y, \delta} - b \geq 0,
\end{align*}
or 
\begin{align*}
	b - \inner{y, \delta}  \leq 0.
\end{align*}
If the above is true, then 
\begin{align*}
	\sigma_C(\delta)  - \inner{y, \delta} \leq b - \inner{y, \delta}  \leq 0,
%#	\inner{y, \delta} - \inner{Y_{max},\delta } \geq 0.
\end{align*}
and recalling that $\sigma_C(\delta) = \sup_{y \in C} \inner{y, \delta}$, then
\begin{align*}
	\inner{Y - y,\delta } \leq 0.
\end{align*}
(with probability one---do I even need to add this).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

From the two theorems above, it is clear that every direction of constancy is a 
direction of recession.  These induce the following criteria about the existence of 
the MLE in the conventional sense:

\begin{corollary}[Theorem 4, Corollary 5 \citep{Geyer:gdor}] \label{Cor:MLE DNE}
For a full exponential family with the setting as Theorem~\ref{Thm:DOC}, if $\delta$ 
is a 
direction of recession that is not a direction of constancy, then  
\begin{enumerate}
\item the MLE does not exist.
\item $N_C(g(\yobs))$ is a vector subspace.
\item $T_C(g(\yobs))$ is a vector subspace.
\item for all $\eta \in \Xi$, the function $s \mapsto \ell(\eta + s \delta)$ is 
strictly increasing on 
the interval where it is finite.
\end{enumerate}
\end{corollary}

The \emph{relative interior} of a convex set $C$, denoted $\rint C$, is the interior 
relative to its affine hull.  We call $\delta$ a \emph{generic direction of recession} 
(GDOR) if $\delta \in \rint N_C(g(\yobs))$.  By construction it is not a direction of 
constancy.  By Corollary~\ref{Cor:MLE DNE}, a GDOR exists if and only if the MLE does 
not exist.

The well-known condition for the existence of the MLE \citep{Barndorff, Brown:1986} is 
formally stated as follows:
\begin{theorem} \label{Thm:MLE rint}
Under the conditions [CONDITIONS], the MLE exists in the conventional sense and is 
unique if and only if 
$g(\yobs) \in \rint(C)$.
\end{theorem}

When the MLE does not exist for an exponential family in the conventional sense, there 
exists a GDOR along which the log likelihood goes to $+\infty$.  The behavior of the 
density of the distribution is described in the following theorem:

\begin{theorem}[Theorem 6 from \citet{Geyer:gdor}] \label{Thm:LCM}
For a full exponential family with the setting as Theorem~\ref{Thm:DOC}, and 
additionally,
\begin{enumerate}
\item density function \eqref{E:pdf},
\item direction of recession, $\delta$,
\item $H = \{ w \in \RR^d: \inner{ w-g(\yobs), \delta } = 0 \}$,
\item $P( g(Y) \in H) > 0$ for some distribution in the family,
\end{enumerate}
then for all $\eta \in \Xi$
\begin{align} \label{E:LCM}
\lim_{s \to \infty} f_{\eta+s\delta}(y) = 
			\begin{cases} 
			0 								& \inner{g(Y) - g(\yobs), \delta} < 0 \\
			\frac{f_\eta(y)}{P_\eta(g(Y) \in H)} 	& \inner{g(Y) - g(\yobs),
\delta} = 0 \\
			+\infty							& \inner{g(Y) - g(\yobs), \delta} > 0.
		\end{cases}
\end{align}
If $\delta$ is not a direction of constancy, then $s \mapsto P_{\eta+s\delta}( g(Y) 
\in H)$ is continuous and strictly increasing, and 
$P_{\eta+s\delta}( g(Y) \in H) \to 
1$ as $s \to \infty$.
\end{theorem}


The right-hand side of \eqref{E:LCM} can be viewed as a conditional density of a 
distribution with parameter $\eta$ given $g(Y) \in H$ and hence expressed as $f_{\eta}
(\, \cdot\,  | g(Y) \in H)$ (the set that maps to $+\infty$ has probability zero by 
Theorem~\ref{Thm:DOR}).  Because it still has the same functional form as the 
exponential family density, $f_\eta(y)$, the family of distributions
\begin{align} \label{E:f conditional}
\{ f_{\eta}(\, \cdot\,  | g(Y) \in H) \text{ for } \eta \in \Xi \}
\end{align}
is itself an exponential family with the same natural parameters and statistics as the 
original family.  
%Then we can succinctly summarize the result of Theorem~\ref{Thm:LCM} as
%\begin{align*}
%\lim_{s \to \infty} f_{\eta+s\delta}(y) = f_{\eta}( y | g(Y) \in H).
%\end{align*}
It may not be full, however; the full family containing this exponential family,
\begin{align*}
\{ f_{\eta}(\, \cdot\,  | g(Y) \in H) \text{ for }  \eta + \gamma: \eta \in \Xi \text
{ and } \gamma \in \Gammalim \},
\end{align*}
where $\Gammalim$ is the constancy space of \eqref{E:f conditional},
is called the \emph{limiting conditional model} (LCM) \citep{Geyer:gdor}.  
An MLE for this new model must exist, since $g(\yobs)$ cannot lie on the boundary of 
the convex support which was itself determined by $g(\yobs)$ lying in the interior of 
it.  


\subsection{Theorem 6, \citet[p. 271]{Geyer:gdor}, 7/25/10}
And now the big one:
\begin{theorem}
If $\delta$ is a direction of recession, and
\begin{align*}
H = \set{ w \in \RR^p: \inner{w-y,\delta}=0 },
\end{align*}
and $P(Y \in H) > 0$ for some distribution in the family, and hence for all, then for 
all $\theta \in \Theta$,
\begin{align*}
\lim_{s \to \infty} f_{\theta+s\delta}(\omega) = 
			\begin{cases} 
			0 								& \inner{Y(\omega) - y ,\delta} < 0 \\
			\frac{f_\theta(\omega)}{P_\theta(Y \in H)} 	& \inner{Y(\omega) - y ,
\delta} = 0 \\
			+\infty							& \inner{Y(\omega) - y ,\delta} > 0.
		\end{cases}
\end{align*}
If $\delta$ is not a direction of constancy, then $s \mapsto P_{\theta+s\delta}( Y 
\in H)$ is continuous and strictly increasing, and $P_{\theta+s\delta}( Y \in H) \to 
1$ as $s \to \infty$.
\end{theorem}
Before going on to the proof, let's think about what this means.  Suppose $\delta$ is 
a DOR and not a DOC, because the latter is not interesting.  So, the MLE does not 
exist because the log-likelihood, despite being strictly convex, is strictly 
increasing when we go in the direction of $\delta$.  That's what DOR means.  So then 
what?

Well, we need to think first about $H$ again, this plane that is perpendicular to $
\delta$ on which the observed data $y$ lies.  Looking first at the last sentence of 
this theorem, it says that for the sequence of exponential families with parameter 
value $\theta+s\delta$, the probability of $Y$ occurring on the plane $H$ is strictly 
increasing, and in fact, going to 1.  This is what Charlie is referring to when he 
says things like ``the probability is accumulating on the boundary".  So, $H$ 
actually coincides with the boundary of the convex hull, the observed data $y$ sits 
is on this boundary (hence the MLE does not exist, by the usual MLE existence 
theorem, and hence a DOR exists).  When we take the parameter value further and 
further along in the direction of the DOR, the samples generated from that model will 
be increasingly cluster around that boundary face of the convex hull on which $y$ 
sits.

So how about the rest of the theorem?  We saw in Theorem 3 that $\inner{Y-y,\delta} 
\leq 0$ so the set where the above is $+\infty$ has probability zero.  I think the 
set for which $\inner{Y-y,\delta} = 0$ corresponds to a DOC, doesn't it?  Hmm, 
something's not right with my reasoning.

Charlie's paper says: the right side (excluding the 0 probability part) is the 
density of the conditional distribution given the event $Y \in H$ of the distribution 
having parameter value $\theta$, so we could denote the RHS by $f_\theta(\omega | Y 
\in H)$.  The exponential family with the \emph{full} natural parameter space (I'm 
glossing over some stuff) is the limiting condition model (LCM).  ``If an MLE exists 
for the LCM, then it maximizes the likelihood in the family that is the union of the 
LCM and the original family, and it maximizes the likelihood in the family that is 
the set of all limits of sequences of distributions in the original family.  When 
this happens, we say we have found an MLE in the Barndorff-Nielsen completion of the 
original family."

\begin{proof}
Hmm, I'm concerned about the different forms of $H$: are they actually all the same 
thing?  Well, carry on.

By Theorem 3, if $\delta$ is a DOR, then $\inner{Y-y, \delta} \leq 0$ which implies 
that $\inner{Y, \delta} \leq \inner{y, \delta}$.  So, the largest value that $\inner
{Y, \delta}$ can take on is $\inner{y, \delta}$.  Then $\sigma_C(\delta) = \inner{y, 
\delta}$.

Focus on the form of the density for $f_{\theta+s\delta}(\omega)$.  According the 
form of the relative density previously defined with respect to $\psi$,
\begin{align*}
 f_{\theta+s\delta}(\omega) &= e^{ \inner{y,\theta+s\delta - \psi} - c(\theta+s
\delta) + c(\psi)  } \\
 	&= e^{ \inner{y,\theta - \psi} -c(\theta) + c(\theta) + s \inner{y,\delta} - c
(\theta+s\delta) + c(\psi)  } \\
 	&= e^{ \inner{y,\theta - \psi} -c(\theta) + c(\psi) - c(\theta+s\delta) + s 
\inner{y,\delta} + c(\theta)   } \\
 	&= f_\theta(\omega) \frac{e^{c(\theta)}}{e^{ c(\theta+s\delta) - \inner{y,\delta}
s } }.
\end{align*}
This form is starting to look familiar, in particular, the denominator is from 
Theorem 2.2 from \citet{Geyer:1990} except with $\inner{y,\delta}$ instead of $b$ 
(where this 
$y$ is the dummy variable in the density, not the observed data).  So what happens as 
$s \to \infty$?  

It depends on how $\inner{Y(\omega),\delta}$ relates to $\sup_{y \in C}\inner{y,
\delta} = \inner{y,\delta}$, for the observed data $y$.  (our notation is getting a 
little confusing since $y$ can denote the observed data $y$ or the dummy variable in 
the density).  
\begin{align*}
	f_{\theta+s\delta}(\omega) = f_\theta(\omega) \frac{e^{c(\theta)}}{e^{ c(\theta+s
\delta) - \inner{y,\delta}s } } 
	\to	
			\begin{cases} 
			0 								& \inner{Y(\omega),\delta} < \inner{y,
\delta} \\
			\frac{f_\theta(\omega)}{P_\theta(Y \in H)} 	& \inner{Y(\omega) ,
\delta} = \inner{y,\delta} \\
			+\infty							& \inner{Y(\omega),\delta} > \inner{y,
\delta}.
	\end{cases}
\end{align*}
For $\delta$ a DOR but not DOC, we need to show that the $P_{\theta+s\delta}(Y \in H) 
\to 1$ 
as $s \to \infty$, where $H = \set{w \in \RR^p: \inner{w-\yobs,d}=0}$.
\begin{align*}
 P_{\theta+s\delta}(Y \in H) &= \int_H e^{\inner{y, \theta+s\delta - \psi} - c(\theta
+s\delta) + c(\psi)} \, d\psi \\
		&= \E_\psi \left \{ I_H e^{\inner{y, \theta - \psi} +c(\psi) - [c(\theta+s
\delta) -s\inner{y,\delta}]} \right 
\}\\
		&= \E_\psi \left \{ I_H  \frac{e^{c(\theta)} }{ e^{ c(\theta+s\delta) -s\inner
{y,\delta} } } 
		e^{\inner{y, \theta - \psi} -c(\theta)+c(\psi)}  \right \}.
\end{align*}
Since the indicator function, $I_H$, fixes $\inner{ y, \delta}$ to be a constant, it 
can pulled out of the expectation, so that 
\begin{align*}
		 P_{\theta+s\delta}(Y \in H)
		 &= \frac{e^{c(\theta)} }{ e^{ c(\theta+s\delta) -s\inner{y,\delta} } }
		 \E_\psi \left \{ I_H   
		e^{\inner{y, \theta - \psi} -c(\theta)+c(\psi)}  \right \}, \\
		 &= \frac{e^{c(\theta)} }{ e^{ c(\theta+s\delta) -s\inner{y,\delta} } } 
		 P_\theta ( Y \in H ).
		 \end{align*}
 
Then applying Theorem 2.2 from Geyer:1990,
\begin{align*}
 P_{\theta+s\delta}(Y \in H)
		&\to \frac{1}{P_\theta(Y \in H)}  P_{\theta}(Y \in H) = 1
 \end{align*}
 as $s \to \infty$.  
 
 That this probability is strictly increasing follows directly from the DOR property 
which requires $\ell( \theta+s
\delta)$ to be strictly increasing.   
\end{proof}


\subsection{log likelihood is bounded by LCM}
\subsection{summary}



In summary, by Theorem~\ref{Thm:MLE rint}, when the observed statistic, $g(\yobs)$, is 
on the boundary of the convex support $C$, the MLE does not exist in the conventional 
sense.
Then by Corollary~\ref{Cor:MLE DNE}, there must exist a direction of recession, $
\delta$, that is not a direction of constancy.  
According to Theorem~\ref{Thm:LCM}, as we move in the natural parameter space in the 
direction of $\delta$, the distribution with this parameter value will put 
increasingly probability on points for which the natural statistics, $g(Y)$, land in 
the hyperplane $H$ on which $g(\yobs)$ lies.  This hyperplane is orthogonal to $\delta
$ by construction and contains the boundary of the convex support.  Samples generated 
from these models will therefore increasingly fall on the face on which $g(\yobs)$ 
lies as $\eta$ increases in the direction of $\delta$.  Theorem~\ref{Thm:LCM} says 
that eventually, all generated sample points will fall on this boundary.


\subsection{Mean value parameterization}

An alternative parameterization of an exponential family is the mean value 
parameterization.  For each natural parameter $\eta$, we can define the mean parameter 
$\mu$ such that
\begin{align*}
	\mu = \E_\eta g(Y).
\end{align*}
The space for the mean value parameterization is the same as the convex support of the 
natural statistics and thus lends itself to easier interpretability  \citep
{Handcock:degeneracy, Rinaldo:2009}.  In particular, $\mu = g(\yobs)$ is the MLE in 
the mean value parameterization.  \citeauthor{Handcock:degeneracy} observed that mean 
value parameters located too close to the boundary of the convex support correspond to 
degenerate distributions.  

There exists a one-to-one mapping between a natural parameter and its mean value 
parameter, and one can calculate mean value parameters from natural parameters. In 
general there is no simply way to get the natural parameter value from the mean value 
parameter (if there was, then finding MLEs would be very easy and we wouldn't need all 
these algorithms!).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linearity}\label{S:linearity}
\citet{Geyer:gdor} defines the \emph{linearity} of a set of points $V$ to be
\begin{align*}
	L = \{ w \in V: -w \in \con( \pos V) \}.
\end{align*}
It is the subspace with bases determined by $V$.

We use this to find the face of the polyhedral convex support on which the observed 
statistic lies in the interior of.  The function \texttt{linearity} in the \texttt
{rcdd} package can take as input the rays directed from observed statistic to sample 
points generated via MCMC from the distribution with parameter value of interest.  
From these, it determines the rays that form the basis of a vector subspace.  Of 
course we are not interested in the vector subspace; we need only take the points 
corresponding to these bases as our empirical face $F$.  By construction, the observed 
statistic lies in the interior of $F$.

%Let $\widetilde{T}_C(x)$ be the empirical tangent cone at the point $x$ constructed 
%from a set of points $W$ all in $C$.  That is,
%\begin{align*}
%	\widetilde{T}_C(x) = \{ w - x: w \in W\}.
%\end{align*}
%Then for $V = \widetilde{T}_C(g(\yobs))$, the linearity corresponds to the set of 
%points from $W$ (shifted by $g(\yobs)$) that form what we refer to as the empirical 
%face $F$.  
%In our algorithm, we use the MCMC sample points for $W$.  \citet{Geyer:gdor} provides 
%the function \texttt{linearity} in the R package \texttt{rcdd} to do this computation.

%Consider the case when $g(\yobs) \in \rintr C$.  Then the MLE exists, and the 
%linearity will correspond to all the points $W \in C$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{One-sided confidence interval}

Consider the probability distributions defined with log likelihood $\ell( \eta + s 
\delta)$ as defined 
by \eqref{E:loglike}, where $s$ is a real scalar and $\delta$ is a direction of 
recession.  Then 
as $s$ goes from $-\infty$ to $+\infty$, the probability of observing $g(Y) \in H$ 
goes from zero to one.  This 
function is strictly increasing by Theorem~\ref{Thm:LCM}.  Thus we can find the unique 
$s$, call it $\hat{s}$, that makes 
this probability 0.05.  Then $[\hat{s}, +\infty)$ is a 95\% confidence interval for 
the scalar parameter 
$s$, and, in turn, $[\hat{\eta} + \hat{s}\delta, +\infty)$ gives a 95\% confidence 
interval for $\etaMLE
$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Subsampling}
%Once our algorithm has determined that the observed data is on the boundary and thus 
%the MLE does not exist in the conven
Our algorithm depends on being able to to subsample from the MC sample points when 
optimizing in the LCM, restricting to the sample points that comprise the support of 
this model.  The subsampling operation itself is straightforward since we determined 
empirically the face that forms this support.  

However, it is not obvious that when we draw $g(Y_1), \ldots, g(Y_m)$ from the 
original model with parameter value $\eta$ that the subsample restricted to the 
support is in fact a sample from the LCM with parameter value $\eta$.  This is a 
convenient consequence of Theorem~\ref{Thm:LCM}: by this stage in the algorithm, the 
majority of sample points (greater than 60\%) are on the face.  Then $s$ in the 
expression $P_{\eta + s \delta}(g(Y) \in H)$ is a large value.  But since the LCM 
density is the limit of the density of the original model as $s$ goes to $+\infty$, 
the distribution of the LCM is accordingly well approximated.  (\textbf{BUT WELL 
ENOUGH?})



\chapter{Algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Long range search algorithm for MLEs} \label{Section:Algorithm}
We now present our search algorithm, which will converge to the optimum for any 
continuously differentiable, strictly 
convex (or concave) function.  The algorithm and requirements are presented in 
Theorem~\ref{Thm:Line Search}.  Proofs 
are in Appendix~\ref{Section:Proofs}. 


We apply the algorithm in Theorem~\ref{Thm:Line Search works} to the specific setting 
of finding the MLE in a regular 
exponential family when the MLE is known to exist and be unique and the gradient can 
be calculated exactly.

%\section{Line search convergence}
In order to be consistent with the general optimization literature \citep
{Fletcher,NW}, we state our algorithm in this 
section from the perspective of a minimization problem.  Thus we wish to minimize a 
real-valued objective function $f$ 
defined on $\RR^n$.  


%%%%%%%%%%% BEGIN THEOREM %%%%%%%%%%%%%%
\begin{theorem}[Convex function root search] \label{Thm:Line Search}
Consider any line search of the form 
\begin{align}
	x_{k+1} &= x_k + \alpha_k p_k \label{E:x_update}
\end{align}
used to minimize the objective function $f$, which satisfies the following 
assumptions:
\begin{enumerate}
	\item The objective function $f$ is bounded below in $\RR^n$. \label{ass:one}
	\item The objective function $f$ is proper, lower semicontinuous, and strictly 
convex.
	\item The objective function $f$ is differentiable in an open set $\NN$ containing 
the level set $\lev_{\leq f
(x_0)} f$, which is bounded, where $x_0$ is the starting point of the iteration.
%\item The \emph{step length} $\alpha_k$ is greater than $0$ unless $\nabla f(x_k) = 0$, 
% in which case $x_k$ is already the solution and the search is complete.
\item The \emph{search direction} $p_k$ is a non-zero \emph{descent direction} \label
{ass:four}
such that the angle $\theta_k$ between the search direction $p_k$ and steepest descent 
direction $-\nabla f(x_k)$ is 
restricted to be less than 90 degrees by
\begin{align*}
\cos \theta_k \geq \delta > 0
\end{align*}
 for some fixed $\delta > 0$.  
\end{enumerate}

Then, unless $\nabla f(x_k) = 0$, in which case $x_k$ is already the solution and the 
search is complete, it is 
possible to find a step length $\alpha_k$ that satisfies the \emph{curvature 
condition}
\begin{align}
	c \nabla f(x_k)^T p_k &\leq \nabla f( x_k + \alpha_k p_k)^T p_k \leq 0 \label
{E:Wolfe-mod}
\end{align}
for some fixed $0 < c < 1$.

Furthermore, repeated iterations of \eqref{E:x_update} satisfying assumptions~\ref
{ass:one} through~\ref{ass:four} and \eqref{E:Wolfe-mod} will produce 
a sequence, $x_1, x_2, \ldots$ such that
\begin{align*}
	\lim_{k \to \infty} || \nabla f(x_k) || = 0.
\end{align*}
\end{theorem}

%\section{Line search for MLE estimation}
\begin{theorem}[Exponential family log likelihood maximization] \label{Thm:log like 
max}
Consider any line search of the form 
\begin{align}
	\eta_{k+1} &= \eta_k + \alpha_k p_k \label{E:eta_update}
\end{align}
used to minimize the negative log likelihood function $-\ell(\cdot)$ of a regular 
exponential family on a finite sample space, where the \emph{search direction} $p_k$ 
is a non-zero \emph{descent direction}
such that the angle $\theta_k$ between the search direction $p_k$ and steepest descent 
direction $-\nabla \ell(\eta_k)$ is 
restricted to be less than 90 degrees by
\begin{align*}
\cos \theta_k \geq \delta > 0
\end{align*}
 for some fixed $\delta > 0$.  

Then, unless $\nabla \ell(\eta_k) = 0$, in which case $\eta_k$ is already the solution 
and the search is complete, it is 
possible to find a step length $\alpha_k$ that satisfies the \emph{curvature 
condition}
\begin{align}
	0 \leq \nabla \ell( \eta_k + \alpha_k p_k)^T p_k  \leq c \nabla \ell(\eta_k)^T p_k  
\label{E:Wolfe-ll}
\end{align}
for some fixed $0 < c < 1$.

Furthermore, repeated iterations of \eqref{E:eta_update} along a descent direction 
satisfying \eqref{E:Wolfe-ll} will produce a sequence, $\eta_1, \eta_2, \ldots$ such 
that
\begin{align*}
	\lim_{k \to \infty} || \nabla \ell(\eta_k) || = 0.
\end{align*}
\end{theorem}



We apply Theorem~\ref{Thm:Line Search} to the setting of exponential families to find 
the MLE when it exists.  

\begin{theorem}[] \label{Thm:Line Search works}
For a regular exponential family with minimal representation where the MLE exists, the 
line search described in 
Theorem~\ref{Thm:Line Search} can be applied to the negative log likelihood function 
$-\ell(\eta)$ so that a search 
starting at any $\eta_0 \in \Xi$ will converge to the MLE of $\eta$.
\end{theorem}

The issue of MLE existence is a problem in computational geometry, not an optimization 
problem, so we do not address it 
here.  See \citep{Geyer:gdor,Rinaldo:2009} and references cited therein.


\chapter{Algorithm pseudocode}
The algorithm is summarized on the next page.  Below are a few comments about notation 
and equations:
\begin{itemize}
\item $C$ is the convex support.
\item $Y$ is the random variable in the sample space.
\item $g(\cdot)$ is the function that maps $y$ to its natural statistics.
\item $\yobs$ is the observed observed, $g(\yobs)$ the natural statistics of the observed data.
\item $\con()$ is the convex hull of a set of points.
\item $\bd()$ is the boundary of a set of points.
\item $\norm{\cdot}$ is the Euclidean norm.
\item The curvature condition \eqref{E:curvature} used here comes from the search 
algorithm of \citet{Okabayashi:longrange} and is necessary for guaranteeing 
convergence to the MLE when it exists.
%\item $\bar{Y}_m$ is the sample mean, $\frac{1}{m}\sum_{i=1}^m g(Y_i)$
\end{itemize}
\newpage
{\small
\noindent \textbf{LCM MLE Algorithm}

\noindent \begin{algorithmic}[1]
\State Get an initial value, $\eta_1 = (0, \ldots 0)$.
\State \textbf{Sample} $Y_1, \ldots, Y_m$ from the distribution with parameter $\eta_{1}$.  
\State Set $ty.hull = \con(g(Y_1), \ldots, g(Y_m) )$.
\State Approximate 
\begin{align} \label{E:nabla ell approx}
\nabla \ell( \eta_1) \approx g(\yobs) - \frac{1}{m}\sum_{i=1}^m g(Y_i).
\end{align}
\State Set $p_1 = \nabla \ell( \eta_1)$, $k=1$, $LCM.flag$ = FALSE, $c=0.2$.\\
%, $LCM.k=1$, 
%\State , $on.boundary$ = FALSE, $on.interior$ = FALSE.
%\State Set $face.cutoff = 0.30$.

\While{$\lVert \nabla \ell( \eta_k) \rVert > \epsilon$}
\State \textbf{Find} a step size $\alpha_k$ that satisfies the \textit{curvature condition}
\begin{align}\label{E:curvature}
	 0 & \leq \nabla \ell( \eta_k + \alpha_k p_k)^T p_k \leq c \nabla \ell(\eta_k)^T p_k.
\end{align}
%\State for some $0 < c < 1$. % (Calculating $\nabla \ell(\cdot)$ requires more sampling)

\State $\eta_{k+1} = \eta_k + \alpha_k p_k$.
\State \textbf{Sample} $Y_1, \ldots, Y_m$ from the distribution with parameter $\eta_{k+1}$.
\If{$LCM.flag=$TRUE}
	\State Restrict sample points to those in empirical face.
\EndIf
\State Call the resulting sample points $g(Y_{(1)}), \ldots, g(Y_{(k)})$.
\State \textbf{Update} $ty.hull$ to reflect new sample points, $g(Y_{(1)}), \ldots, g(Y_{(k)})$.\\
\State \textbf{Question: $g(\yobs) \in \bd( \con(g(Y_{(1)}), \ldots, g(Y_{(k)}) ))$? }
%\If{Yes, $g(\yobs)$ is outside the convex hull}
%	\State  Keep sampling.  
%%	\State $on.boundary$ = FALSE.
%	\State Skip to estimating $\nabla \ell( \eta_{k+1})$.
%\ElsIf{No, $\yobs$ is inside the convex hull}
%	\State \textbf{The MLE exists}.  Finding it should be straightforward, (except 
%when 
%	\State it isn't ...) 
%	\State $on.boundary$ = FALSE, $on.interior$ = TRUE.
%	\State Skip to estimating $\nabla \ell( \eta_{k+1})$.
%\Else 
\If{Yes} %\Comment{$g(\yobs)$ is on the boundary of convex hull}
	\State \textbf{Either:}
	\State (1) the MLE exists but the sample just touches $g(\yobs)$, 
	\State (2) the MLE does \emph{not} exist; both $g(\yobs)$ and our sample points  
	\State are touching the boundary of $C$.
	\State \textbf{Find} the empirical face $F$ of $ty.hull$ on which $g(\yobs)$ lies.
%	\State \textbf{Calculate} $face.prop$, the proportion of the sample that falls on
%	\State this face.\\
%\newpage
	\If{$>60\%$ of the sample points are on $F$}
		\State Conclude that we are in case (2); case (1) is very unlikely.
		\State \textbf{Set} $LCM.flag$ = TRUE.
	\EndIf
\EndIf\\
%\State \textbf{Calculate} $\nabla \ell( \eta_{k+1})$ as follows: \label{Calc:nabla}
%\If{ $LCM.flag$ == FALSE }
%	\State $\nabla \ell( \eta_{k+1}) \approx g(\yobs) - \frac{1}{m}\sum_{i=1}^m g(Y_i)$
%\Else
	\State Approximate
	\begin{align} \label{E:nabla ell approx LCM}
	\nabla \ell( \eta_{k+1}) \approx g(\yobs) - \frac{1}{k}\sum_{i=1}^k g(Y_{(i)}).
	\end{align}

%, where the sample mean is restricted 
%	\State to empirical face points.
%\EndIf\\

\State \textbf{Find} the new search direction $p_{k+1}$, which must be an ascent 
direction.
\Statex This may involve using a direction normal to empirical face, or a regression 
\Statex over previous $\eta_k$ values, or simply using steepest ascent, $\nabla \ell
( \eta_{k+1})$.

\State $k = k + 1$.
\EndWhile
\end{algorithmic}
}
\chapter{Refinements of algorithm}

In Theorem~\ref{Thm:Line Search}, we restricted our search direction $p_k$ to be a 
descent direction, so that $\nabla f
(x_k)^T p_k < 0$ or, alternatively, the angle $\theta_k$ between the search direction 
$p_k$ and steepest descent 
direction $-\nabla f(x_k)$ is less than 90 degrees.  However, this still leaves many 
possibilities for the choice of 
$p_k$ other than steepest descent.  In addition, we have specified restrictions on the 
step size $\alpha_k$ in the 
curvature condition \eqref{E:Wolfe-mod} with $0 < c < 1$, but it would be useful to 
know if certain values of $c$ are 
better than others.

\section{Search directions}
In our examples in Section~\ref{S:Examples}, we default to steepest descent directions 
in our implementation for 
transparency.  Although often effective in early steps, steepest descent directions 
can result in a zigzagging 
trajectory of the sequence $x_k$ \citep{Sun:2006}.  Conjugate gradient methods address 
this phenomena and cover the 
sample space more efficiently \citep{NW}.  It is easy to implement a variant of the 
Polak-Ribi\`{e}re method
\citep[pp.~120--122]{NW} here, requiring little more in terms of calculation or 
storage.  The search direction $p_k$ would update 
with an extra intermediate step as follows:
\begin{align*}
	\gamma_{k+1}^{PR} &= \max \left( 0, \frac{ [ \nabla f( x_{k+1}) ]^T( \nabla f( x_{k+1} ) - \nabla f( x_k) )  }
{ \lVert \nabla f( x_k) \rVert^2 } \right )\\
	p_{k+1} &= -\nabla f( x_{k+1}) + \gamma_{k+1}^{PR} \, p_k.
\end{align*}
Note that when $\gamma_{k+1}^{PR} = 0$, $p_{k+1}$ will be just $-\nabla f( x_{k+1})$, 
the direction of steepest 
descent, and thus serves as a ``reset''.  The curvature condition \eqref{E:Wolfe-mod} 
guarantees that this method always 
yields a descent direction for $p_{k+1}$ and thus Theorem~\ref{Thm:Line Search} still 
holds.  


\subsection{Further Refinement on Search Directions}
We require that the search direction, $p_k$, be an ascent direction, but make no 
further restrictions.
If we use only steepest ascent directions, using $p_k = \nabla \ell( \eta_k)$, our 
algorithm 
may not be efficient in scaling the log likelihood due to excessive ``zig-zagging" as 
illustrated in Figure 
\ref{F:zigzag}.  This is especially problematic when the MLE does not exist in the 
conventional sense---the MLE is actually off at infinity and a zig-zagging route may 
take an especially long time to realize this.  \citet{Okabayashi:longrange} made the 
case for using search directions chosen according to Polak-Ribiere conjugate gradient 
updates.  

Here we suggest another alternative that is especially useful when it appears that the 
observed statistic might fall on the boundary of the convex support.  Because our 
algorithm computes the normal cone when it finds an empirical face, we suggest using 
the average normal cone vector as a search direction, checking first that it is an 
ascent direction to ensure the algorithm proceeds uphill.  As theory in the previous 
sections show, a GDOR, if it exists, is any vector in the relative interior of the 
normal cone at the observed statistic.  So, the search direction chosen in this manner 
may in fact be a GDOR of the original model and hence result in large steps when 
meeting the curvature condition.  Alternatively, we also consider using search 
directions resulting from a regression through the previous few parameter values to 
break the zig-zagging pattern.

\begin{figure}[!ht]
\centering
\includegraphics[height=2.6in,width=2.6in]{Figures/zigzag-eta}
\includegraphics[height=2.6in,width=2.6in]{Figures/zagplusnorm-eta}
\caption{Contour plots of the log likelihood when the MLE does not exist.  The surface 
of the log 
likelihood tends to flatten, though technically it is still concave.  This can cause 
the steepest ascent 
algorithm to zigzag, which is inefficient (top).  However, by periodically using 
search directions 
determined by normal vectors  derived from the empirical face or regression through 
previous points, the algorithm can make much larger steps (bottom).}
\label{F:zigzag}
\end{figure}


\section{Step size}
We now turn our attention to the optimal step size $\alpha_k$ when our objective 
function is the log likelihood of an 
exponential family.  Taking the derivative of $\ell( \eta_k + \alpha_k p_k)$ with 
respect to $\alpha_k$ shows that the 
log likelihood is maximized as a function of $\alpha_k$ along the direction $p_k$  
when 
\begin{align*}
	\nabla \ell( \eta_{k+1} )^T p_k = 0.
\end{align*}

By choosing $c$ to be small, say 0.2, we ensure that the step taken is close to 
maximizing the log likelihood along the 
search direction.  This is also apparent in Figure~\ref{F:alpha_region}. 

Making $c$ too small, however, may make it difficult to find an $\alpha_k$ that meets 
the curvature condition \eqref
{E:curvature mod} since this search must be done numerically.  In fact, as the line 
search nears the MLE and $\nabla \ell( \eta_k)$ gets smaller, the rightmost term in \eqref{E:curvature mod} gets 
smaller in magnitude (it equals $c \lVert \nabla \ell(\eta_k) \rVert^2$ if using steepest ascent directions), making a 
numerical search for $\alpha_k$ 
more challenging.  

%Finally, while the choice of 0.2 for $c$ worked well in the problems we explored 
%regardless of search 
%directions used, it follows from our discussion in the previous section that it may 
%make sense to use slightly larger 
%values of $c$ when using steepest ascent directions, thereby reducing the zigzagging 
%phenomenon, but smaller values for 
%$c$ when using conjugate gradient methods.


\section{MCMC approximations} \label{section:MCMC approx}
Our algorithm requires us to be able to calculate $\nabla \ell(\eta)$ using \eqref
{E:nabla ell}.  For many 
applications, we will need to approximate $\E_{\eta}g(Y)$ using MCMC.  That is,
\begin{align}
 	\nabla \ell (\eta) = g(y) - \E_\eta g(Y) \approx g(y) - \frac{1}{m}\sum_{i=1}^m g
(Y_i), \label{E:nabla ell approx}
\end{align}
where $Y_1, \ldots, Y_m$ are MCMC draws from the distribution with parameter $\eta$.  
There are many MCMC algorithms 
such as Metropolis-Hastings or Swensen-Wang (used for the Ising model example in 
Section~\ref{S:Examples:Ising}); see \citep{Brooks} and references cited therein.
We show examples in the next section where $\nabla \ell(\eta)$ can be calculated 
exactly and where it must be 
approximated.

The accuracy of the approximation in \eqref{E:nabla ell approx} increases with Monte 
Carlo sample size $m$. 
When the current estimate is far away from the MLE, we can use smaller $m$ to save 
time and work with a 
fairly noisy approximation of the gradient.  However, when the current estimate 
approaches the MLE, larger $m$ are necessary.

Our algorithm relies on the computed values of $\nabla \ell(\eta)$ in the curvature 
condition \eqref{E:curvature mod}, 
as well as the stop condition for the algorithm, $\lVert \nabla \ell( \eta_k ) \rVert 
< \epsilon$.  Given that we may 
only have approximations of $\nabla \ell(\eta)$, we cannot know for certain if either 
of these conditions are truly 
met.  We can ameliorate this by constructing confidence intervals for each of the 
inequalities.  

For the inequalities in \eqref{E:curvature mod}, we can estimate asymptotic standard 
errors of $\nabla \ell( \eta_k + 
\alpha_k p_k)^T p_k$  and $c \nabla \ell(\eta_k)^T p_k - \nabla \ell( \eta_k + 
\alpha_k p_k)^T p_k$ by appealing to the 
Markov chain Central limit theorem \citep{Chan:1994,Jones:2004,Roberts:1997,Roberts:
2004}.
The \texttt{initseq} function from the R package \texttt{mcmc} \citep{mcmc:R} can be 
used to estimate asymptotic 
standard errors for univariate functionals of reversible Markov chains: given an MCMC 
sample for a univariate 
quantity, \texttt{initseq}
returns a value (divided by sample size) that is an estimate of the asymptotic 
variance in the Markov chain central 
limit theorem.  Both of the quantities in \eqref{E:curvature mod} are univariate.  In 
the second expression, $c \nabla \ell(\eta_k)^T 
p_k - \nabla \ell( \eta_k + \alpha_k p_k)^T p_k$, the MCMC sample generated for $
\nabla \ell( \eta_k + \alpha_k p_k)^T 
p_k$ is independent of the sample generated for $c \nabla \ell(\eta_k)^T p_k$.  Thus 
\texttt{initseq} can be applied 
to each sample separately and the results summed for an estimated variance.  
We can then be approximately 95\% confident (non-simultaneously) that $\alpha_k$ 
satisfies \eqref{E:curvature 
mod} if
\begin{align*}
	 \nabla \ell( \eta_k + \alpha_k p_k)^T p_k - 1.645 \cdot \text{se}_1 > 0 \\
	 c \nabla \ell(\eta_k)^T p_k - \nabla \ell( \eta_k + \alpha_k p_k)^T p_k - 1.645 
\cdot \text{se}_2 > 0 
\end{align*}
where $\text{se}_1$ and $\text{se}_2$ are the asymptotic standard errors for $\nabla 
\ell( \eta_k + \alpha_k p_k)^T p_k
$  and $c \nabla \ell(\eta_k)^T p_k - \nabla \ell( \eta_k + \alpha_k p_k)^T p_k$, 
respectively, calculated as described.

The delta method can be applied to estimate a standard error for $\lVert \nabla \ell
( \eta_k ) \rVert$. 
%%%%%%%%%%%%% 1/25/11 -Newly added from Algorithm.tex
The multivariate version of the delta method states that for a sequence of r.v. $B_n$ 
such that
\begin{align*}
	\sqrt{n} ( B_n - \beta) \stackrel{\DD}{\longrightarrow} N( 0, \Sigma ),
\end{align*}
then for a function $h(B)$, where $h$ such that $\nabla h$ is defined and non-zero,
\begin{align*}
	\sqrt{n} \left ( h(B_n) - h(\beta) \right ) \stackrel{\DD}{\longrightarrow} N 
\left ( 0, \nabla h( \beta)^T \Sigma\nabla h( \beta)  \right ).
\end{align*}

We set $B_n = \nabla \ell_n( \theta)$ and $\beta =\nabla \ell( \theta)$, where we 
know that $B_n \stackrel{a.s.}{\longrightarrow} B$ by SLLN.  
We also do not know $\Sigma$, the variance of $\nabla \ell( \theta)$, but this we will 
approximate with $
\hat{\Sigma}$, the scaled sample variance-covariance matrix of our MCMC batches of 
the canonical statistic (the \texttt{initseq} function 
requires a univariate vector and so cannot be used here).  That is,
\begin{align*}
	\hat{\Sigma} = \frac{1}{nbatch}\frac{1}{m-1}\sum_{i=1}^{m} (g(Y_i) - \overline{g
(Y)})( g(Y_i) - 
\overline{g(Y)})^T.
\end{align*}

%%%%%%%%%%%%%%%
Thus the asymptotic variance is calculated by
\begin{align*}
	V \left( \lVert \nabla \ell( \eta_k ) \rVert \right )= \frac{1}{\lVert \nabla \ell
( \eta_k ) \rVert^2} \nabla \ell
( \eta_k )^T \, \hat{\Sigma} \,  \nabla \ell( \eta_k ),
\end{align*}% added hat to Sigma
%where $\Sigma$ is the variance matrix of $\nabla \ell( \eta_k )$ and can be estimated 
%by the sample variance matrix of 
%the batch mean vectors of $g(Y_1), \ldots, g(Y_n)$ divided by the number of batches 
%(the \texttt{initseq} function 
%requires a univariate vector and so cannot be used here).  
We can be approximately 95\% confident that $\lVert 
\nabla \ell( \eta_k ) \rVert > \epsilon$ if 
\begin{align*}
	\lVert \nabla \ell( \eta_k ) \rVert - 1.645 \sqrt{ V \left( \lVert \nabla \ell
( \eta_k ) \rVert \right )} > 
\epsilon.
\end{align*}
%To be conservative, we suggest using a larger quantile, say 2, 
%in the confidence intervals instead of the 1.645 used above.

In practice, however, use of confidence intervals does not appear necessary with  
Monte Carlo sample sizes that are set 
large enough so that these standard errors are initially small relative to the point 
estimates.  The ratio of point 
estimate to standard error of course decreases as the algorithm progresses and the 
estimate of the parameter nears the 
MLE, reflected in $\nabla \ell( \eta_k )$ nearing 0.  Thus these confidence intervals 
are most useful as a guide for
when to increase the MCMC sample size, or when to switch methods, or when to terminate 
the algorithm.



\section{Combining with other algorithms}
We believe the best use of this algorithm is in combination with other faster methods 
like MCMC-MLE \citep{Geyer:1992}
or Newton-Raphson safeguarded by our line search algorithm.  Our algorithm with 
steepest ascent or conjugate gradient search direction
should be used initially from ``long range'', when one has no good intuition for an 
initial value.
It is well known that when the objective function is quadratic the conjugate gradient 
method with exact arithmetic converges to the solution
in at most $d$ steps, where $d$ is the dimension of the problem \citep{NW}.  As a rule 
of thumb, we think using our 
algorithm for $d$ steps before switching seems reasonable.
 


\chapter{Examples} \label{S:Examples}
%\input{saithesis-examples.tex}

\chapter{Discussion}
We have presented a simple line search algorithm for finding the MLE of a regular 
exponential family when the MLE 
exists.  The algorithm avoids the trial and error experimentation of tuning parameters 
and starting points commonly associated with optimization routines
not invented by optimization specialists.  Our algorithm is modeled after algorithms 
discussed in optimization textbooks \citep{Fletcher,NW,Sun:2006},
all of which are safeguarded to ensure rapid automatic convergence.
%Because it only relies on first order derivatives, this approach avoids problems with 
%near-singular Fisher information 
%matrices that plague methods like Newton-Raphson.  The reliant on a curvature 
%condition for step size makes it less 
%sensitive to poor initial values that are problematic for MCMC-MLE and  SA in 
practice.

Convergence is guaranteed when the gradient can be calculated exactly.  Even when the 
gradient cannot be calculated 
exactly and is only estimable via MCMC, the algorithm is still useful in practice, as 
demonstrated by the Ising model 
example.  We have also described a way to construct and use confidence intervals to 
make convergence highly probable.

The algorithm can be computationally demanding.  When the current iteration approaches 
the solution, the 
curvature condition for step size becomes more difficult to satisfy and the method may 
require several iterations of 
MCMC sampling and perhaps an increase in MCMC sample size.  Eventual increase in MCMC 
sample size is unavoidable,
because the achievable accuracy is inversely proportional to the square root of the 
MCMC sample size, as in all Monte Carlo.
Thus we believe the best use of this algorithm is in combination with other faster 
methods like MCMC-MLE \citep{Geyer:1992}
or Newton-Raphson safeguarded by our line search algorithm.  Our 
algorithm should be used from ``long range'', when one has no good intuition for an 
initial value and is concerned about 
picking one that is far from the MLE.  The switch between types of search direction 
(steepest ascent, conjugate gradient,
or Newton) within our algorithm or the switch to another algorithm (such as MCMC-MLE 
\citep{Geyer:1992})
need not require manual intervention.  When used in combination in this
manner, we do not think the confidence intervals are necessary as the curvature 
condition is quite easily satisfied 
when the current iteration is far from the MLE.

One way to improve performance is to use conjugate gradient search directions rather 
than steepest ascent.  In our 
examples, this reduced the number of iterations by over 25\%.  However, in other 
problems we tried with different 
dimensionality, this performance varied significantly and it appears that no guarantee 
can be made about quantity of 
improvement in performance, though in all cases we examined, it never did worse.  This 
is no surprise, because the
necessity of ``preconditioning'' for good performance of the conjugate gradient 
algorithm is well known (but no
good ``preconditioner'' is available for maximum likelihood in exponential families).

There are several outstanding issues.  Most notably, we have not showed convergence of 
the algorithm when the gradient 
is approximated via MCMC.  This is a more difficult theoretical problem and is the 
motivation for stochastic 
approximation research.  
Further work is necessary to determine if one can adapt our restrictive curvature 
condition \eqref{E:Wolfe-mod} to the 
approach of \citet{Andrieu:2005} or \citet{Liang:2010} in MCMC stochastic 
approximation.  

Another remaining issue is the stopping criteria: what value should be chosen for $
\epsilon$ in the exit condition
$\lVert  \nabla \ell( \eta_k ) \rVert < \epsilon$?  Because the value of $\lVert  
\nabla \ell( \eta_k ) \rVert$ can only 
be approximated via MCMC, one cannot be certain if this condition is actually 
satisfied.  Here again, the switch to 
another methodology may be appropriate, though at least in our Ising model example, 
our use of 10,000 for the MCMC 
sample size and 0.005 for $\epsilon$ were successful in obtaining a reasonable 
parameter estimate. 

 A final remaining issue is estimation of Monte Carlo error of the estimates.  Here 
too we recommend switching to another
algorithm at the end.  The MCMC-MLE procedure gives accurate error estimates \citep
{Geyer:1994}.
For very small steps these are essentially the same as the Monte Carlo error of a 
single unsafeguarded Newton-Raphson step,
so the method in \citep{Geyer:1994} can be used for either.

%A natural extension of this algorithm is to the case where the MLE may not exist.  
%This occurs with positive probability for discrete state space exponential families and 
%is a practical concern when estimating parameters \citep{Rinaldo:2009, Geyer:gdor}.  
%In such instances, the concave log likelihood continues to increase as $\eta \to \infty$ along 
%certain directions of recession.  Our algorithm can still be applied to such a setting to climb the 
%log likelihood until the gradient approaches zero and help identify the directions of recession. 




% References don't have to be double spaced either
\setstretch{1.3}
\bibliographystyle{ims}
%\bibliographystyle{imsart-nameyear}
\bibliography{References}
%\bibliography{/Users/saipuck/Tako/THESIS/References}


% text in appendices may be single spaced, if desired
\setstretch{1.3} % to 1.3 spacing
\appendix
\chapter{Proofs} \label{Section:Proofs}
%\input{saithesis-proof-ls.tex}


\end{document}