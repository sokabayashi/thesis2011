The pseudocode presented in Section~\ref{S:algorithm overview} provides the basic framework for the application of our algorithm.
For cases where the MLE is known to exist, the expression for the gradient $\nabla \ell(\eta)$ need be adapted to the setting of the problem, using an appropriate MCMC sampler as described by \eqref{E:nabla ell approx} if it cannot be calculated exactly.  
An efficient root finding algorithm to find a step size $\alpha_k$ that satisfies the curvature condition \eqref{E:Wolfe-ll} is also highly desirable (and is in fact
an area for further research).  However, the overall algorithm
will still essentially be the same.

More complexity is required for the setting where the MLE may not exist.  We will need 
to make use of the results presented in Chapter~\ref{Chapter:Background} which in turn 
rely heavily linear programming methodologies.  For example, even when coordinates 
of a point $g(\yobs)$ and the vertices that define a convex polytope $C$ are known, it 
is not a trivial matter to know if that point lies in the interior or exactly on 
a boundary of $C$.  Further complicating this process is that the representation of 
numbers in computers is not exact, e.g., \texttt{0.3 / 3 == 1} evaluates
to \texttt{FALSE}.  Fortunately the tools we need have already been made available to us through the \texttt{rcdd} package \citep{rcdd:R} in R.  

In this chapter, we first present an expanded pseudocode to deal with the possible
non-existence of MLEs.  We then point out the specific functionality that is necessary,
and describe the commands by which each of these operations can be performed in R.

\section{Algorithm pseudocode}
Below are a few comments about notation and equations:
\begin{itemize}
\item $C$ is the convex support.
\item $Y$ is the random variable in the sample space.
\item $g(\cdot)$ is the function that maps $y$ to its natural statistics.
\item $\yobs$ is the observed observed, $g(\yobs)$ the natural statistics of the observed data.
\item $\con()$ is the convex hull of a set of points.
\item $\bd()$ is the boundary of a set of points.
\item $\norm{\cdot}$ is the Euclidean norm.
\item The curvature condition \eqref{E:Wolfe-ll} used here comes from the search 
algorithm of \citet{Okabayashi:longrange} and is necessary for guaranteeing 
convergence to the MLE when it exists.
%\item $\bar{Y}_m$ is the sample mean, $\frac{1}{m}\sum_{i=1}^m g(Y_i)$
\end{itemize}
%\newpage
{\small
\noindent \textbf{LCM MLE Algorithm}

\noindent \begin{algorithmic}[1]
\State Get an initial value, $\eta_1 = (0, \ldots 0)$.
\State \textbf{Sample} $Y_1, \ldots, Y_m$ from the distribution with parameter $\eta_{1}$.  
\State Set $ty.hull = \con(g(Y_1), \ldots, g(Y_m) )$.
\State Approximate 
\begin{align*}
\nabla \ell( \eta_1) \approx g(\yobs) - \frac{1}{m}\sum_{i=1}^m g(Y_i).
\end{align*}
\State Set $p_1 = \nabla \ell( \eta_1)$, $k=1$, $LCM.flag$ = FALSE, $c=0.2$.\\
%, $LCM.k=1$, 
%\State , $on.boundary$ = FALSE, $on.interior$ = FALSE.
%\State Set $face.cutoff = 0.30$.

\While{$\lVert \nabla \ell( \eta_k) \rVert > \epsilon$}
\State \textbf{Find} a step size $\alpha_k$ that satisfies the \textit{curvature condition}
\begin{align*}%\label{E:curvature}
	 0 & \leq \nabla \ell( \eta_k + \alpha_k p_k)^T p_k \leq c \nabla \ell(\eta_k)^T p_k.
\end{align*}
%\State for some $0 < c < 1$. % (Calculating $\nabla \ell(\cdot)$ requires more sampling)

\State $\eta_{k+1} = \eta_k + \alpha_k p_k$.
\State \textbf{Sample} $Y_1, \ldots, Y_m$ from the distribution with parameter $\eta_{k+1}$.
\If{$LCM.flag=$TRUE}
	\State Restrict sample points to those in empirical face.
\EndIf
\State Call the resulting sample points $g(Y_{(1)}), \ldots, g(Y_{(k)})$.
\State \textbf{Update} $ty.hull$ to reflect new sample points, $g(Y_{(1)}), \ldots, g(Y_{(k)})$.\\
\State \textbf{Question: $g(\yobs) \in \bd( \con(g(Y_{(1)}), \ldots, g(Y_{(k)}) ))$? }
%\If{Yes, $g(\yobs)$ is outside the convex hull}
%	\State  Keep sampling.  
%%	\State $on.boundary$ = FALSE.
%	\State Skip to estimating $\nabla \ell( \eta_{k+1})$.
%\ElsIf{No, $\yobs$ is inside the convex hull}
%	\State \textbf{The MLE exists}.  Finding it should be straightforward, (except 
%when 
%	\State it isn't ...) 
%	\State $on.boundary$ = FALSE, $on.interior$ = TRUE.
%	\State Skip to estimating $\nabla \ell( \eta_{k+1})$.
%\Else 
\If{Yes} %\Comment{$g(\yobs)$ is on the boundary of convex hull}
	\State \textbf{Either:}
	\State (1) the MLE exists but the sample just touches $g(\yobs)$, 
	\State (2) the MLE does \emph{not} exist; both $g(\yobs)$ and our sample points  
	\State are touching the boundary of $C$.
	\State \textbf{Find} the empirical face $F$ of $ty.hull$ on which $g(\yobs)$ lies.
%	\State \textbf{Calculate} $face.prop$, the proportion of the sample that falls on
%	\State this face.\\
%\newpage
	\If{$>60\%$ of the sample points are on $F$}
		\State Conclude that we are in case (2); case (1) is very unlikely.
		\State \textbf{Set} $LCM.flag$ = TRUE.
	\EndIf
\EndIf\\
%\State \textbf{Calculate} $\nabla \ell( \eta_{k+1})$ as follows: \label{Calc:nabla}
%\If{ $LCM.flag$ == FALSE }
%	\State $\nabla \ell( \eta_{k+1}) \approx g(\yobs) - \frac{1}{m}\sum_{i=1}^m g(Y_i)$
%\Else
	\State Approximate
	\begin{align} \label{E:nabla ell approx LCM}
	\nabla \ell( \eta_{k+1}) \approx g(\yobs) - \frac{1}{k}\sum_{i=1}^k g(Y_{(i)}).
	\end{align}

%, where the sample mean is restricted 
%	\State to empirical face points.
%\EndIf\\

\State \textbf{Find} the new search direction $p_{k+1}$, which must be an ascent 
direction.
\Statex This may involve using a direction normal to empirical face, or a regression 
\Statex over previous $\eta_k$ values, or simply using steepest ascent, $\nabla \ell
( \eta_{k+1})$.

\State $k = k + 1$.
\EndWhile
\end{algorithmic}
}


In these next sections, we detail how to apply these tools to the various problems we encounter.  We then show how we must modify our algorithm for this more general setting.
We attempt to present this material in such a way that even if the syntax of the 
code change, the overall functionality is still made clear.

We will need to find ways to perform the following operations:
\begin{itemize}
\item Find the convex hull of a set of points.
\item Determine if a point is on the interior or boundary of a convex polytope.
\item Once it is determined that a point is on the boundary, determine the face.
\item Find normal and tangent cones.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programming LCM theory}
\text{rcdd} \citep{rcdd}

\subsection{Linearity}\label{S:linearity}
\citet{Geyer:gdor} defines the \emph{linearity} of a set of points $V$ to be
\begin{align*}
	L = \{ w \in V: -w \in \con( V ) \}.
\end{align*}
The set $L$ is a finite set and the same $L$ we saw in Theorem~\ref{Thm:L-GDOR}.

\hl{We use this to find the face of the polyhedral convex support on which the observed 
statistic lies in the relative interior.  The function \texttt{linearity} in the \texttt
{rcdd} package can take as input the rays directed from observed statistic to sample 
points generated via MCMC from the distribution with parameter value of interest.  
From these, it determines the rays that form the basis of a vector subspace.  
Of course we are not interested in the vector subspace; we need only take the points 
corresponding to these bases as our empirical face $F$.  By construction, the observed 
statistic lies in the interior of $F$.}  By Corollary~\ref{Cor:spanL}, 
$S \cap H = S \cap(\yobs + \spanl (L))$.  We are interested in $\spanl L$.

%Let $\widetilde{T}_C(x)$ be the empirical tangent cone at the point $x$ constructed 
%from a set of points $W$ all in $C$.  That is,
%\begin{align*}
%	\widetilde{T}_C(x) = \{ w - x: w \in W\}.
%\end{align*}
%Then for $V = \widetilde{T}_C(g(\yobs))$, the linearity corresponds to the set of 
%points from $W$ (shifted by $g(\yobs)$) that form what we refer to as the empirical 
%face $F$.  
%In our algorithm, we use the MCMC sample points for $W$.  \citet{Geyer:gdor} provides 
%the function \texttt{linearity} in the R package \texttt{rcdd} to do this computation.

%Consider the case when $g(\yobs) \in \rintr C$.  Then the MLE exists, and the 
%linearity will correspond to all the points $W \in C$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Subsampling}
%Once our algorithm has determined that the observed data is on the boundary and thus 
%the MLE does not exist in the conven
Our algorithm depends on being able to to subsample from the MC sample points when 
optimizing in the LCM, restricting to the sample points that comprise the support of 
this model.  The subsampling operation itself is straightforward since we determined 
empirically the face that forms this support.  

However, it is not obvious that when we draw $g(Y_1), \ldots, g(Y_m)$ from the 
original model with parameter value $\eta$ that the subsample restricted to the 
support is in fact a sample from the LCM with parameter value $\eta$.  This is a 
convenient consequence of Theorem~\ref{Thm:LCM}: by this stage in the algorithm, the 
majority of sample points (greater than 60\%) are on the face.  Then $s$ in the 
expression $P_{\eta + s \delta}(g(Y) \in H)$ is a large value.  But since the LCM 
density is the limit of the density of the original model as $s$ goes to $+\infty$, 
the distribution of the LCM is accordingly well approximated.  

$P_{LCM, \eta} = P_\eta( \cdot \mid g(Y) \in H )$


\subsection{Convex hull}
Finding the convex hull from a set of points is of course a well-studied problem.  We 
mention it here to describe the sequence of functions we use to find the convex hull.  
Beginning with a large number of MC sample points, say 10,000, we first use the 
\texttt{unique} function in the \texttt{base} package in R to reduce to the distinct 
points in the set.  Then we apply the \texttt{redundant} function in the \texttt{rcdd} 
package to reduce to a linearly independent set, which corresponds to the V-
representation of the convex hull.

Our algorithm utilizes the H-representation of the sample points to determine if $g(\yobs)$ lies on the 
exterior, boundary, or interior of the convex hull of a set of points.  
This requires switching back and forth between the V-representation and 
the H-representation.  
This functionality is provided by the \texttt{scdd} function, which takes as an 
argument the original representation (H or V) and returns the toggled representation.

Either representation can be expressed with rational arithmetic, making equality 
comparisons exact.  This is particularly important when trying to determine the 
location of $g(\yobs)$ relative to the convex hull of a set of points.  The \texttt
{rcdd} package also provides the necessary matrix arithmetic functions to work with 
the rational representation.
