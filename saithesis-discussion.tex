The goal of this dissertation is to introduce a new practical approach
for finding the MLE of a discrete exponential family when the MLE exists
in the conventional sense,
and calculating one-sided confidence intervals for the parameters when it does not.

%We have presented a simple line search algorithm for finding the MLE of a regular 
%exponential family when the MLE exists.  
The algorithm avoids the trial and error experimentation of tuning parameters 
and starting points commonly associated with optimization routines
not invented by optimization specialists.  Our algorithm is modeled after algorithms 
discussed in optimization textbooks \citep{Fletcher,NW,Sun:2006},
all of which are safeguarded to ensure rapid automatic convergence.
%Because it only relies on first order derivatives, this approach avoids problems with 
%near-singular Fisher information 
%matrices that plague methods like Newton-Raphson.  The reliant on a curvature 
%condition for step size makes it less 
%sensitive to poor initial values that are problematic for MCMC-MLE and  SA in 
%practice.

In the setting where the MLE exists, convergence is guaranteed when the gradient 
can be calculated exactly.  Even when the 
gradient cannot be calculated 
exactly and is only estimable via MCMC, the algorithm is still useful in practice, as 
demonstrated by the Ising model and Faux Magnolia High
examples.  We have also described a way to construct and use confidence intervals to 
make convergence highly probable.

For the setting where the MLE does not exist and the convex support is 
not known (as in ERGMs), our algorithm uses MCMC sampling to
both direct the search towards the face in which the observed statistic lies
while exploring the geometry of the convex support.  We use
computationally efficient methods that avoid calculating H-representations
of convex hulls and only do comparisons using rational arithmetic.
We know of no other general approach for dealing with the setting where
the convex support is unknown in advance.

The algorithm can be computationally demanding.  When the current iteration approaches 
the solution, the 
curvature condition for step size becomes more difficult to satisfy and the method may 
require several iterations of 
MCMC sampling and perhaps an increase in MCMC sample size.  Eventual increase in MCMC 
sample size is unavoidable,
because the achievable accuracy is inversely proportional to the square root of the 
MCMC sample size, as in all Monte Carlo.
Thus we believe the best use of this algorithm is in combination with other faster 
methods like MCMC-MLE \citep{Geyer:1992}
or Newton-Raphson safeguarded by our line search algorithm.  Our 
algorithm should be used from ``long range'', when one has no good intuition for an 
initial value and is concerned about 
picking one that is far from the MLE.  The switch between types of search direction 
(steepest ascent, conjugate gradient,
or Newton) within our algorithm or the switch to another algorithm (such as MCMC-MLE 
\citep{Geyer:1992})
need not require manual intervention.  When used in combination in this
manner, we do not think the confidence intervals are necessary as the curvature 
condition is quite easily satisfied 
when the current iteration is far from the MLE.

One way to improve performance is to use conjugate gradient search directions rather 
than steepest ascent.  In our 
examples, this reduced the number of iterations by over 20\%.  However, in other 
problems we tried with different 
dimensionality, this performance varied significantly and it appears that no guarantee 
can be made about quantity of 
improvement in performance, though in all cases we examined, it never did worse.  This 
is no surprise, because the
necessity of ``preconditioning'' for good performance of the conjugate gradient 
algorithm is well known (but 
we know of no literature about ``preconditioners'' for 
maximum likelihood in exponential families).

\section{Areas for further research}
\subsection{Convergence}
There are several outstanding issues.  We have not showed convergence of 
the algorithm when the gradient 
is approximated via MCMC.  This is a more difficult theoretical problem and is the 
motivation for stochastic 
approximation research.  
Further work is necessary to determine if one can adapt our restrictive curvature 
condition \eqref{E:Wolfe-ll} to the 
approach of \citet{Andrieu:2005} or \citet{Liang:2010} in MCMC stochastic 
approximation.  

\subsection{Step size search}
The process of finding a step size $\alpha$ to satisfy the curvature condition
 \eqref{E:Wolfe-ll} can be improved.
The spline approximation of $\nabla \ell(\eta + \alpha p)^T p$ as a function of $\alpha$ should ideally incorporate the fact that the function is strictly
concave (Section~\ref{S:Step size}).  
As noted above as well as in Section~\ref{S:MCMC approx}, when the gradient must be approximated
via MCMC, 
\eqref{E:Wolfe-ll} becomes increasingly difficult to satisfy as the algorithm nears the
solution.  Our approach currently generates two independent MCMC samples, one for
the distribution at $\eta$, the other at $\eta + \alpha p$.  By using simulating tempering
in the manner of \citet{Geyer:1995}, we might effectively introduce correlation between these
samples, making \eqref{E:Wolfe-ll} easier to satisfy.  

\subsection{Switch criteria}
Another remaining issue is the stopping or switching criteria: while our Ising model example
in Section~\ref{S:Example:Ising} illustrated that it is in fact possible to get reasonable 
parameter estimates only using our algorithm, we believe it will be more computationally
efficient to switch to another algorithm like Newton-Raphson or MCMC-MLE.  

But when?  
The Faux Magnolia example in Section~\ref{S:Example:FauxMagnolia} showed that using 
our algorithm for $2d$ steepest ascent updates was not enough (though it was adequate when
 using conjugate gradient directions).
As noted in Section~\ref{S:Combine}, MCMC-MLE converges if and only 
if the importance weights for the sample in \eqref{E:r_hat} stabilize.  Establishing criteria on these importance weights should make it 
possible to switch to MCMC-MLE only when it can attain the MLE in one update.

\subsection{Monte Carlo standard errors}
 A final remaining issue is estimation of Monte Carlo error of the estimates.  
 Here too we recommend switching to another
algorithm at the end.  The MCMC-MLE procedure gives accurate error estimates \citep{Geyer:1994,Hunter:2006}.
For very small steps these are essentially the same as the Monte Carlo error of a single unsafeguarded Newton-Raphson step,
so the method in \citep{Geyer:1994} can be used for either.




